{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3966420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/', 'content_type': 'text/html; charset=utf-8', 'title': 'LangChain Expression Language (LCEL) | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest â€œprompt + LLMâ€� chain to the most complex chains (weâ€™ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:First-class streaming support\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. Weâ€™re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains itâ€™s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and itâ€™s available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment\\nAny chain created with LCEL can be easily deployed using LangServe.Help us out by providing feedback on this documentation page:PreviousWeb scrapingNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/streaming/', 'content_type': 'text/html; charset=utf-8', 'title': 'Streaming | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'Streaming is critical in making applications based on LLMs feel responsive to end-users.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nStreaming | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageStreamingOn this pageStreaming With LangChainStreaming is critical in making applications based on LLMs feel responsive to end-users.Important LangChain primitives like LLMs, parsers, prompts, retrievers, and agents implement the LangChain Runnable Interface.This interface provides two general approaches to stream content:sync stream and async astream: a default implementation of streaming that streams the final output from the chain.async astream_events and async astream_log: these provide a way to stream both intermediate steps and final output from the chain.Let\\'s take a look at both approaches, and try to understand how to use them. ğŸ¥·Using Streamâ€‹All Runnable objects implement a sync method called stream and an async variant called astream. These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.Streaming is only possible if all steps in the program know how to process an input stream; i.e., process an input chunk one at a time, and yield a corresponding output chunk.The complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.The best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!LLMs and Chat Modelsâ€‹Large language models and their chat variants are the primary bottleneck in LLM based apps. ğŸ™ŠLarge language models can take several seconds to generate a complete response to a query. This is far slower than the ~200-300 ms threshold at which an application feels responsive to an end user.The key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model token by token.We will show examples of streaming using the chat model from Anthropic. To use the model, you will need to install the langchain-anthropic package. You can do this with the following command:pip install -qU langchain-anthropic# Showing the example using anthropic, but you can use# your favorite chat model!from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic()chunks = []async for chunk in model.astream(\"hello. tell me something about yourself\"):    chunks.append(chunk)    print(chunk.content, end=\"|\", flush=True)API Reference:ChatAnthropic Hello|!| My| name| is| Claude|.| I|\\'m| an| AI| assistant| created| by| An|throp|ic| to| be| helpful|,| harmless|,| and| honest|.||Let\\'s inspect one of the chunkschunks[0]AIMessageChunk(content=\\' Hello\\')We got back something called an AIMessageChunk. This chunk represents a part of an AIMessage.Message chunks are additive by design -- one can simply add them up to get the state of the response so far!chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]AIMessageChunk(content=\\' Hello! My name is\\')Chainsâ€‹Virtually all LLM applications involve more steps than just a call to a language model.Let\\'s build a simple chain using LangChain Expression Language (LCEL) that combines a prompt, model and a parser and verify that streaming works.We will use StrOutputParser to parse the output from the model. This is a simple parser that extracts the content field from an AIMessageChunk, giving us the token returned by the model.tipLCEL is a declarative way to specify a \"program\" by chainining together different LangChain primitives. Chains created using LCEL benefit from an automatic implementation of stream and astream allowing streaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable interface.from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")parser = StrOutputParser()chain = prompt | model | parserasync for chunk in chain.astream({\"topic\": \"parrot\"}):    print(chunk, end=\"|\", flush=True)API Reference:StrOutputParserChatPromptTemplate Here|\\'s| a| silly| joke| about| a| par|rot|:|What| kind| of| teacher| gives| good| advice|?| An| ap|-|parent| (|app|arent|)| one|!||You might notice above that parser actually doesn\\'t block the streaming output from the model, and instead processes each chunk individually. Many of the LCEL primitives also support this kind of transform-style passthrough streaming, which can be very convenient when constructing apps.Certain runnables, like prompt templates and chat models, cannot process individual chunks and instead aggregate all previous steps. This will interrupt the streaming process. Custom functions can be designed to return generators, whichnoteIf the above functionality is not relevant to what you\\'re building, you do not have to use the LangChain Expression Language to use LangChain and can instead rely on a standard imperative programming approach by\\ncaling invoke, batch or stream on each component individually, assigning the results to variables and then using them downstream as you see fit.If that works for your needs, then that\\'s fine by us ğŸ‘Œ!Working with Input Streamsâ€‹What if you wanted to stream JSON from the output as it was being generated?If you were to rely on json.loads to parse the partial json, the parsing would fail as the partial json wouldn\\'t be valid json.You\\'d likely be at a complete loss of what to do and claim that it wasn\\'t possible to stream JSON.Well, turns out there is a way to do it -- the parser needs to operate on the input stream, and attempt to \"auto-complete\" the partial json into a valid state.Let\\'s see such a parser in action to understand what this means.from langchain_core.output_parsers import JsonOutputParserchain = (    model | JsonOutputParser())  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some modelsasync for text in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'):    print(text, flush=True)API Reference:JsonOutputParser{}{\\'countries\\': []}{\\'countries\\': [{}]}{\\'countries\\': [{\\'name\\': \\'\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 6739}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 673915}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Sp\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 4675}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 467547}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'Japan\\'}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'Japan\\', \\'population\\': 12}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'Japan\\', \\'population\\': 12647}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'Japan\\', \\'population\\': 1264764}]}{\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {\\'name\\': \\'Spain\\', \\'population\\': 46754778}, {\\'name\\': \\'Japan\\', \\'population\\': 126476461}]}Now, let\\'s break streaming. We\\'ll use the previous example and append an extraction function at the end that extracts the country names from the finalized JSON.dangerAny steps in the chain that operate on finalized inputs rather than on input streams can break streaming functionality via stream or astream.tipLater, we will discuss the astream_events API which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on finalized inputs.from langchain_core.output_parsers import (    JsonOutputParser,)# A function that operates on finalized inputs# rather than on an input_streamdef _extract_country_names(inputs):    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"    if not isinstance(inputs, dict):        return \"\"    if \"countries\" not in inputs:        return \"\"    countries = inputs[\"countries\"]    if not isinstance(countries, list):        return \"\"    country_names = [        country.get(\"name\") for country in countries if isinstance(country, dict)    ]    return country_nameschain = model | JsonOutputParser() | _extract_country_namesasync for text in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'):    print(text, end=\"|\", flush=True)API Reference:JsonOutputParser[\\'France\\', \\'Spain\\', \\'Japan\\']|Generator Functionsâ€‹Le\\'ts fix the streaming using a generator function that can operate on the input stream.tipA generator function (a function that uses yield) allows writing code that operators on input streamsfrom langchain_core.output_parsers import JsonOutputParserasync def _extract_country_names_streaming(input_stream):    \"\"\"A function that operates on input streams.\"\"\"    country_names_so_far = set()    async for input in input_stream:        if not isinstance(input, dict):            continue        if \"countries\" not in input:            continue        countries = input[\"countries\"]        if not isinstance(countries, list):            continue        for country in countries:            name = country.get(\"name\")            if not name:                continue            if name not in country_names_so_far:                yield name                country_names_so_far.add(name)chain = model | JsonOutputParser() | _extract_country_names_streamingasync for text in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'):    print(text, end=\"|\", flush=True)API Reference:JsonOutputParserFrance|Sp|Spain|Japan|noteBecause the code above is relying on JSON auto-completion, you may see partial names of countries (e.g., Sp and Spain), which is not what one would want for an extraction result!We\\'re focusing on streaming concepts, not necessarily the results of the chains.Non-streaming componentsâ€‹Some built-in components like Retrievers do not offer any streaming. What happens if we try to stream them? ğŸ¤¨from langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingstemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    [\"harrison worked at kensho\", \"harrison likes spicy food\"],    embedding=OpenAIEmbeddings(),)retriever = vectorstore.as_retriever()chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]chunksAPI Reference:FAISSStrOutputParserChatPromptTemplateRunnablePassthroughOpenAIEmbeddings[[Document(page_content=\\'harrison worked at kensho\\'),  Document(page_content=\\'harrison likes spicy food\\')]]Stream just yielded the final result from that component.This is OK ğŸ¥¹! Not all components have to implement streaming -- in some cases streaming is either unnecessary, difficult or just doesn\\'t make sense.tipAn LCEL chain constructed using non-streaming components, will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain.retrieval_chain = (    {        \"context\": retriever.with_config(run_name=\"Docs\"),        \"question\": RunnablePassthrough(),    }    | prompt    | model    | StrOutputParser())for chunk in retrieval_chain.stream(    \"Where did harrison work? \" \"Write 3 made up sentences about this place.\"):    print(chunk, end=\"|\", flush=True) Based| on| the| given| context|,| the| only| information| provided| about| where| Harrison| worked| is| that| he| worked| at| Ken|sh|o|.| Since| there| are| no| other| details| provided| about| Ken|sh|o|,| I| do| not| have| enough| information| to| write| 3| additional| made| up| sentences| about| this| place|.| I| can| only| state| that| Harrison| worked| at| Ken|sh|o|.||Now that we\\'ve seen how stream and astream work, let\\'s venture into the world of streaming events. ğŸ��ï¸�Using Stream Eventsâ€‹Event Streaming is a beta API. This API may change a bit based on feedback.noteIntroduced in langchain-core 0.1.14.import langchain_corelangchain_core.__version__\\'0.1.18\\'For the astream_events API to work properly:Use async throughout the code to the extent possible (e.g., async tools etc)Propagate callbacks if defining custom functions / runnablesWhenever using runnables without LCEL, make sure to call .astream() on LLMs rather than .ainvoke to force the LLM to stream tokens.Let us know if anything doesn\\'t work as expected! :)Event Referenceâ€‹Below is a reference table that shows some events that might be emitted by the various Runnable objects.noteWhen streaming is implemented properly, the inputs to a runnable will not be known until after the input stream has been entirely consumed. This means that inputs will often be included only for end events and rather than for start events.eventnamechunkinputoutputon_chat_model_start[model name]{\"messages\": [[SystemMessage, HumanMessage]]}on_chat_model_stream[model name]AIMessageChunk(content=\"hello\")on_chat_model_end[model name]{\"messages\": [[SystemMessage, HumanMessage]]}{\"generations\": [...], \"llm_output\": None, ...}on_llm_start[model name]{\\'input\\': \\'hello\\'}on_llm_stream[model name]\\'Hello\\'on_llm_end[model name]\\'Hello human!\\'on_chain_startformat_docson_chain_streamformat_docs\"hello world!, goodbye world!\"on_chain_endformat_docs[Document(...)]\"hello world!, goodbye world!\"on_tool_startsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_streamsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_endsome_tool{\"x\": 1, \"y\": \"2\"}on_retriever_start[retriever name]{\"query\": \"hello\"}on_retriever_chunk[retriever name]{documents: [...]}on_retriever_end[retriever name]{\"query\": \"hello\"}{documents: [...]}on_prompt_start[template_name]{\"question\": \"hello\"}on_prompt_end[template_name]{\"question\": \"hello\"}ChatPromptValue(messages: [SystemMessage, ...])Chat Modelâ€‹Let\\'s start off by looking at the events produced by a chat model.events = []async for event in model.astream_events(\"hello\", version=\"v1\"):    events.append(event)/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.  warn_beta(noteHey what\\'s that funny version=\"v1\" parameter in the API?! ğŸ˜¾This is a beta API, and we\\'re almost certainly going to make some changes to it.This version parameter will allow us to minimize such breaking changes to your code. In short, we are annoying you now, so we don\\'t have to annoy you later.Let\\'s take a look at the few of the start event and a few of the end events.events[:3][{\\'event\\': \\'on_chat_model_start\\',  \\'run_id\\': \\'555843ed-3d24-4774-af25-fbf030d5e8c4\\',  \\'name\\': \\'ChatAnthropic\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'data\\': {\\'input\\': \\'hello\\'}}, {\\'event\\': \\'on_chat_model_stream\\',  \\'run_id\\': \\'555843ed-3d24-4774-af25-fbf030d5e8c4\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'name\\': \\'ChatAnthropic\\',  \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' Hello\\')}}, {\\'event\\': \\'on_chat_model_stream\\',  \\'run_id\\': \\'555843ed-3d24-4774-af25-fbf030d5e8c4\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'name\\': \\'ChatAnthropic\\',  \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\'!\\')}}]events[-2:][{\\'event\\': \\'on_chat_model_stream\\',  \\'run_id\\': \\'555843ed-3d24-4774-af25-fbf030d5e8c4\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'name\\': \\'ChatAnthropic\\',  \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\'\\')}}, {\\'event\\': \\'on_chat_model_end\\',  \\'name\\': \\'ChatAnthropic\\',  \\'run_id\\': \\'555843ed-3d24-4774-af25-fbf030d5e8c4\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'data\\': {\\'output\\': AIMessageChunk(content=\\' Hello!\\')}}]Chainâ€‹Let\\'s revisit the example chain that parsed streaming JSON to explore the streaming events API.chain = (    model | JsonOutputParser())  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some modelsevents = [    event    async for event in chain.astream_events(        \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',        version=\"v1\",    )]If you examine at the first few events, you\\'ll notice that there are 3 different start events rather than 2 start events.The three start events correspond to:The chain (model + parser)The modelThe parserevents[:3][{\\'event\\': \\'on_chain_start\\',  \\'run_id\\': \\'b1074bff-2a17-458b-9e7b-625211710df4\\',  \\'name\\': \\'RunnableSequence\\',  \\'tags\\': [],  \\'metadata\\': {},  \\'data\\': {\\'input\\': \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'}}, {\\'event\\': \\'on_chat_model_start\\',  \\'name\\': \\'ChatAnthropic\\',  \\'run_id\\': \\'6072be59-1f43-4f1c-9470-3b92e8406a99\\',  \\'tags\\': [\\'seq:step:1\\'],  \\'metadata\\': {},  \\'data\\': {\\'input\\': {\\'messages\\': [[HumanMessage(content=\\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\')]]}}}, {\\'event\\': \\'on_parser_start\\',  \\'name\\': \\'JsonOutputParser\\',  \\'run_id\\': \\'bf978194-0eda-4494-ad15-3a5bfe69cd59\\',  \\'tags\\': [\\'seq:step:2\\'],  \\'metadata\\': {},  \\'data\\': {}}]What do you think you\\'d see if you looked at the last 3 events? what about the middle?Let\\'s use this API to take output the stream events from the model and the parser. We\\'re ignoring start events, end events and events from the chain.num_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(            f\"Chat model chunk: {repr(event[\\'data\\'][\\'chunk\\'].content)}\",            flush=True,        )    if kind == \"on_parser_stream\":        print(f\"Parser chunk: {event[\\'data\\'][\\'chunk\\']}\", flush=True)    num_events += 1    if num_events > 30:        # Truncate the output        print(\"...\")        breakChat model chunk: \\' Here\\'Chat model chunk: \\' is\\'Chat model chunk: \\' the\\'Chat model chunk: \\' JSON\\'Chat model chunk: \\' with\\'Chat model chunk: \\' the\\'Chat model chunk: \\' requested\\'Chat model chunk: \\' countries\\'Chat model chunk: \\' and\\'Chat model chunk: \\' their\\'Chat model chunk: \\' populations\\'Chat model chunk: \\':\\'Chat model chunk: \\'\\\\n\\\\n```\\'Chat model chunk: \\'json\\'Parser chunk: {}Chat model chunk: \\'\\\\n{\\'Chat model chunk: \\'\\\\n \\'Chat model chunk: \\' \"\\'Chat model chunk: \\'countries\\'Chat model chunk: \\'\":\\'Parser chunk: {\\'countries\\': []}Chat model chunk: \\' [\\'Chat model chunk: \\'\\\\n   \\'Parser chunk: {\\'countries\\': [{}]}Chat model chunk: \\' {\\'...Because both the model and the parser support streaming, we see sreaming events from both components in real time! Kind of cool isn\\'t it? ğŸ¦œFiltering Eventsâ€‹Because this API produces so many events, it is useful to be able to filter on events.You can filter by either component name, component tags or component type.By Nameâ€‹chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(    {\"run_name\": \"my_parser\"})max_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",    include_names=[\"my_parser\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break{\\'event\\': \\'on_parser_start\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': []}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'\\'}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\'}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 6739}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 673915}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}]}}}{\\'event\\': \\'on_parser_stream\\', \\'name\\': \\'my_parser\\', \\'run_id\\': \\'f2ac1d1c-e14a-45fc-8990-e5c24e707299\\', \\'tags\\': [\\'seq:step:2\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': {\\'countries\\': [{\\'name\\': \\'France\\', \\'population\\': 67391582}, {}]}}}...By Typeâ€‹chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(    {\"run_name\": \"my_parser\"})max_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",    include_types=[\"chat_model\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break{\\'event\\': \\'on_chat_model_start\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'input\\': {\\'messages\\': [[HumanMessage(content=\\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\')]]}}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' Here\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' is\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' the\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' JSON\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' with\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' the\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' requested\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' countries\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' and\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'model\\', \\'run_id\\': \\'98a6e192-8159-460c-ba73-6dfc921e3777\\', \\'tags\\': [\\'seq:step:1\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' their\\')}}...By Tagsâ€‹cautionTags are inherited by child components of a given runnable. If you\\'re using tags to filter, make sure that this is what you want.chain = (model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})max_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",    include_tags=[\"my_chain\"],):    print(event)    max_events += 1    if max_events > 10:        # Truncate output        print(\"...\")        break{\\'event\\': \\'on_chain_start\\', \\'run_id\\': \\'190875f3-3fb7-49ad-9b6e-f49da22f3e49\\', \\'name\\': \\'RunnableSequence\\', \\'tags\\': [\\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\'}}{\\'event\\': \\'on_chat_model_start\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'input\\': {\\'messages\\': [[HumanMessage(content=\\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\')]]}}}{\\'event\\': \\'on_parser_start\\', \\'name\\': \\'JsonOutputParser\\', \\'run_id\\': \\'3b5e4ca1-40fe-4a02-9a19-ba2a43a6115c\\', \\'tags\\': [\\'seq:step:2\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' Here\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' is\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' the\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' JSON\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' with\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' the\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' requested\\')}}{\\'event\\': \\'on_chat_model_stream\\', \\'name\\': \\'ChatAnthropic\\', \\'run_id\\': \\'ff58f732-b494-4ff9-852a-783d42f4455d\\', \\'tags\\': [\\'seq:step:1\\', \\'my_chain\\'], \\'metadata\\': {}, \\'data\\': {\\'chunk\\': AIMessageChunk(content=\\' countries\\')}}...Non-streaming componentsâ€‹Remember how some components don\\'t stream well because they don\\'t operate on input streams?While such components can break streaming of the final output when using astream, astream_events will still yield streaming events from intermediate steps that support streaming!# Function that does not support streaming.# It operates on the finalizes inputs rather than# operating on the input stream.def _extract_country_names(inputs):    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"    if not isinstance(inputs, dict):        return \"\"    if \"countries\" not in inputs:        return \"\"    countries = inputs[\"countries\"]    if not isinstance(countries, list):        return \"\"    country_names = [        country.get(\"name\") for country in countries if isinstance(country, dict)    ]    return country_nameschain = (    model | JsonOutputParser() | _extract_country_names)  # This parser only works with OpenAI right nowAs expected, the astream API doesn\\'t work correctly because _extract_country_names doesn\\'t operate on streams.async for chunk in chain.astream(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',):    print(chunk, flush=True)[\\'France\\', \\'Spain\\', \\'Japan\\']Now, let\\'s confirm that with astream_events we\\'re still seeing streaming output from the model and the parser.num_events = 0async for event in chain.astream_events(    \\'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`\\',    version=\"v1\",):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(            f\"Chat model chunk: {repr(event[\\'data\\'][\\'chunk\\'].content)}\",            flush=True,        )    if kind == \"on_parser_stream\":        print(f\"Parser chunk: {event[\\'data\\'][\\'chunk\\']}\", flush=True)    num_events += 1    if num_events > 30:        # Truncate the output        print(\"...\")        breakChat model chunk: \\' Here\\'Chat model chunk: \\' is\\'Chat model chunk: \\' the\\'Chat model chunk: \\' JSON\\'Chat model chunk: \\' with\\'Chat model chunk: \\' the\\'Chat model chunk: \\' requested\\'Chat model chunk: \\' countries\\'Chat model chunk: \\' and\\'Chat model chunk: \\' their\\'Chat model chunk: \\' populations\\'Chat model chunk: \\':\\'Chat model chunk: \\'\\\\n\\\\n```\\'Chat model chunk: \\'json\\'Parser chunk: {}Chat model chunk: \\'\\\\n{\\'Chat model chunk: \\'\\\\n \\'Chat model chunk: \\' \"\\'Chat model chunk: \\'countries\\'Chat model chunk: \\'\":\\'Parser chunk: {\\'countries\\': []}Chat model chunk: \\' [\\'Chat model chunk: \\'\\\\n   \\'Parser chunk: {\\'countries\\': [{}]}Chat model chunk: \\' {\\'Chat model chunk: \\'\\\\n     \\'Chat model chunk: \\' \"\\'...Propagating Callbacksâ€‹cautionIf you\\'re using invoking runnables inside your tools, you need to propagate callbacks to the runnable; otherwise, no stream events will be generated.noteWhen using RunnableLambdas or @chain decorator, callbacks are propagated automatically behind the scenes.from langchain_core.runnables import RunnableLambdafrom langchain_core.tools import tooldef reverse_word(word: str):    return word[::-1]reverse_word = RunnableLambda(reverse_word)@tooldef bad_tool(word: str):    \"\"\"Custom tool that doesn\\'t propagate callbacks.\"\"\"    return reverse_word.invoke(word)async for event in bad_tool.astream_events(\"hello\", version=\"v1\"):    print(event)API Reference:RunnableLambdatool{\\'event\\': \\'on_tool_start\\', \\'run_id\\': \\'ae7690f8-ebc9-4886-9bbe-cb336ff274f2\\', \\'name\\': \\'bad_tool\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\'}}{\\'event\\': \\'on_tool_stream\\', \\'run_id\\': \\'ae7690f8-ebc9-4886-9bbe-cb336ff274f2\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\': \\'bad_tool\\', \\'data\\': {\\'chunk\\': \\'olleh\\'}}{\\'event\\': \\'on_tool_end\\', \\'name\\': \\'bad_tool\\', \\'run_id\\': \\'ae7690f8-ebc9-4886-9bbe-cb336ff274f2\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'output\\': \\'olleh\\'}}Here\\'s a re-implementation that does propagate callbacks correctly. You\\'ll notice that now we\\'re getting events from the reverse_word runnable as well.@tooldef correct_tool(word: str, callbacks):    \"\"\"A tool that correctly propagates callbacks.\"\"\"    return reverse_word.invoke(word, {\"callbacks\": callbacks})async for event in correct_tool.astream_events(\"hello\", version=\"v1\"):    print(event){\\'event\\': \\'on_tool_start\\', \\'run_id\\': \\'384f1710-612e-4022-a6d4-8a7bb0cc757e\\', \\'name\\': \\'correct_tool\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\'}}{\\'event\\': \\'on_chain_start\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'c4882303-8867-4dff-b031-7d9499b39dda\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'c4882303-8867-4dff-b031-7d9499b39dda\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'hello\\', \\'output\\': \\'olleh\\'}}{\\'event\\': \\'on_tool_stream\\', \\'run_id\\': \\'384f1710-612e-4022-a6d4-8a7bb0cc757e\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\': \\'correct_tool\\', \\'data\\': {\\'chunk\\': \\'olleh\\'}}{\\'event\\': \\'on_tool_end\\', \\'name\\': \\'correct_tool\\', \\'run_id\\': \\'384f1710-612e-4022-a6d4-8a7bb0cc757e\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'output\\': \\'olleh\\'}}If you\\'re invoking runnables from within Runnable Lambdas or @chains, then callbacks will be passed automatically on your behalf.from langchain_core.runnables import RunnableLambdaasync def reverse_and_double(word: str):    return await reverse_word.ainvoke(word) * 2reverse_and_double = RunnableLambda(reverse_and_double)await reverse_and_double.ainvoke(\"1234\")async for event in reverse_and_double.astream_events(\"1234\", version=\"v1\"):    print(event)API Reference:RunnableLambda{\\'event\\': \\'on_chain_start\\', \\'run_id\\': \\'4fe56c7b-6982-4999-a42d-79ba56151176\\', \\'name\\': \\'reverse_and_double\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_start\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'335fe781-8944-4464-8d2e-81f61d1f85f5\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'335fe781-8944-4464-8d2e-81f61d1f85f5\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\', \\'output\\': \\'4321\\'}}{\\'event\\': \\'on_chain_stream\\', \\'run_id\\': \\'4fe56c7b-6982-4999-a42d-79ba56151176\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\': \\'reverse_and_double\\', \\'data\\': {\\'chunk\\': \\'43214321\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_and_double\\', \\'run_id\\': \\'4fe56c7b-6982-4999-a42d-79ba56151176\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'output\\': \\'43214321\\'}}And with the @chain decorator:from langchain_core.runnables import chain@chainasync def reverse_and_double(word: str):    return await reverse_word.ainvoke(word) * 2await reverse_and_double.ainvoke(\"1234\")async for event in reverse_and_double.astream_events(\"1234\", version=\"v1\"):    print(event)API Reference:chain{\\'event\\': \\'on_chain_start\\', \\'run_id\\': \\'7485eedb-1854-429c-a2f8-03d01452daef\\', \\'name\\': \\'reverse_and_double\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_start\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'e7cddab2-9b95-4e80-abaf-4b2429117835\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_word\\', \\'run_id\\': \\'e7cddab2-9b95-4e80-abaf-4b2429117835\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'input\\': \\'1234\\', \\'output\\': \\'4321\\'}}{\\'event\\': \\'on_chain_stream\\', \\'run_id\\': \\'7485eedb-1854-429c-a2f8-03d01452daef\\', \\'tags\\': [], \\'metadata\\': {}, \\'name\\': \\'reverse_and_double\\', \\'data\\': {\\'chunk\\': \\'43214321\\'}}{\\'event\\': \\'on_chain_end\\', \\'name\\': \\'reverse_and_double\\', \\'run_id\\': \\'7485eedb-1854-429c-a2f8-03d01452daef\\', \\'tags\\': [], \\'metadata\\': {}, \\'data\\': {\\'output\\': \\'43214321\\'}}Help us out by providing feedback on this documentation page:PreviousAdvantages of LCELNextAdd message history (memory)Using StreamLLMs and Chat ModelsChainsWorking with Input StreamsNon-streaming componentsUsing Stream EventsEvent ReferenceChat ModelChainFiltering EventsNon-streaming componentsPropagating CallbacksCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/get_started/', 'content_type': 'text/html; charset=utf-8', 'title': 'Get started | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'LCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nGet started | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageGet startedOn this pageGet startedLCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.Basic example: prompt + model + output parserâ€‹The most basic and common use case is chaining a prompt template and a model together. To see how this works, let\\'s create a chain that takes a topic and generates a joke:%pip install --upgrade --quiet  langchain-core langchain-community langchain-openaiOpenAIAnthropicGoogleCohereFireworksAIMistralAITogetherAIInstall dependenciespip install -qU langchain-openaiSet environment variablesimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4\")Install dependenciespip install -qU langchain-anthropicSet environment variablesimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")Install dependenciespip install -qU langchain-google-vertexaiSet environment variablesimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAImodel = ChatVertexAI(model=\"gemini-pro\")Install dependenciespip install -qU langchain-cohereSet environment variablesimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoheremodel = ChatCohere(model=\"command-r\")Install dependenciespip install -qU langchain-fireworksSet environment variablesimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksmodel = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")Install dependenciespip install -qU langchain-mistralaiSet environment variablesimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAImodel = ChatMistralAI(model=\"mistral-large-latest\")Install dependenciespip install -qU langchain-openaiSet environment variablesimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAImodel = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")output_parser = StrOutputParser()chain = prompt | model | output_parserchain.invoke({\"topic\": \"ice cream\"})API Reference:StrOutputParserChatPromptTemplate\"Why don\\'t ice creams ever get invited to parties?\\\\n\\\\nBecause they always drip when things heat up!\"Notice this line of the code, where we piece together these different components into a single chain using LCEL:chain = prompt | model | output_parserThe | symbol is similar to a unix pipe operator, which chains together the different components, feeding the output from one component as input into the next component. In this chain the user input is passed to the prompt template, then the prompt template output is passed to the model, then the model output is passed to the output parser. Let\\'s take a look at each component individually to really understand what\\'s going on.1. Promptâ€‹prompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue. A PromptValue is a wrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input). It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string.prompt_value = prompt.invoke({\"topic\": \"ice cream\"})prompt_valueChatPromptValue(messages=[HumanMessage(content=\\'tell me a short joke about ice cream\\')])prompt_value.to_messages()[HumanMessage(content=\\'tell me a short joke about ice cream\\')]prompt_value.to_string()\\'Human: tell me a short joke about ice cream\\'2. Modelâ€‹The PromptValue is then passed to model. In this case our model is a ChatModel, meaning it will output a BaseMessage.message = model.invoke(prompt_value)messageAIMessage(content=\"Why don\\'t ice creams ever get invited to parties?\\\\n\\\\nBecause they always bring a melt down!\")If our model was an LLM, it would output a string.from langchain_openai import OpenAIllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")llm.invoke(prompt_value)API Reference:OpenAI\\'\\\\n\\\\nRobot: Why did the ice cream truck break down? Because it had a meltdown!\\'3. Output parserâ€‹And lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a\\nBaseMessage as input. The specific StrOutputParser simply converts any input into a string.output_parser.invoke(message)\"Why did the ice cream go to therapy? \\\\n\\\\nBecause it had too many toppings and couldn\\'t find its cone-fidence!\"4. Entire Pipelineâ€‹To follow the steps along:We pass in user input on the desired topic as {\"topic\": \"ice cream\"}The prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt. The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object. Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method. infoNote that if youâ€™re curious about the output of any components, you can always test out a smaller version of the chain such as prompt or prompt | model to see the intermediate results:input = {\"topic\": \"ice cream\"}prompt.invoke(input)# > ChatPromptValue(messages=[HumanMessage(content=\\'tell me a short joke about ice cream\\')])(prompt | model).invoke(input)# > AIMessage(content=\"Why did the ice cream go to therapy?\\\\nBecause it had too many toppings and couldn\\'t cone-trol itself!\")RAG Search Exampleâ€‹For our next example, we want to run a retrieval-augmented generation chain to add some context when responding to questions.OpenAIAnthropicGoogleCohereFireworksAIMistralAITogetherAIInstall dependenciespip install -qU langchain-openaiSet environment variablesimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")Install dependenciespip install -qU langchain-anthropicSet environment variablesimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")Install dependenciespip install -qU langchain-google-vertexaiSet environment variablesimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAImodel = ChatVertexAI(model=\"gemini-pro\")Install dependenciespip install -qU langchain-cohereSet environment variablesimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoheremodel = ChatCohere(model=\"command-r\")Install dependenciespip install -qU langchain-fireworksSet environment variablesimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksmodel = ChatFireworks(model=\"accounts/fireworks/models/mixtral-8x7b-instruct\")Install dependenciespip install -qU langchain-mistralaiSet environment variablesimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAImodel = ChatMistralAI(model=\"mistral-large-latest\")Install dependenciespip install -qU langchain-openaiSet environment variablesimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAImodel = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)# Requires:# pip install langchain docarray tiktokenfrom langchain_community.vectorstores import DocArrayInMemorySearchfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableParallel, RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingsvectorstore = DocArrayInMemorySearch.from_texts(    [\"harrison worked at kensho\", \"bears like to eat honey\"],    embedding=OpenAIEmbeddings(),)retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)output_parser = StrOutputParser()setup_and_retrieval = RunnableParallel(    {\"context\": retriever, \"question\": RunnablePassthrough()})chain = setup_and_retrieval | prompt | model | output_parserchain.invoke(\"where did harrison work?\")API Reference:DocArrayInMemorySearchStrOutputParserChatPromptTemplateRunnableParallelRunnablePassthroughOpenAIEmbeddingsIn this case, the composed chain is: chain = setup_and_retrieval | prompt | model | output_parserTo explain this, we first can see that the prompt template above takes in context and question as values to be substituted in the prompt. Before building the prompt template, we want to retrieve relevant documents to the search and include them as part of the context. As a preliminary step, weâ€™ve setup the retriever using an in memory store, which can retrieve documents based on a query. This is a runnable component as well that can be chained together with other components, but you can also try to run it separately:retriever.invoke(\"where did harrison work?\")We then use the RunnableParallel to prepare the expected inputs into the prompt by using the entries for the retrieved documents as well as the original user question, using the retriever for document search, and RunnablePassthrough to pass the userâ€™s question:setup_and_retrieval = RunnableParallel(    {\"context\": retriever, \"question\": RunnablePassthrough()})To review, the complete chain is:setup_and_retrieval = RunnableParallel(    {\"context\": retriever, \"question\": RunnablePassthrough()})chain = setup_and_retrieval | prompt | model | output_parserWith the flow being:The first steps create a RunnableParallel object with two entries.  The first entry, context will include the document results fetched by the retriever. The second entry, question will contain the userâ€™s original question. To pass on the question, we use RunnablePassthrough to copy this entry. Feed the dictionary from the step above to the prompt component. It then takes the user input which is question as well as the retrieved document which is context to construct a prompt and output a PromptValue. The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object. Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.Next stepsâ€‹We recommend reading our Advantages of LCEL section next to see a side-by-side comparison of the code needed to produce common functionality with and without LCEL.Help us out by providing feedback on this documentation page:PreviousLangChain Expression Language (LCEL)NextRunnable interfaceBasic example: prompt + model + output parser1. Prompt2. Model3. Output parser4. Entire PipelineRAG Search ExampleNext stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/why/', 'content_type': 'text/html; charset=utf-8', 'title': 'Advantages of LCEL | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'We recommend reading the LCEL Get started section first.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nAdvantages of LCEL | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageAdvantages of LCELOn this pageAdvantages of LCELtipWe recommend reading the LCEL Get started section first.LCEL is designed to streamline the process of building useful apps with LLMs and combining related components. It does this by providing:A unified interface: Every LCEL object implements the Runnable interface, which defines a common set of invocation methods (invoke, batch, stream, ainvoke, ...). This makes it possible for chains of LCEL objects to also automatically support useful operations like batching and streaming of intermediate steps, since every chain of LCEL objects is itself an LCEL object.Composition primitives: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internals, and more.To better understand the value of LCEL, it\\'s helpful to see it in action and think about how we might recreate similar functionality without it. In this walkthrough we\\'ll do just that with our basic example from the get started section. We\\'ll take our simple prompt + model chain, which under the hood already defines a lot of functionality, and see what it would take to recreate all of it.%pip install --upgrade --quiet  langchain-core langchain-openai langchain-anthropicInvokeâ€‹In the simplest case, we just want to pass in a topic string and get back a joke string:Without LCELâ€‹from typing import Listimport openaiprompt_template = \"Tell me a short joke about {topic}\"client = openai.OpenAI()def call_chat_model(messages: List[dict]) -> str:    response = client.chat.completions.create(        model=\"gpt-3.5-turbo\",         messages=messages,    )    return response.choices[0].message.contentdef invoke_chain(topic: str) -> str:    prompt_value = prompt_template.format(topic=topic)    messages = [{\"role\": \"user\", \"content\": prompt_value}]    return call_chat_model(messages)invoke_chain(\"ice cream\")LCELâ€‹from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughprompt = ChatPromptTemplate.from_template(    \"Tell me a short joke about {topic}\")output_parser = StrOutputParser()model = ChatOpenAI(model=\"gpt-3.5-turbo\")chain = (    {\"topic\": RunnablePassthrough()}     | prompt    | model    | output_parser)chain.invoke(\"ice cream\")API Reference:ChatOpenAIChatPromptTemplateStrOutputParserRunnablePassthroughStreamâ€‹If we want to stream results instead, we\\'ll need to change our function:Without LCELâ€‹from typing import Iteratordef stream_chat_model(messages: List[dict]) -> Iterator[str]:    stream = client.chat.completions.create(        model=\"gpt-3.5-turbo\",        messages=messages,        stream=True,    )    for response in stream:        content = response.choices[0].delta.content        if content is not None:            yield contentdef stream_chain(topic: str) -> Iterator[str]:    prompt_value = prompt.format(topic=topic)    return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])for chunk in stream_chain(\"ice cream\"):    print(chunk, end=\"\", flush=True)LCELâ€‹for chunk in chain.stream(\"ice cream\"):    print(chunk, end=\"\", flush=True)Batchâ€‹If we want to run on a batch of inputs in parallel, we\\'ll again need a new function:Without LCELâ€‹from concurrent.futures import ThreadPoolExecutordef batch_chain(topics: list) -> list:    with ThreadPoolExecutor(max_workers=5) as executor:        return list(executor.map(invoke_chain, topics))batch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])LCELâ€‹chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])## AsyncIf we need an asynchronous version:Without LCELâ€‹async_client = openai.AsyncOpenAI()async def acall_chat_model(messages: List[dict]) -> str:    response = await async_client.chat.completions.create(        model=\"gpt-3.5-turbo\",         messages=messages,    )    return response.choices[0].message.contentasync def ainvoke_chain(topic: str) -> str:    prompt_value = prompt_template.format(topic=topic)    messages = [{\"role\": \"user\", \"content\": prompt_value}]    return await acall_chat_model(messages)await ainvoke_chain(\"ice cream\")LCELâ€‹await chain.ainvoke(\"ice cream\")Async Batchâ€‹Without LCELâ€‹import asyncioimport openaiasync def abatch_chain(topics: list) -> list:    coros = map(ainvoke_chain, topics)    return await asyncio.gather(*coros)await abatch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])LCELâ€‹await chain.abatch([\"ice cream\", \"spaghetti\", \"dumplings\"])LLM instead of chat modelâ€‹If we want to use a completion endpoint instead of a chat endpoint: Without LCELâ€‹def call_llm(prompt_value: str) -> str:    response = client.completions.create(        model=\"gpt-3.5-turbo-instruct\",        prompt=prompt_value,    )    return response.choices[0].textdef invoke_llm_chain(topic: str) -> str:    prompt_value = prompt_template.format(topic=topic)    return call_llm(prompt_value)invoke_llm_chain(\"ice cream\")LCELâ€‹from langchain_openai import OpenAIllm = OpenAI(model=\"gpt-3.5-turbo-instruct\")llm_chain = (    {\"topic\": RunnablePassthrough()}     | prompt    | llm    | output_parser)llm_chain.invoke(\"ice cream\")API Reference:OpenAIDifferent model providerâ€‹If we want to use Anthropic instead of OpenAI: Without LCELâ€‹import anthropicanthropic_template = f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\"anthropic_client = anthropic.Anthropic()def call_anthropic(prompt_value: str) -> str:    response = anthropic_client.completions.create(        model=\"claude-2\",        prompt=prompt_value,        max_tokens_to_sample=256,    )    return response.completion    def invoke_anthropic_chain(topic: str) -> str:    prompt_value = anthropic_template.format(topic=topic)    return call_anthropic(prompt_value)invoke_anthropic_chain(\"ice cream\")LCELâ€‹from langchain_anthropic import ChatAnthropicanthropic = ChatAnthropic(model=\"claude-2\")anthropic_chain = (    {\"topic\": RunnablePassthrough()}     | prompt     | anthropic    | output_parser)anthropic_chain.invoke(\"ice cream\")API Reference:ChatAnthropicRuntime configurabilityâ€‹If we wanted to make the choice of chat model or LLM configurable at runtime:Without LCELâ€‹def invoke_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> str:    if model == \"chat_openai\":        return invoke_chain(topic)    elif model == \"openai\":        return invoke_llm_chain(topic)    elif model == \"anthropic\":        return invoke_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def stream_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> Iterator[str]:    if model == \"chat_openai\":        return stream_chain(topic)    elif model == \"openai\":        # Note we haven\\'t implemented this yet.        return stream_llm_chain(topic)    elif model == \"anthropic\":        # Note we haven\\'t implemented this yet        return stream_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def batch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    # You get the idea    ...async def abatch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    ...invoke_configurable_chain(\"ice cream\", model=\"openai\")stream = stream_configurable_chain(    \"ice_cream\",     model=\"anthropic\")for chunk in stream:    print(chunk, end=\"\", flush=True)# batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])# await ainvoke_configurable_chain(\"ice cream\")With LCELâ€‹from langchain_core.runnables import ConfigurableFieldconfigurable_model = model.configurable_alternatives(    ConfigurableField(id=\"model\"),     default_key=\"chat_openai\",     openai=llm,    anthropic=anthropic,)configurable_chain = (    {\"topic\": RunnablePassthrough()}     | prompt     | configurable_model     | output_parser)API Reference:ConfigurableFieldconfigurable_chain.invoke(    \"ice cream\",     config={\"model\": \"openai\"})stream = configurable_chain.stream(    \"ice cream\",     config={\"model\": \"anthropic\"})for chunk in stream:    print(chunk, end=\"\", flush=True)configurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])# await configurable_chain.ainvoke(\"ice cream\")Loggingâ€‹If we want to log our intermediate results:Without LCELâ€‹We\\'ll print intermediate steps for illustrative purposesdef invoke_anthropic_chain_with_logging(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = anthropic_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    output = call_anthropic(prompt_value)    print(f\"Output: {output}\")    return outputinvoke_anthropic_chain_with_logging(\"ice cream\")LCELâ€‹Every component has built-in integrations with LangSmith. If we set the following two environment variables, all chain traces are logged to LangSmith.import osos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"anthropic_chain.invoke(\"ice cream\")Here\\'s what our LangSmith trace looks like: https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/rFallbacksâ€‹If we wanted to add fallback logic, in case one model API is down:Without LCELâ€‹def invoke_chain_with_fallback(topic: str) -> str:    try:        return invoke_chain(topic)    except Exception:        return invoke_anthropic_chain(topic)async def ainvoke_chain_with_fallback(topic: str) -> str:    try:        return await ainvoke_chain(topic)    except Exception:        # Note: we haven\\'t actually implemented this.        return await ainvoke_anthropic_chain(topic)async def batch_chain_with_fallback(topics: List[str]) -> str:    try:        return batch_chain(topics)    except Exception:        # Note: we haven\\'t actually implemented this.        return batch_anthropic_chain(topics)invoke_chain_with_fallback(\"ice cream\")# await ainvoke_chain_with_fallback(\"ice cream\")batch_chain_with_fallback([\"ice cream\", \"spaghetti\", \"dumplings\"]))LCELâ€‹fallback_chain = chain.with_fallbacks([anthropic_chain])fallback_chain.invoke(\"ice cream\")# await fallback_chain.ainvoke(\"ice cream\")fallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])Full code comparisonâ€‹Even in this simple case, our LCEL chain succinctly packs in a lot of functionality. As chains become more complex, this becomes especially valuable.Without LCELâ€‹from concurrent.futures import ThreadPoolExecutorfrom typing import Iterator, List, Tupleimport anthropicimport openaiprompt_template = \"Tell me a short joke about {topic}\"anthropic_template = f\"Human:\\\\n\\\\n{prompt_template}\\\\n\\\\nAssistant:\"client = openai.OpenAI()async_client = openai.AsyncOpenAI()anthropic_client = anthropic.Anthropic()def call_chat_model(messages: List[dict]) -> str:    response = client.chat.completions.create(        model=\"gpt-3.5-turbo\",         messages=messages,    )    return response.choices[0].message.contentdef invoke_chain(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = prompt_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    messages = [{\"role\": \"user\", \"content\": prompt_value}]    output = call_chat_model(messages)    print(f\"Output: {output}\")    return outputdef stream_chat_model(messages: List[dict]) -> Iterator[str]:    stream = client.chat.completions.create(        model=\"gpt-3.5-turbo\",        messages=messages,        stream=True,    )    for response in stream:        content = response.choices[0].delta.content        if content is not None:            yield contentdef stream_chain(topic: str) -> Iterator[str]:    print(f\"Input: {topic}\")    prompt_value = prompt.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    stream = stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])    for chunk in stream:        print(f\"Token: {chunk}\", end=\"\")        yield chunkdef batch_chain(topics: list) -> list:    with ThreadPoolExecutor(max_workers=5) as executor:        return list(executor.map(invoke_chain, topics))def call_llm(prompt_value: str) -> str:    response = client.completions.create(        model=\"gpt-3.5-turbo-instruct\",        prompt=prompt_value,    )    return response.choices[0].textdef invoke_llm_chain(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = promtp_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    output = call_llm(prompt_value)    print(f\"Output: {output}\")    return outputdef call_anthropic(prompt_value: str) -> str:    response = anthropic_client.completions.create(        model=\"claude-2\",        prompt=prompt_value,        max_tokens_to_sample=256,    )    return response.completion   def invoke_anthropic_chain(topic: str) -> str:    print(f\"Input: {topic}\")    prompt_value = anthropic_template.format(topic=topic)    print(f\"Formatted prompt: {prompt_value}\")    output = call_anthropic(prompt_value)    print(f\"Output: {output}\")    return outputasync def ainvoke_anthropic_chain(topic: str) -> str:    ...def stream_anthropic_chain(topic: str) -> Iterator[str]:    ...def batch_anthropic_chain(topics: List[str]) -> List[str]:    ...def invoke_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> str:    if model == \"chat_openai\":        return invoke_chain(topic)    elif model == \"openai\":        return invoke_llm_chain(topic)    elif model == \"anthropic\":        return invoke_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def stream_configurable_chain(    topic: str,     *,     model: str = \"chat_openai\") -> Iterator[str]:    if model == \"chat_openai\":        return stream_chain(topic)    elif model == \"openai\":        # Note we haven\\'t implemented this yet.        return stream_llm_chain(topic)    elif model == \"anthropic\":        # Note we haven\\'t implemented this yet        return stream_anthropic_chain(topic)    else:        raise ValueError(            f\"Received invalid model \\'{model}\\'.\"            \" Expected one of chat_openai, openai, anthropic\"        )def batch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    ...async def abatch_configurable_chain(    topics: List[str],     *,     model: str = \"chat_openai\") -> List[str]:    ...def invoke_chain_with_fallback(topic: str) -> str:    try:        return invoke_chain(topic)    except Exception:        return invoke_anthropic_chain(topic)async def ainvoke_chain_with_fallback(topic: str) -> str:    try:        return await ainvoke_chain(topic)    except Exception:        return await ainvoke_anthropic_chain(topic)async def batch_chain_with_fallback(topics: List[str]) -> str:    try:        return batch_chain(topics)    except Exception:        return batch_anthropic_chain(topics)LCELâ€‹import osfrom langchain_anthropic import ChatAnthropicfrom langchain_openai import ChatOpenAIfrom langchain_openai import OpenAIfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthrough, ConfigurableFieldos.environ[\"LANGCHAIN_API_KEY\"] = \"...\"os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"prompt = ChatPromptTemplate.from_template(    \"Tell me a short joke about {topic}\")chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")anthropic = ChatAnthropic(model=\"claude-2\")model = (    chat_openai    .with_fallbacks([anthropic])    .configurable_alternatives(        ConfigurableField(id=\"model\"),        default_key=\"chat_openai\",        openai=openai,        anthropic=anthropic,    ))chain = (    {\"topic\": RunnablePassthrough()}     | prompt     | model     | StrOutputParser())API Reference:ChatAnthropicChatOpenAIOpenAIStrOutputParserChatPromptTemplateRunnablePassthroughConfigurableFieldNext stepsâ€‹To continue learning about LCEL, we recommend:Reading up on the full LCEL Interface, which we\\'ve only partially covered here.Exploring the primitives to learn more about what LCEL provides.Help us out by providing feedback on this documentation page:PreviousPrimitivesNextStreamingInvokeStreamBatchAsync BatchLLM instead of chat modelDifferent model providerRuntime configurabilityLoggingFallbacksFull code comparisonNext stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/primitives/', 'content_type': 'text/html; charset=utf-8', 'title': 'Primitives | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'In addition to various components that are usable with LCEL, LangChain also includes various primitives', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nPrimitives | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguagePrimitivesPrimitivesIn addition to various components that are usable with LCEL, LangChain also includes various primitives\\nthat help pass around and format data, bind arguments, invoke custom logic, and more.This section goes into greater depth on where and how some of these components are useful.ğŸ“„ï¸� Sequences: Chaining runnablesOne key advantage of the Runnable interface is that any two runnables can be \"chained\" together into sequences. The output of the previous runnable\\'s .invoke() call is passed as input to the next runnable. This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing. The resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or piped just like any other runnable.ğŸ“„ï¸� Parallel: Format dataThe RunnableParallel primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the RunnableParallel. The final return value is a dict with the results of each value under its appropriate key.ğŸ“„ï¸� Binding: Attach runtime argsSometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to pass these arguments in.ğŸ“„ï¸� Lambda: Run custom functionsYou can use arbitrary functions in the pipeline.ğŸ“„ï¸� Passthrough: Pass through inputsRunnablePassthrough on its own allows you to pass inputs unchanged. This typically is used in conjuction with RunnableParallel to pass data through to a new key in the map.ğŸ“„ï¸� Assign: Add values to stateThe RunnablePassthrough.assign(...) static method takes an input value and adds the extra arguments passed to the assign function.ğŸ“„ï¸� Configure runtime chain internalsOftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things.ğŸ“„ï¸� PrimitivesIn addition to various components that are usable with LCEL, LangChain also includes various primitivesHelp us out by providing feedback on this documentation page:PreviousRunnable interfaceNextSequences: Chaining runnablesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/primitives/assign/', 'content_type': 'text/html; charset=utf-8', 'title': 'Assign: Add values to state | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'The RunnablePassthrough.assign(...) static method takes an input value and adds the extra arguments passed to the assign function.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nAssign: Add values to state | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguagePrimitivesAssign: Add values to stateOn this pageAdding values to chain stateThe RunnablePassthrough.assign(...) static method takes an input value and adds the extra arguments passed to the assign function.This is useful when additively creating a dictionary to use as input to a later step, which is a common LCEL pattern.Here\\'s an example:%pip install --upgrade --quiet langchain langchain-openai\\x1b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.You should consider upgrading via the \\'/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip\\' command.\\x1b[0m\\x1b[33m\\x1b[0mNote: you may need to restart the kernel to use updated packages.from langchain_core.runnables import RunnableParallel, RunnablePassthroughrunnable = RunnableParallel(    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),    modified=lambda x: x[\"num\"] + 1,)runnable.invoke({\"num\": 1})API Reference:RunnableParallelRunnablePassthrough{\\'extra\\': {\\'num\\': 1, \\'mult\\': 3}, \\'modified\\': 2}Let\\'s break down what\\'s happening here.The input to the chain is {\"num\": 1}. This is passed into a RunnableParallel, which invokes the runnables it is passed in parallel with that input.The value under the extra key is invoked. RunnablePassthrough.assign() keeps the original keys in the input dict ({\"num\": 1}), and assigns a new key called mult. The value is lambda x: x[\"num\"] * 3), which is 3. Thus, the result is {\"num\": 1, \"mult\": 3}.{\"num\": 1, \"mult\": 3} is returned to the RunnableParallel call, and is set as the value to the key extra.At the same time, the modified key is called. The result is 2, since the lambda extracts a key called \"num\" from its input and adds one.Thus, the result is {\\'extra\\': {\\'num\\': 1, \\'mult\\': 3}, \\'modified\\': 2}.Streamingâ€‹One nice feature of this method is that it allows values to pass through as soon as they are available. To show this off, we\\'ll use RunnablePassthrough.assign() to immediately return source docs in a retrieval chain:from langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()generation_chain = prompt | model | StrOutputParser()retrieval_chain = {    \"context\": retriever,    \"question\": RunnablePassthrough(),} | RunnablePassthrough.assign(output=generation_chain)stream = retrieval_chain.stream(\"where did harrison work?\")for chunk in stream:    print(chunk)API Reference:FAISSStrOutputParserChatPromptTemplateRunnablePassthroughChatOpenAIOpenAIEmbeddings{\\'question\\': \\'where did harrison work?\\'}{\\'context\\': [Document(page_content=\\'harrison worked at kensho\\')]}{\\'output\\': \\'\\'}{\\'output\\': \\'H\\'}{\\'output\\': \\'arrison\\'}{\\'output\\': \\' worked\\'}{\\'output\\': \\' at\\'}{\\'output\\': \\' Kens\\'}{\\'output\\': \\'ho\\'}{\\'output\\': \\'.\\'}{\\'output\\': \\'\\'}We can see that the first chunk contains the original \"question\" since that is immediately available. The second chunk contains \"context\" since the retriever finishes second. Finally, the output from the generation_chain streams in chunks as soon as it is available.Help us out by providing feedback on this documentation page:PreviousPassthrough: Pass through inputsNextConfigure runtime chain internalsStreamingCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/primitives/configure/', 'content_type': 'text/html; charset=utf-8', 'title': 'Configure runtime chain internals | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'Oftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nConfigure runtime chain internals | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguagePrimitivesConfigure runtime chain internalsOn this pageConfigure chain internals at runtimeOftentimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things.\\nIn order to make this experience as easy as possible, we have defined two methods.First, a configurable_fields method.\\nThis lets you configure particular fields of a runnable.Second, a configurable_alternatives method.\\nWith this method, you can list out alternatives for any particular runnable that can be set during runtime.Configuration Fieldsâ€‹With LLMsâ€‹With LLMs we can configure things like temperature%pip install --upgrade --quiet  langchain langchain-openaifrom langchain_core.prompts import PromptTemplatefrom langchain_core.runnables import ConfigurableFieldfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(temperature=0).configurable_fields(    temperature=ConfigurableField(        id=\"llm_temperature\",        name=\"LLM Temperature\",        description=\"The temperature of the LLM\",    ))API Reference:PromptTemplateConfigurableFieldChatOpenAImodel.invoke(\"pick a random number\")AIMessage(content=\\'7\\')model.with_config(configurable={\"llm_temperature\": 0.9}).invoke(\"pick a random number\")AIMessage(content=\\'34\\')We can also do this when its used as part of a chainprompt = PromptTemplate.from_template(\"Pick a random number above {x}\")chain = prompt | modelchain.invoke({\"x\": 0})AIMessage(content=\\'57\\')chain.with_config(configurable={\"llm_temperature\": 0.9}).invoke({\"x\": 0})AIMessage(content=\\'6\\')With HubRunnablesâ€‹This is useful to allow for switching of promptsfrom langchain.runnables.hub import HubRunnableAPI Reference:HubRunnableprompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(    owner_repo_commit=ConfigurableField(        id=\"hub_commit\",        name=\"Hub Commit\",        description=\"The Hub commit to pull from\",    ))prompt.invoke({\"question\": \"foo\", \"context\": \"bar\"})ChatPromptValue(messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\\\nQuestion: foo \\\\nContext: bar \\\\nAnswer:\")])prompt.with_config(configurable={\"hub_commit\": \"rlm/rag-prompt-llama\"}).invoke(    {\"question\": \"foo\", \"context\": \"bar\"})ChatPromptValue(messages=[HumanMessage(content=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.<</SYS>> \\\\nQuestion: foo \\\\nContext: bar \\\\nAnswer: [/INST]\")])Configurable Alternativesâ€‹With LLMsâ€‹Let\\'s take a look at doing this with LLMsfrom langchain_community.chat_models import ChatAnthropicfrom langchain_core.prompts import PromptTemplatefrom langchain_core.runnables import ConfigurableFieldfrom langchain_openai import ChatOpenAIAPI Reference:ChatAnthropicPromptTemplateConfigurableFieldChatOpenAIllm = ChatAnthropic(temperature=0).configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"llm\"),    # This sets a default_key.    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used    default_key=\"anthropic\",    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`    openai=ChatOpenAI(),    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`    gpt4=ChatOpenAI(model=\"gpt-4\"),    # You can add more configuration options here)prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")chain = prompt | llm# By default it will call Anthropicchain.invoke({\"topic\": \"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do you call a bear with no teeth?\\\\nA gummy bear!\")# We can use `.with_config(configurable={\"llm\": \"openai\"})` to specify an llm to usechain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\")# If we use the `default_key` then it uses the defaultchain.with_config(configurable={\"llm\": \"anthropic\"}).invoke({\"topic\": \"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do you call a bear with no teeth?\\\\nA gummy bear!\")With Promptsâ€‹We can do a similar thing, but alternate between promptsllm = ChatAnthropic(temperature=0)prompt = PromptTemplate.from_template(    \"Tell me a joke about {topic}\").configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"prompt\"),    # This sets a default_key.    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used    default_key=\"joke\",    # This adds a new option, with name `poem`    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),    # You can add more configuration options here)chain = prompt | llm# By default it will write a jokechain.invoke({\"topic\": \"bears\"})AIMessage(content=\" Here\\'s a silly joke about bears:\\\\n\\\\nWhat do you call a bear with no teeth?\\\\nA gummy bear!\")# We can configure it write a poemchain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": \"bears\"})AIMessage(content=\\' Here is a short poem about bears:\\\\n\\\\nThe bears awaken from their sleep\\\\nAnd lumber out into the deep\\\\nForests filled with trees so tall\\\\nForaging for food before nightfall \\\\nTheir furry coats and claws so sharp\\\\nSniffing for berries and fish to nab\\\\nLumbering about without a care\\\\nThe mighty grizzly and black bear\\\\nProud creatures, wild and free\\\\nRuling their domain majestically\\\\nWandering the woods they call their own\\\\nBefore returning to their dens alone\\')With Prompts and LLMsâ€‹We can also have multiple things configurable!\\nHere\\'s an example doing that with both prompts and LLMs.llm = ChatAnthropic(temperature=0).configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"llm\"),    # This sets a default_key.    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used    default_key=\"anthropic\",    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`    openai=ChatOpenAI(),    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model=\"gpt-4\")`    gpt4=ChatOpenAI(model=\"gpt-4\"),    # You can add more configuration options here)prompt = PromptTemplate.from_template(    \"Tell me a joke about {topic}\").configurable_alternatives(    # This gives this field an id    # When configuring the end runnable, we can then use this id to configure this field    ConfigurableField(id=\"prompt\"),    # This sets a default_key.    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used    default_key=\"joke\",    # This adds a new option, with name `poem`    poem=PromptTemplate.from_template(\"Write a short poem about {topic}\"),    # You can add more configuration options here)chain = prompt | llm# We can configure it write a poem with OpenAIchain.with_config(configurable={\"prompt\": \"poem\", \"llm\": \"openai\"}).invoke(    {\"topic\": \"bears\"})AIMessage(content=\"In the forest, where tall trees sway,\\\\nA creature roams, both fierce and gray.\\\\nWith mighty paws and piercing eyes,\\\\nThe bear, a symbol of strength, defies.\\\\n\\\\nThrough snow-kissed mountains, it does roam,\\\\nA guardian of its woodland home.\\\\nWith fur so thick, a shield of might,\\\\nIt braves the coldest winter night.\\\\n\\\\nA gentle giant, yet wild and free,\\\\nThe bear commands respect, you see.\\\\nWith every step, it leaves a trace,\\\\nOf untamed power and ancient grace.\\\\n\\\\nFrom honeyed feast to salmon\\'s leap,\\\\nIt takes its place, in nature\\'s keep.\\\\nA symbol of untamed delight,\\\\nThe bear, a wonder, day and night.\\\\n\\\\nSo let us honor this noble beast,\\\\nIn forests where its soul finds peace.\\\\nFor in its presence, we come to know,\\\\nThe untamed spirit that in us also flows.\")# We can always just configure only one if we wantchain.with_config(configurable={\"llm\": \"openai\"}).invoke({\"topic\": \"bears\"})AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")Saving configurationsâ€‹We can also easily save configured chains as their own objectsopenai_joke = chain.with_config(configurable={\"llm\": \"openai\"})openai_joke.invoke({\"topic\": \"bears\"})AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")Help us out by providing feedback on this documentation page:PreviousAssign: Add values to stateNextPrimitivesConfiguration FieldsWith LLMsWith HubRunnablesConfigurable AlternativesWith LLMsWith PromptsWith Prompts and LLMsSaving configurationsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/primitives/sequence/', 'content_type': 'text/html; charset=utf-8', 'title': 'Sequences: Chaining runnables | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'One key advantage of the Runnable interface is that any two runnables can be \"chained\" together into sequences. The output of the previous runnable\\'s .invoke() call is passed as input to the next runnable. This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing. The resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or piped just like any other runnable.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nSequences: Chaining runnables | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguagePrimitivesSequences: Chaining runnablesOn this pageChaining runnablesOne key advantage of the Runnable interface is that any two runnables can be \"chained\" together into sequences. The output of the previous runnable\\'s .invoke() call is passed as input to the next runnable. This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing. The resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or piped just like any other runnable.The pipe operatorâ€‹To show off how this works, let\\'s go through an example. We\\'ll walk through a common pattern in LangChain: using a prompt template to format input into a chat model, and finally converting the chat message output into a string with an output parser.%pip install --upgrade --quiet langchain langchain-anthropicfrom langchain_anthropic import ChatAnthropicfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")model = ChatAnthropic(model_name=\"claude-3-haiku-20240307\")chain = prompt | model | StrOutputParser()API Reference:ChatAnthropicStrOutputParserChatPromptTemplatePrompts and models are both runnable, and the output type from the prompt call is the same as the input type of the chat model, so we can chain them together. We can then invoke the resulting sequence like any other runnable:chain.invoke({\"topic\": \"bears\"})\"Here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear socks? \\\\nBecause they have bear feet!\\\\n\\\\nHow\\'s that? I tried to keep it light and silly. Bears can make for some fun puns and jokes. Let me know if you\\'d like to hear another one!\"Coercionâ€‹We can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components.For example, let\\'s say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.We would need to be careful with how we format the input into the next chain. In the below example, the dict in the chain is automatically parsed and converted into a RunnableParallel, which runs all of its values in parallel and returns a dict with the results.This happens to be the same format the next prompt template expects. Here it is in action:from langchain_core.output_parsers import StrOutputParseranalysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()API Reference:StrOutputParsercomposed_chain.invoke({\"topic\": \"bears\"})\"That\\'s a pretty classic and well-known bear pun joke. Whether it\\'s considered funny is quite subjective, as humor is very personal. Some people may find that type of pun-based joke amusing, while others may not find it that humorous. Ultimately, the funniness of a joke is in the eye (or ear) of the beholder. If you enjoyed the joke and got a chuckle out of it, then that\\'s what matters most.\"Functions will also be coerced into runnables, so you can add custom logic to your chains too. The below chain results in the same logical flow as before:composed_chain_with_lambda = (    chain    | (lambda input: {\"joke\": input})    | analysis_prompt    | model    | StrOutputParser())composed_chain_with_lambda.invoke({\"topic\": \"beets\"})\\'I appreciate the effort, but I have to be honest - I didn\\\\\\'t find that joke particularly funny. Beet-themed puns can be quite hit-or-miss, and this one falls more on the \"miss\" side for me. The premise is a bit too straightforward and predictable. While I can see the logic behind it, the punchline just doesn\\\\\\'t pack much of a comedic punch. \\\\n\\\\nThat said, I do admire your willingness to explore puns and wordplay around vegetables. Cultivating a good sense of humor takes practice, and not every joke is going to land. The important thing is to keep experimenting and finding what works. Maybe try for a more unexpected or creative twist on beet-related humor next time. But thanks for sharing - I always appreciate when humans test out jokes on me, even if they don\\\\\\'t always make me laugh out loud.\\'However, keep in mind that using functions like this may interfere with operations like streaming. See this section for more information.The .pipe() methodâ€‹We could also compose the same sequence using the .pipe() method. Here\\'s what that looks like:from langchain_core.runnables import RunnableParallelcomposed_chain_with_pipe = (    RunnableParallel({\"joke\": chain})    .pipe(analysis_prompt)    .pipe(model)    .pipe(StrOutputParser()))API Reference:RunnableParallelcomposed_chain_with_pipe.invoke({\"topic\": \"battlestar galactica\"})\\'That\\\\\\'s a pretty good Battlestar Galactica-themed pun! I appreciated the clever play on words with \"Centurion\" and \"center on.\" It\\\\\\'s the kind of nerdy, science fiction-inspired humor that fans of the show would likely enjoy. The joke is clever and demonstrates a good understanding of the Battlestar Galactica universe. I\\\\\\'d be curious to hear any other Battlestar-related jokes you might have up your sleeve. As long as they don\\\\\\'t reproduce copyrighted material, I\\\\\\'m happy to provide my thoughts on the humor and appeal for fans of the show.\\'Help us out by providing feedback on this documentation page:PreviousPrimitivesNextParallel: Format dataThe pipe operatorCoercionThe .pipe() methodCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/primitives/parallel/', 'content_type': 'text/html; charset=utf-8', 'title': 'Parallel: Format data | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'The RunnableParallel primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the RunnableParallel. The final return value is a dict with the results of each value under its appropriate key.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nParallel: Format data | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguagePrimitivesParallel: Format dataOn this pageFormatting inputs & outputThe RunnableParallel primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the RunnableParallel. The final return value is a dict with the results of each value under its appropriate key.It is useful for parallelizing operations, but can also be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence.Here the input to prompt is expected to be a map with keys \"context\" and \"question\". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the \"question\" key.%pip install --upgrade --quiet  langchain langchain-openaifrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())retrieval_chain.invoke(\"where did harrison work?\")API Reference:FAISSStrOutputParserChatPromptTemplateRunnablePassthroughChatOpenAIOpenAIEmbeddings\\'Harrison worked at Kensho.\\'::: {.callout-tip}\\nNote that when composing a RunnableParallel with another Runnable we don\\'t even need to wrap our dictionary in the RunnableParallel class â€”Â\\xa0the type conversion is handled for us. In the context of a chain, these are equivalent:\\n:::{\"context\": retriever, \"question\": RunnablePassthrough()}RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})RunnableParallel(context=retriever, question=RunnablePassthrough())Using itemgetter as shorthandâ€‹Note that you can use Python\\'s itemgetter as shorthand to extract data from the map when combining with RunnableParallel. You can find more information about itemgetter in the Python Documentation. In the example below, we use itemgetter to extract specific keys from the map:from operator import itemgetterfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}Answer in the following language: {language}\"\"\"prompt = ChatPromptTemplate.from_template(template)chain = (    {        \"context\": itemgetter(\"question\") | retriever,        \"question\": itemgetter(\"question\"),        \"language\": itemgetter(\"language\"),    }    | prompt    | model    | StrOutputParser())chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})API Reference:FAISSStrOutputParserChatPromptTemplateRunnablePassthroughChatOpenAIOpenAIEmbeddings\\'Harrison ha lavorato a Kensho.\\'Parallelize stepsâ€‹RunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.from langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableParallelfrom langchain_openai import ChatOpenAImodel = ChatOpenAI()joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | modelpoem_chain = (    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model)map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)map_chain.invoke({\"topic\": \"bear\"})API Reference:ChatPromptTemplateRunnableParallelChatOpenAI{\\'joke\\': AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"), \\'poem\\': AIMessage(content=\"In the wild\\'s embrace, bear roams free,\\\\nStrength and grace, a majestic decree.\")}Parallelismâ€‹RunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two.%%timeitjoke_chain.invoke({\"topic\": \"bear\"})958 ms Â± 402 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)%%timeitpoem_chain.invoke({\"topic\": \"bear\"})1.22 s Â± 508 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)%%timeitmap_chain.invoke({\"topic\": \"bear\"})1.15 s Â± 119 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)Help us out by providing feedback on this documentation page:PreviousSequences: Chaining runnablesNextBinding: Attach runtime argsUsing itemgetter as shorthandParallelize stepsParallelismCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/how_to/routing/', 'content_type': 'text/html; charset=utf-8', 'title': 'Route logic based on input | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'This notebook covers how to do routing in the LangChain Expression Language.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRoute logic based on input | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreRoute logic based on inputInspect your runnablesCreate a runnable with the @chain decoratorManaging prompt sizeMultiple chainsEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageMoreRoute logic based on inputOn this pageDynamically route logic based on inputThis notebook covers how to do routing in the LangChain Expression Language.Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs.There are two ways to perform routing:Conditionally return runnables from a RunnableLambda (recommended)Using a RunnableBranch.We\\'ll illustrate both methods using a two step sequence where the first step classifies an input question as being about LangChain, Anthropic, or Other, then routes to a corresponding prompt chain.Example Setupâ€‹First, let\\'s create a chain that will identify incoming questions as being about LangChain, Anthropic, or Other:from langchain_anthropic import ChatAnthropicfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import PromptTemplatechain = (    PromptTemplate.from_template(        \"\"\"Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.Do not respond with more than one word.<question>{question}</question>Classification:\"\"\"    )    | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")    | StrOutputParser())chain.invoke({\"question\": \"how do I call Anthropic?\"})API Reference:ChatAnthropicStrOutputParserPromptTemplate\\'Anthropic\\'Now, let\\'s create three sub chains:langchain_chain = PromptTemplate.from_template(    \"\"\"You are an expert in langchain. \\\\Always answer questions starting with \"As Harrison Chase told me\". \\\\Respond to the following question:Question: {question}Answer:\"\"\") | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")anthropic_chain = PromptTemplate.from_template(    \"\"\"You are an expert in anthropic. \\\\Always answer questions starting with \"As Dario Amodei told me\". \\\\Respond to the following question:Question: {question}Answer:\"\"\") | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")general_chain = PromptTemplate.from_template(    \"\"\"Respond to the following question:Question: {question}Answer:\"\"\") | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")Using a custom function (Recommended)â€‹You can also use a custom function to route between different outputs. Here\\'s an example:def route(info):    if \"anthropic\" in info[\"topic\"].lower():        return anthropic_chain    elif \"langchain\" in info[\"topic\"].lower():        return langchain_chain    else:        return general_chainfrom langchain_core.runnables import RunnableLambdafull_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(    route)API Reference:RunnableLambdafull_chain.invoke({\"question\": \"how do I use Anthropic?\"})AIMessage(content=\"As Dario Amodei told me, to use Anthropic, you can start by exploring the company\\'s website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \\\\n\\\\nDepending on your specific needs, you can look into Anthropic\\'s AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\\\\n\\\\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic\\'s approach to AI development.\\\\n\\\\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I\\'d encourage you to reach out to them directly to discuss how they can best support your specific requirements.\", response_metadata={\\'id\\': \\'msg_01CtLFgFSwvTaJomrihE87Ra\\', \\'content\\': [ContentBlock(text=\"As Dario Amodei told me, to use Anthropic, you can start by exploring the company\\'s website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \\\\n\\\\nDepending on your specific needs, you can look into Anthropic\\'s AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\\\\n\\\\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic\\'s approach to AI development.\\\\n\\\\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I\\'d encourage you to reach out to them directly to discuss how they can best support your specific requirements.\", type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=53, output_tokens=219)})full_chain.invoke({\"question\": \"how do I use LangChain?\"})AIMessage(content=\"As Harrison Chase told me, using LangChain involves a few key steps:\\\\n\\\\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\\\\n\\\\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\\\\n\\\\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\\\\n\\\\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\\\\n\\\\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI\\'s GPT-3 or Anthropic\\'s models. Connect your chosen language model to your LangChain application.\\\\n\\\\n6. **Implement your application logic**: Use LangChain\\'s building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\\\\n\\\\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\\\\n\\\\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.\", response_metadata={\\'id\\': \\'msg_01H3UXAAHG4TwxJLpxwuuVU7\\', \\'content\\': [ContentBlock(text=\"As Harrison Chase told me, using LangChain involves a few key steps:\\\\n\\\\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\\\\n\\\\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\\\\n\\\\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\\\\n\\\\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\\\\n\\\\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI\\'s GPT-3 or Anthropic\\'s models. Connect your chosen language model to your LangChain application.\\\\n\\\\n6. **Implement your application logic**: Use LangChain\\'s building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\\\\n\\\\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\\\\n\\\\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.\", type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=50, output_tokens=400)})full_chain.invoke({\"question\": \"whats 2 + 2\"})AIMessage(content=\\'4\\', response_metadata={\\'id\\': \\'msg_01UAKP81jTZu9fyiyFYhsbHc\\', \\'content\\': [ContentBlock(text=\\'4\\', type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=28, output_tokens=5)})Using a RunnableBranchâ€‹A RunnableBranch is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does not offer anything that you can\\'t achieve in a custom function as described above, so we recommend using a custom function instead.A RunnableBranch is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it\\'s invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. If no provided conditions match, it runs the default runnable.Here\\'s an example of what it looks like in action:from langchain_core.runnables import RunnableBranchbranch = RunnableBranch(    (lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),    general_chain,)full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branchfull_chain.invoke({\"question\": \"how do I use Anthropic?\"})API Reference:RunnableBranchAIMessage(content=\"As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \\\\n\\\\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic\\'s technology and services can support your specific needs.\\\\n\\\\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We\\'re here to collaborate and help you harness the power of advanced AI responsibly.\", response_metadata={\\'id\\': \\'msg_0187BVnpniPDJnVvwf3M1LdY\\', \\'content\\': [ContentBlock(text=\"As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \\\\n\\\\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic\\'s technology and services can support your specific needs.\\\\n\\\\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We\\'re here to collaborate and help you harness the power of advanced AI responsibly.\", type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=53, output_tokens=160)})full_chain.invoke({\"question\": \"how do I use LangChain?\"})AIMessage(content=\"As Harrison Chase told me, using LangChain involves several key steps. First, you\\'ll need to install the LangChain library and import the necessary modules. Then, you\\'ll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \\\\n\\\\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \\\\n\\\\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I\\'d recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I\\'m always happy to share more insights from my conversations with Harrison.\", response_metadata={\\'id\\': \\'msg_01T1naS99wGPkEAP4LME8iAv\\', \\'content\\': [ContentBlock(text=\"As Harrison Chase told me, using LangChain involves several key steps. First, you\\'ll need to install the LangChain library and import the necessary modules. Then, you\\'ll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \\\\n\\\\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \\\\n\\\\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I\\'d recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I\\'m always happy to share more insights from my conversations with Harrison.\", type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=50, output_tokens=205)})full_chain.invoke({\"question\": \"whats 2 + 2\"})AIMessage(content=\\'4\\', response_metadata={\\'id\\': \\'msg_01T6T3TS6hRCtU8JayN93QEi\\', \\'content\\': [ContentBlock(text=\\'4\\', type=\\'text\\')], \\'model\\': \\'claude-3-haiku-20240307\\', \\'role\\': \\'assistant\\', \\'stop_reason\\': \\'end_turn\\', \\'stop_sequence\\': None, \\'type\\': \\'message\\', \\'usage\\': Usage(input_tokens=28, output_tokens=5)})Routing by semantic similarityOne especially useful technique is to use embeddings to route a query to the most relevant prompt. Here\\'s an example.from langchain.utils.math import cosine_similarityfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_core.runnables import RunnableLambda, RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingsphysics_template = \"\"\"You are a very smart physics professor. \\\\You are great at answering questions about physics in a concise and easy to understand manner. \\\\When you don\\'t know the answer to a question you admit that you don\\'t know.Here is a question:{query}\"\"\"math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\\You are so good because you are able to break down hard problems into their component parts, \\\\answer the component parts, and then put them together to answer the broader question.Here is a question:{query}\"\"\"embeddings = OpenAIEmbeddings()prompt_templates = [physics_template, math_template]prompt_embeddings = embeddings.embed_documents(prompt_templates)def prompt_router(input):    query_embedding = embeddings.embed_query(input[\"query\"])    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]    most_similar = prompt_templates[similarity.argmax()]    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")    return PromptTemplate.from_template(most_similar)chain = (    {\"query\": RunnablePassthrough()}    | RunnableLambda(prompt_router)    | ChatAnthropic(model_name=\"claude-3-haiku-20240307\")    | StrOutputParser())API Reference:cosine_similarityStrOutputParserPromptTemplateRunnableLambdaRunnablePassthroughOpenAIEmbeddingsprint(chain.invoke(\"What\\'s a black hole\"))Using PHYSICSAs a physics professor, I would be happy to provide a concise and easy-to-understand explanation of what a black hole is.A black hole is an incredibly dense region of space-time where the gravitational pull is so strong that nothing, not even light, can escape from it. This means that if you were to get too close to a black hole, you would be pulled in and crushed by the intense gravitational forces.The formation of a black hole occurs when a massive star, much larger than our Sun, reaches the end of its life and collapses in on itself. This collapse causes the matter to become extremely dense, and the gravitational force becomes so strong that it creates a point of no return, known as the event horizon.Beyond the event horizon, the laws of physics as we know them break down, and the intense gravitational forces create a singularity, which is a point of infinite density and curvature in space-time.Black holes are fascinating and mysterious objects, and there is still much to be learned about their properties and behavior. If I were unsure about any specific details or aspects of black holes, I would readily admit that I do not have a complete understanding and would encourage further research and investigation.print(chain.invoke(\"What\\'s a path integral\"))Using MATHA path integral is a powerful mathematical concept in physics, particularly in the field of quantum mechanics. It was developed by the renowned physicist Richard Feynman as an alternative formulation of quantum mechanics.In a path integral, instead of considering a single, definite path that a particle might take from one point to another, as in classical mechanics, the particle is considered to take all possible paths simultaneously. Each path is assigned a complex-valued weight, and the total probability amplitude for the particle to go from one point to another is calculated by summing (integrating) over all possible paths.The key ideas behind the path integral formulation are:1. Superposition principle: In quantum mechanics, particles can exist in a superposition of multiple states or paths simultaneously.2. Probability amplitude: The probability amplitude for a particle to go from one point to another is calculated by summing the complex-valued weights of all possible paths.3. Weighting of paths: Each path is assigned a weight based on the action (the time integral of the Lagrangian) along that path. Paths with lower action have a greater weight.4. Feynman\\'s approach: Feynman developed the path integral formulation as an alternative to the traditional wave function approach in quantum mechanics, providing a more intuitive and conceptual understanding of quantum phenomena.The path integral approach is particularly useful in quantum field theory, where it provides a powerful framework for calculating transition probabilities and understanding the behavior of quantum systems. It has also found applications in various areas of physics, such as condensed matter, statistical mechanics, and even in finance (the path integral approach to option pricing).The mathematical construction of the path integral involves the use of advanced concepts from functional analysis and measure theory, making it a powerful and sophisticated tool in the physicist\\'s arsenal.Help us out by providing feedback on this documentation page:PreviousAdd message history (memory)NextInspect your runnablesExample SetupUsing a custom function (Recommended)Using a RunnableBranchCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/how_to/inspect/', 'content_type': 'text/html; charset=utf-8', 'title': 'Inspect your runnables | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'Once you create a runnable with LCEL, you may often want to inspect it to get a better sense for what is going on. This notebook covers some methods for doing so.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nInspect your runnables | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreRoute logic based on inputInspect your runnablesCreate a runnable with the @chain decoratorManaging prompt sizeMultiple chainsEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageMoreInspect your runnablesOn this pageInspect your runnablesOnce you create a runnable with LCEL, you may often want to inspect it to get a better sense for what is going on. This notebook covers some methods for doing so.First, let\\'s create an example LCEL. We will create one that does retrieval%pip install --upgrade --quiet  langchain langchain-openai faiss-cpu tiktokenfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsAPI Reference:FAISSStrOutputParserChatPromptTemplateRunnablePassthroughChatOpenAIOpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())Get a graphâ€‹You can get a graph of the runnablechain.get_graph()Print a graphâ€‹While that is not super legible, you can print it to get a display that\\'s easier to understandchain.get_graph().print_ascii()           +---------------------------------+                    | Parallel<context,question>Input |                    +---------------------------------+                             **               **                                 ***                   ***                            **                         **           +----------------------+              +-------------+  | VectorStoreRetriever |              | Passthrough |  +----------------------+              +-------------+                      **               **                                      ***         ***                                           **     **                                +----------------------------------+                   | Parallel<context,question>Output |                   +----------------------------------+                                     *                                                      *                                                      *                                           +--------------------+                                 | ChatPromptTemplate |                                 +--------------------+                                            *                                                      *                                                      *                                               +------------+                                         | ChatOpenAI |                                         +------------+                                                *                                                      *                                                      *                                            +-----------------+                                    | StrOutputParser |                                    +-----------------+                                              *                                                      *                                                      *                                         +-----------------------+                              | StrOutputParserOutput |                              +-----------------------+Get the promptsâ€‹An important part of every chain is the prompts that are used. You can get the prompts present in the chain:chain.get_prompts()[ChatPromptTemplate(input_variables=[\\'context\\', \\'question\\'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\\'context\\', \\'question\\'], template=\\'Answer the question based only on the following context:\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\n\\'))])]Help us out by providing feedback on this documentation page:PreviousRoute logic based on inputNextCreate a runnable with the @chain decoratorGet a graphPrint a graphGet the promptsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/cookbook/multiple_chains/', 'content_type': 'text/html; charset=utf-8', 'title': 'Multiple chains | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'Runnables can easily be used to string together multiple Chains', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nMultiple chains | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreRoute logic based on inputInspect your runnablesCreate a runnable with the @chain decoratorManaging prompt sizeMultiple chainsEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageMoreMultiple chainsOn this pageMultiple chainsRunnables can easily be used to string together multiple Chains%pip install --upgrade --quiet  langchain langchain-openaifrom operator import itemgetterfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIprompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")prompt2 = ChatPromptTemplate.from_template(    \"what country is the city {city} in? respond in {language}\")model = ChatOpenAI()chain1 = prompt1 | model | StrOutputParser()chain2 = (    {\"city\": chain1, \"language\": itemgetter(\"language\")}    | prompt2    | model    | StrOutputParser())chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})API Reference:StrOutputParserChatPromptTemplateChatOpenAI\\'El paÃ\\xads donde se encuentra la ciudad de Honolulu, donde naciÃ³ Barack Obama, el 44Âº Presidente de los Estados Unidos, es Estados Unidos. Honolulu se encuentra en la isla de Oahu, en el estado de HawÃ¡i.\\'from langchain_core.runnables import RunnablePassthroughprompt1 = ChatPromptTemplate.from_template(    \"generate a {attribute} color. Return the name of the color and nothing else:\")prompt2 = ChatPromptTemplate.from_template(    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\")prompt3 = ChatPromptTemplate.from_template(    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\")prompt4 = ChatPromptTemplate.from_template(    \"What is the color of {fruit} and the flag of {country}?\")model_parser = model | StrOutputParser()color_generator = (    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser})color_to_fruit = prompt2 | model_parsercolor_to_country = prompt3 | model_parserquestion_generator = (    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4)API Reference:RunnablePassthroughquestion_generator.invoke(\"warm\")ChatPromptValue(messages=[HumanMessage(content=\\'What is the color of strawberry and the flag of China?\\', additional_kwargs={}, example=False)])prompt = question_generator.invoke(\"warm\")model.invoke(prompt)AIMessage(content=\\'The color of an apple is typically red or green. The flag of China is predominantly red with a large yellow star in the upper left corner and four smaller yellow stars surrounding it.\\', additional_kwargs={}, example=False)Branching and Mergingâ€‹You may want the output of one component to be processed by 2 or more other components. RunnableParallels let you split or fork the chain so multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:     Input      / \\\\     /   \\\\ Branch1 Branch2     \\\\   /      \\\\ /      Combineplanner = (    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")    | ChatOpenAI()    | StrOutputParser()    | {\"base_response\": RunnablePassthrough()})arguments_for = (    ChatPromptTemplate.from_template(        \"List the pros or positive aspects of {base_response}\"    )    | ChatOpenAI()    | StrOutputParser())arguments_against = (    ChatPromptTemplate.from_template(        \"List the cons or negative aspects of {base_response}\"    )    | ChatOpenAI()    | StrOutputParser())final_responder = (    ChatPromptTemplate.from_messages(        [            (\"ai\", \"{original_response}\"),            (\"human\", \"Pros:\\\\n{results_1}\\\\n\\\\nCons:\\\\n{results_2}\"),            (\"system\", \"Generate a final response given the critique\"),        ]    )    | ChatOpenAI()    | StrOutputParser())chain = (    planner    | {        \"results_1\": arguments_for,        \"results_2\": arguments_against,        \"original_response\": itemgetter(\"base_response\"),    }    | final_responder)chain.invoke({\"input\": \"scrum\"})\\'While Scrum has its potential cons and challenges, many organizations have successfully embraced and implemented this project management framework to great effect. The cons mentioned above can be mitigated or overcome with proper training, support, and a commitment to continuous improvement. It is also important to note that not all cons may be applicable to every organization or project.\\\\n\\\\nFor example, while Scrum may be complex initially, with proper training and guidance, teams can quickly grasp the concepts and practices. The lack of predictability can be mitigated by implementing techniques such as velocity tracking and release planning. The limited documentation can be addressed by maintaining a balance between lightweight documentation and clear communication among team members. The dependency on team collaboration can be improved through effective communication channels and regular team-building activities.\\\\n\\\\nScrum can be scaled and adapted to larger projects by using frameworks like Scrum of Scrums or LeSS (Large Scale Scrum). Concerns about speed versus quality can be addressed by incorporating quality assurance practices, such as continuous integration and automated testing, into the Scrum process. Scope creep can be managed by having a well-defined and prioritized product backlog, and a strong product owner can be developed through training and mentorship.\\\\n\\\\nResistance to change can be overcome by providing proper education and communication to stakeholders and involving them in the decision-making process. Ultimately, the cons of Scrum can be seen as opportunities for growth and improvement, and with the right mindset and support, they can be effectively managed.\\\\n\\\\nIn conclusion, while Scrum may have its challenges and potential cons, the benefits and advantages it offers in terms of collaboration, flexibility, adaptability, transparency, and customer satisfaction make it a widely adopted and successful project management framework. With proper implementation and continuous improvement, organizations can leverage Scrum to drive innovation, efficiency, and project success.\\'Help us out by providing feedback on this documentation page:PreviousManaging prompt sizeNextğŸ¦œğŸ›\\xa0ï¸� LangSmithBranching and MergingCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/how_to/message_history/', 'content_type': 'text/html; charset=utf-8', 'title': 'Add message history (memory) | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'The RunnableWithMessageHistory lets us add message history to certain types of chains. It wraps another Runnable and manages the chat message history for it.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nAdd message history (memory) | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageAdd message history (memory)On this pageAdd message history (memory)The RunnableWithMessageHistory lets us add message history to certain types of chains. It wraps another Runnable and manages the chat message history for it.Specifically, it can be used for any Runnable that takes as input one ofa sequence of BaseMessagea dict with a key that takes a sequence of BaseMessagea dict with a key that takes the latest message(s) as a string or sequence of BaseMessage, and a separate key that takes historical messagesAnd returns as output one ofa string that can be treated as the contents of an AIMessagea sequence of BaseMessagea dict with a key that contains a sequence of BaseMessageLet\\'s take a look at some examples to see how it works. First we construct a runnable (which here accepts a dict as input and returns a message as output):from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_openai.chat_models import ChatOpenAImodel = ChatOpenAI()prompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"You\\'re an assistant who\\'s good at {ability}. Respond in 20 words or fewer\",        ),        MessagesPlaceholder(variable_name=\"history\"),        (\"human\", \"{input}\"),    ])runnable = prompt | modelAPI Reference:ChatPromptTemplateMessagesPlaceholderChatOpenAITo manage the message history, we will need:This runnable;A callable that returns an instance of BaseChatMessageHistory.Check out the memory integrations page for implementations of chat message histories using Redis and other providers. Here we demonstrate using an in-memory ChatMessageHistory as well as more persistent storage using RedisChatMessageHistory.In-memoryâ€‹Below we show a simple example in which the chat history lives in memory, in this case via a global Python dict.We construct a callable get_session_history that references this dict to return an instance of ChatMessageHistory. The arguments to the callable can be specified by passing a configuration to the RunnableWithMessageHistory at runtime. By default, the configuration parameter is expected to be a single string session_id. This can be adjusted via the history_factory_config kwarg.Using the single-parameter default:from langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_core.runnables.history import RunnableWithMessageHistorystore = {}def get_session_history(session_id: str) -> BaseChatMessageHistory:    if session_id not in store:        store[session_id] = ChatMessageHistory()    return store[session_id]with_message_history = RunnableWithMessageHistory(    runnable,    get_session_history,    input_messages_key=\"input\",    history_messages_key=\"history\",)API Reference:ChatMessageHistoryBaseChatMessageHistoryRunnableWithMessageHistoryNote that we\\'ve specified input_messages_key (the key to be treated as the latest input message) and history_messages_key (the key to add historical messages to).When invoking this new runnable, we specify the corresponding chat history via a configuration parameter:with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},    config={\"configurable\": {\"session_id\": \"abc123\"}},)AIMessage(content=\\'Cosine is a trigonometric function that calculates the ratio of the adjacent side to the hypotenuse of a right triangle.\\')# Rememberswith_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What?\"},    config={\"configurable\": {\"session_id\": \"abc123\"}},)AIMessage(content=\\'Cosine is a mathematical function used to calculate the length of a side in a right triangle.\\')# New session_id --> does not remember.with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What?\"},    config={\"configurable\": {\"session_id\": \"def234\"}},)AIMessage(content=\\'I can help with math problems. What do you need assistance with?\\')The configuration parameters by which we track message histories can be customized by passing in a list of ConfigurableFieldSpec objects to the history_factory_config parameter. Below, we use two parameters: a user_id and conversation_id.from langchain_core.runnables import ConfigurableFieldSpecstore = {}def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:    if (user_id, conversation_id) not in store:        store[(user_id, conversation_id)] = ChatMessageHistory()    return store[(user_id, conversation_id)]with_message_history = RunnableWithMessageHistory(    runnable,    get_session_history,    input_messages_key=\"input\",    history_messages_key=\"history\",    history_factory_config=[        ConfigurableFieldSpec(            id=\"user_id\",            annotation=str,            name=\"User ID\",            description=\"Unique identifier for the user.\",            default=\"\",            is_shared=True,        ),        ConfigurableFieldSpec(            id=\"conversation_id\",            annotation=str,            name=\"Conversation ID\",            description=\"Unique identifier for the conversation.\",            default=\"\",            is_shared=True,        ),    ],)API Reference:ConfigurableFieldSpecwith_message_history.invoke(    {\"ability\": \"math\", \"input\": \"Hello\"},    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}},)Examples with runnables of different signaturesâ€‹The above runnable takes a dict as input and returns a BaseMessage. Below we show some alternatives.Messages input, dict outputâ€‹from langchain_core.messages import HumanMessagefrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({\"output_message\": ChatOpenAI()})def get_session_history(session_id: str) -> BaseChatMessageHistory:    if session_id not in store:        store[session_id] = ChatMessageHistory()    return store[session_id]with_message_history = RunnableWithMessageHistory(    chain,    get_session_history,    output_messages_key=\"output_message\",)with_message_history.invoke(    [HumanMessage(content=\"What did Simone de Beauvoir believe about free will\")],    config={\"configurable\": {\"session_id\": \"baz\"}},)API Reference:HumanMessageRunnableParallel{\\'output_message\\': AIMessage(content=\"Simone de Beauvoir believed in the existence of free will. She argued that individuals have the ability to make choices and determine their own actions, even in the face of social and cultural constraints. She rejected the idea that individuals are purely products of their environment or predetermined by biology or destiny. Instead, she emphasized the importance of personal responsibility and the need for individuals to actively engage in creating their own lives and defining their own existence. De Beauvoir believed that freedom and agency come from recognizing one\\'s own freedom and actively exercising it in the pursuit of personal and collective liberation.\")}with_message_history.invoke(    [HumanMessage(content=\"How did this compare to Sartre\")],    config={\"configurable\": {\"session_id\": \"baz\"}},){\\'output_message\\': AIMessage(content=\\'Simone de Beauvoir\\\\\\'s views on free will were closely aligned with those of her contemporary and partner Jean-Paul Sartre. Both de Beauvoir and Sartre were existentialist philosophers who emphasized the importance of individual freedom and the rejection of determinism. They believed that human beings have the capacity to transcend their circumstances and create their own meaning and values.\\\\n\\\\nSartre, in his famous work \"Being and Nothingness,\" argued that human beings are condemned to be free, meaning that we are burdened with the responsibility of making choices and defining ourselves in a world that lacks inherent meaning. Like de Beauvoir, Sartre believed that individuals have the ability to exercise their freedom and make choices in the face of external and internal constraints.\\\\n\\\\nWhile there may be some nuanced differences in their philosophical writings, overall, de Beauvoir and Sartre shared a similar belief in the existence of free will and the importance of individual agency in shaping one\\\\\\'s own life.\\')}Messages input, messages outputâ€‹RunnableWithMessageHistory(    ChatOpenAI(),    get_session_history,)Dict with single key for all messages input, messages outputâ€‹from operator import itemgetterRunnableWithMessageHistory(    itemgetter(\"input_messages\") | ChatOpenAI(),    get_session_history,    input_messages_key=\"input_messages\",)Persistent storageâ€‹In many cases it is preferable to persist conversation histories. RunnableWithMessageHistory is agnostic as to how the get_session_history callable retrieves its chat message histories. See here for an example using a local filesystem. Below we demonstrate how one could use Redis. Check out the memory integrations page for implementations of chat message histories using other providers.Setupâ€‹We\\'ll need to install Redis if it\\'s not installed already:%pip install --upgrade --quiet redisStart a local Redis Stack server if we don\\'t have an existing Redis deployment to connect to:docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latestREDIS_URL = \"redis://localhost:6379/0\"LangSmithâ€‹LangSmith is especially useful for something like message history injection, where it can be hard to otherwise understand what the inputs are to various parts of the chain.Note that LangSmith is not needed, but it is helpful.\\nIf you do want to use LangSmith, after you sign up at the link above, make sure to uncoment the below and set your environment variables to start logging traces:# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()Updating the message history implementation just requires us to define a new callable, this time returning an instance of RedisChatMessageHistory:from langchain_community.chat_message_histories import RedisChatMessageHistorydef get_message_history(session_id: str) -> RedisChatMessageHistory:    return RedisChatMessageHistory(session_id, url=REDIS_URL)with_message_history = RunnableWithMessageHistory(    runnable,    get_message_history,    input_messages_key=\"input\",    history_messages_key=\"history\",)API Reference:RedisChatMessageHistoryWe can invoke as before:with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What does cosine mean?\"},    config={\"configurable\": {\"session_id\": \"foobar\"}},)AIMessage(content=\\'Cosine is a trigonometric function that represents the ratio of the adjacent side to the hypotenuse in a right triangle.\\')with_message_history.invoke(    {\"ability\": \"math\", \"input\": \"What\\'s its inverse\"},    config={\"configurable\": {\"session_id\": \"foobar\"}},)AIMessage(content=\\'The inverse of cosine is the arccosine function, denoted as acos or cos^-1, which gives the angle corresponding to a given cosine value.\\')tipLangsmith traceLooking at the Langsmith trace for the second call, we can see that when constructing the prompt, a \"history\" variable has been injected which is a list of two messages (our first input and first output).Help us out by providing feedback on this documentation page:PreviousStreamingNextRoute logic based on inputIn-memoryExamples with runnables of different signaturesPersistent storageSetupLangSmithCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/interface/', 'content_type': 'text/html; charset=utf-8', 'title': 'Runnable interface | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'To make it as easy as possible to create custom chains, we\\'ve implemented a \"Runnable\" protocol. Many LangChain components implement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There are also several useful primitives for working with runnables, which you can read about in this section.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRunnable interface | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageRunnable interfaceOn this pageRunnable interfaceTo make it as easy as possible to create custom chains, we\\'ve implemented a \"Runnable\" protocol. Many LangChain components implement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There are also several useful primitives for working with runnables, which you can read about in this section.This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way.\\nThe standard interface includes:stream: stream back chunks of the responseinvoke: call the chain on an inputbatch: call the chain on a list of inputsThese also have corresponding async methods that should be used with asyncio await syntax for concurrency:astream: stream back chunks of the response asyncainvoke: call the chain on an input asyncabatch: call the chain on a list of inputs asyncastream_log: stream back intermediate steps as they happen, in addition to the final responseastream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)The input type and output type varies by component:ComponentInput TypeOutput TypePromptDictionaryPromptValueChatModelSingle string, list of chat messages or a PromptValueChatMessageLLMSingle string, list of chat messages or a PromptValueStringOutputParserThe output of an LLM or ChatModelDepends on the parserRetrieverSingle stringList of DocumentsToolSingle string or dictionary, depending on the toolDepends on the toolAll runnables expose input and output schemas to inspect the inputs and outputs:input_schema: an input Pydantic model auto-generated from the structure of the Runnableoutput_schema: an output Pydantic model auto-generated from the structure of the RunnableLet\\'s take a look at these methods. To do so, we\\'ll create a super simple PromptTemplate + ChatModel chain.%pip install --upgrade --quiet  langchain-core langchain-community langchain-openaifrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAImodel = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | modelAPI Reference:ChatPromptTemplateChatOpenAIInput Schemaâ€‹A description of the inputs accepted by a Runnable.\\nThis is a Pydantic model dynamically generated from the structure of any Runnable.\\nYou can call .schema() on it to obtain a JSONSchema representation.# The input schema of the chain is the input schema of its first part, the prompt.chain.input_schema.schema(){\\'title\\': \\'PromptInput\\', \\'type\\': \\'object\\', \\'properties\\': {\\'topic\\': {\\'title\\': \\'Topic\\', \\'type\\': \\'string\\'}}}prompt.input_schema.schema(){\\'title\\': \\'PromptInput\\', \\'type\\': \\'object\\', \\'properties\\': {\\'topic\\': {\\'title\\': \\'Topic\\', \\'type\\': \\'string\\'}}}model.input_schema.schema(){\\'title\\': \\'ChatOpenAIInput\\', \\'anyOf\\': [{\\'type\\': \\'string\\'},  {\\'$ref\\': \\'#/definitions/StringPromptValue\\'},  {\\'$ref\\': \\'#/definitions/ChatPromptValueConcrete\\'},  {\\'type\\': \\'array\\',   \\'items\\': {\\'anyOf\\': [{\\'$ref\\': \\'#/definitions/AIMessage\\'},     {\\'$ref\\': \\'#/definitions/HumanMessage\\'},     {\\'$ref\\': \\'#/definitions/ChatMessage\\'},     {\\'$ref\\': \\'#/definitions/SystemMessage\\'},     {\\'$ref\\': \\'#/definitions/FunctionMessage\\'},     {\\'$ref\\': \\'#/definitions/ToolMessage\\'}]}}], \\'definitions\\': {\\'StringPromptValue\\': {\\'title\\': \\'StringPromptValue\\',   \\'description\\': \\'String prompt value.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'text\\': {\\'title\\': \\'Text\\', \\'type\\': \\'string\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'StringPromptValue\\',     \\'enum\\': [\\'StringPromptValue\\'],     \\'type\\': \\'string\\'}},   \\'required\\': [\\'text\\']},  \\'AIMessage\\': {\\'title\\': \\'AIMessage\\',   \\'description\\': \\'A Message from an AI.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'ai\\',     \\'enum\\': [\\'ai\\'],     \\'type\\': \\'string\\'},    \\'example\\': {\\'title\\': \\'Example\\', \\'default\\': False, \\'type\\': \\'boolean\\'}},   \\'required\\': [\\'content\\']},  \\'HumanMessage\\': {\\'title\\': \\'HumanMessage\\',   \\'description\\': \\'A Message from a human.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'human\\',     \\'enum\\': [\\'human\\'],     \\'type\\': \\'string\\'},    \\'example\\': {\\'title\\': \\'Example\\', \\'default\\': False, \\'type\\': \\'boolean\\'}},   \\'required\\': [\\'content\\']},  \\'ChatMessage\\': {\\'title\\': \\'ChatMessage\\',   \\'description\\': \\'A Message that can be assigned an arbitrary speaker (i.e. role).\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'chat\\',     \\'enum\\': [\\'chat\\'],     \\'type\\': \\'string\\'},    \\'role\\': {\\'title\\': \\'Role\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'role\\']},  \\'SystemMessage\\': {\\'title\\': \\'SystemMessage\\',   \\'description\\': \\'A Message for priming AI behavior, usually passed in as the first of a sequence\\\\nof input messages.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'system\\',     \\'enum\\': [\\'system\\'],     \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\']},  \\'FunctionMessage\\': {\\'title\\': \\'FunctionMessage\\',   \\'description\\': \\'A Message for passing the result of executing a function back to a model.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'function\\',     \\'enum\\': [\\'function\\'],     \\'type\\': \\'string\\'},    \\'name\\': {\\'title\\': \\'Name\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'name\\']},  \\'ToolMessage\\': {\\'title\\': \\'ToolMessage\\',   \\'description\\': \\'A Message for passing the result of executing a tool back to a model.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'tool\\',     \\'enum\\': [\\'tool\\'],     \\'type\\': \\'string\\'},    \\'tool_call_id\\': {\\'title\\': \\'Tool Call Id\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'tool_call_id\\']},  \\'ChatPromptValueConcrete\\': {\\'title\\': \\'ChatPromptValueConcrete\\',   \\'description\\': \\'Chat prompt value which explicitly lists out the message types it accepts.\\\\nFor use in external schemas.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'messages\\': {\\'title\\': \\'Messages\\',     \\'type\\': \\'array\\',     \\'items\\': {\\'anyOf\\': [{\\'$ref\\': \\'#/definitions/AIMessage\\'},       {\\'$ref\\': \\'#/definitions/HumanMessage\\'},       {\\'$ref\\': \\'#/definitions/ChatMessage\\'},       {\\'$ref\\': \\'#/definitions/SystemMessage\\'},       {\\'$ref\\': \\'#/definitions/FunctionMessage\\'},       {\\'$ref\\': \\'#/definitions/ToolMessage\\'}]}},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'ChatPromptValueConcrete\\',     \\'enum\\': [\\'ChatPromptValueConcrete\\'],     \\'type\\': \\'string\\'}},   \\'required\\': [\\'messages\\']}}}Output Schemaâ€‹A description of the outputs produced by a Runnable.\\nThis is a Pydantic model dynamically generated from the structure of any Runnable.\\nYou can call .schema() on it to obtain a JSONSchema representation.# The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessagechain.output_schema.schema(){\\'title\\': \\'ChatOpenAIOutput\\', \\'anyOf\\': [{\\'$ref\\': \\'#/definitions/AIMessage\\'},  {\\'$ref\\': \\'#/definitions/HumanMessage\\'},  {\\'$ref\\': \\'#/definitions/ChatMessage\\'},  {\\'$ref\\': \\'#/definitions/SystemMessage\\'},  {\\'$ref\\': \\'#/definitions/FunctionMessage\\'},  {\\'$ref\\': \\'#/definitions/ToolMessage\\'}], \\'definitions\\': {\\'AIMessage\\': {\\'title\\': \\'AIMessage\\',   \\'description\\': \\'A Message from an AI.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'ai\\',     \\'enum\\': [\\'ai\\'],     \\'type\\': \\'string\\'},    \\'example\\': {\\'title\\': \\'Example\\', \\'default\\': False, \\'type\\': \\'boolean\\'}},   \\'required\\': [\\'content\\']},  \\'HumanMessage\\': {\\'title\\': \\'HumanMessage\\',   \\'description\\': \\'A Message from a human.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'human\\',     \\'enum\\': [\\'human\\'],     \\'type\\': \\'string\\'},    \\'example\\': {\\'title\\': \\'Example\\', \\'default\\': False, \\'type\\': \\'boolean\\'}},   \\'required\\': [\\'content\\']},  \\'ChatMessage\\': {\\'title\\': \\'ChatMessage\\',   \\'description\\': \\'A Message that can be assigned an arbitrary speaker (i.e. role).\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'chat\\',     \\'enum\\': [\\'chat\\'],     \\'type\\': \\'string\\'},    \\'role\\': {\\'title\\': \\'Role\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'role\\']},  \\'SystemMessage\\': {\\'title\\': \\'SystemMessage\\',   \\'description\\': \\'A Message for priming AI behavior, usually passed in as the first of a sequence\\\\nof input messages.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'system\\',     \\'enum\\': [\\'system\\'],     \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\']},  \\'FunctionMessage\\': {\\'title\\': \\'FunctionMessage\\',   \\'description\\': \\'A Message for passing the result of executing a function back to a model.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'function\\',     \\'enum\\': [\\'function\\'],     \\'type\\': \\'string\\'},    \\'name\\': {\\'title\\': \\'Name\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'name\\']},  \\'ToolMessage\\': {\\'title\\': \\'ToolMessage\\',   \\'description\\': \\'A Message for passing the result of executing a tool back to a model.\\',   \\'type\\': \\'object\\',   \\'properties\\': {\\'content\\': {\\'title\\': \\'Content\\',     \\'anyOf\\': [{\\'type\\': \\'string\\'},      {\\'type\\': \\'array\\',       \\'items\\': {\\'anyOf\\': [{\\'type\\': \\'string\\'}, {\\'type\\': \\'object\\'}]}}]},    \\'additional_kwargs\\': {\\'title\\': \\'Additional Kwargs\\', \\'type\\': \\'object\\'},    \\'type\\': {\\'title\\': \\'Type\\',     \\'default\\': \\'tool\\',     \\'enum\\': [\\'tool\\'],     \\'type\\': \\'string\\'},    \\'tool_call_id\\': {\\'title\\': \\'Tool Call Id\\', \\'type\\': \\'string\\'}},   \\'required\\': [\\'content\\', \\'tool_call_id\\']}}}Streamâ€‹for s in chain.stream({\"topic\": \"bears\"}):    print(s.content, end=\"\", flush=True)Sure, here\\'s a bear-themed joke for you:Why don\\'t bears wear shoes?Because they already have bear feet!Invokeâ€‹chain.invoke({\"topic\": \"bears\"})AIMessage(content=\"Why don\\'t bears wear shoes? \\\\n\\\\nBecause they have bear feet!\")Batchâ€‹chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])[AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\"), AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\")]You can set the number of concurrent requests by using the max_concurrency parameterchain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"), AIMessage(content=\"Why don\\'t cats play poker in the wild? Too many cheetahs!\")]Async Streamâ€‹async for s in chain.astream({\"topic\": \"bears\"}):    print(s.content, end=\"\", flush=True)Why don\\'t bears wear shoes?Because they have bear feet!Async Invokeâ€‹await chain.ainvoke({\"topic\": \"bears\"})AIMessage(content=\"Why don\\'t bears ever wear shoes?\\\\n\\\\nBecause they already have bear feet!\")Async Batchâ€‹await chain.abatch([{\"topic\": \"bears\"}])[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\")]Async Stream Events (beta)â€‹Event Streaming is a beta API, and may change a bit based on feedback.Note: Introduced in langchain-core 0.2.0For now, when using the astream_events API, for everything to work properly please:Use async throughout the code (including async tools etc)Propagate callbacks if defining custom functions / runnables. Whenever using runnables without LCEL, make sure to call .astream() on LLMs rather than .ainvoke to force the LLM to stream tokens.Event Referenceâ€‹Here is a reference table that shows some events that might be emitted by the various Runnable objects.\\nDefinitions for some of the Runnable are included after the table.âš\\xa0ï¸� When streaming the inputs for the runnable will not be available until the input stream has been entirely consumed This means that the inputs will be available at for the corresponding end hook rather than start event.eventnamechunkinputoutputon_chat_model_start[model name]{\"messages\": [[SystemMessage, HumanMessage]]}on_chat_model_stream[model name]AIMessageChunk(content=\"hello\")on_chat_model_end[model name]{\"messages\": [[SystemMessage, HumanMessage]]}{\"generations\": [...], \"llm_output\": None, ...}on_llm_start[model name]{\\'input\\': \\'hello\\'}on_llm_stream[model name]\\'Hello\\'on_llm_end[model name]\\'Hello human!\\'on_chain_startformat_docson_chain_streamformat_docs\"hello world!, goodbye world!\"on_chain_endformat_docs[Document(...)]\"hello world!, goodbye world!\"on_tool_startsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_streamsome_tool{\"x\": 1, \"y\": \"2\"}on_tool_endsome_tool{\"x\": 1, \"y\": \"2\"}on_retriever_start[retriever name]{\"query\": \"hello\"}on_retriever_chunk[retriever name]{documents: [...]}on_retriever_end[retriever name]{\"query\": \"hello\"}{documents: [...]}on_prompt_start[template_name]{\"question\": \"hello\"}on_prompt_end[template_name]{\"question\": \"hello\"}ChatPromptValue(messages: [SystemMessage, ...])Here are declarations associated with the events shown above:format_docs:def format_docs(docs: List[Document]) -> str:    \\'\\'\\'Format the docs.\\'\\'\\'    return \", \".join([doc.page_content for doc in docs])format_docs = RunnableLambda(format_docs)some_tool:@tooldef some_tool(x: int, y: str) -> dict:    \\'\\'\\'Some_tool.\\'\\'\\'    return {\"x\": x, \"y\": y}prompt:template = ChatPromptTemplate.from_messages(    [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})Let\\'s define a new chain to make it more interesting to show off the astream_events interface (and later the astream_log interface).from langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingstemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)vectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()retrieval_chain = (    {        \"context\": retriever.with_config(run_name=\"Docs\"),        \"question\": RunnablePassthrough(),    }    | prompt    | model.with_config(run_name=\"my_llm\")    | StrOutputParser())API Reference:FAISSStrOutputParserRunnablePassthroughOpenAIEmbeddingsNow let\\'s use astream_events to get events from the retriever and the LLM.async for event in retrieval_chain.astream_events(    \"where did harrison work?\", version=\"v1\", include_names=[\"Docs\", \"my_llm\"]):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(event[\"data\"][\"chunk\"].content, end=\"|\")    elif kind in {\"on_chat_model_start\"}:        print()        print(\"Streaming LLM:\")    elif kind in {\"on_chat_model_end\"}:        print()        print(\"Done streaming LLM.\")    elif kind == \"on_retriever_end\":        print(\"--\")        print(\"Retrieved the following documents:\")        print(event[\"data\"][\"output\"][\"documents\"])    elif kind == \"on_tool_end\":        print(f\"Ended tool: {event[\\'name\\']}\")    else:        pass/home/eugene/src/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.  warn_beta(``````output--Retrieved the following documents:[Document(page_content=\\'harrison worked at kensho\\')]Streaming LLM:|H|arrison| worked| at| Kens|ho|.||Done streaming LLM.Async Stream Intermediate Stepsâ€‹All runnables also have a method .astream_log() which is used to stream (as they happen) all or part of the intermediate steps of your chain/sequence. This is useful to show progress to the user, to use intermediate results, or to debug your chain.You can stream all steps (default) or include/exclude steps by name, tags or metadata.This method yields JSONPatch ops that when applied in the same order as received build up the RunState.class LogEntry(TypedDict):    id: str    \"\"\"ID of the sub-run.\"\"\"    name: str    \"\"\"Name of the object being run.\"\"\"    type: str    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"    tags: List[str]    \"\"\"List of tags for the run.\"\"\"    metadata: Dict[str, Any]    \"\"\"Key-value pairs of metadata for the run.\"\"\"    start_time: str    \"\"\"ISO-8601 timestamp of when the run started.\"\"\"    streamed_output_str: List[str]    \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\"    final_output: Optional[Any]    \"\"\"Final output of this run.    Only available after the run has finished successfully.\"\"\"    end_time: Optional[str]    \"\"\"ISO-8601 timestamp of when the run ended.    Only available after the run has finished.\"\"\"class RunState(TypedDict):    id: str    \"\"\"ID of the run.\"\"\"    streamed_output: List[Any]    \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"    final_output: Optional[Any]    \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.    Only available after the run has finished successfully.\"\"\"    logs: Dict[str, LogEntry]    \"\"\"Map of run names to sub-runs. If filters were supplied, this list will    contain only the runs that matched the filters.\"\"\"Streaming JSONPatch chunksâ€‹This is useful eg. to stream the JSONPatch in an HTTP server, and then apply the ops on the client to rebuild the run state there. See LangServe for tooling to make it easier to build a webserver from any Runnable.async for chunk in retrieval_chain.astream_log(    \"where did harrison work?\", include_names=[\"Docs\"]):    print(\"-\" * 40)    print(chunk)----------------------------------------RunLogPatch({\\'op\\': \\'replace\\',  \\'path\\': \\'\\',  \\'value\\': {\\'final_output\\': None,            \\'id\\': \\'82e9b4b1-3dd6-4732-8db9-90e79c4da48c\\',            \\'logs\\': {},            \\'name\\': \\'RunnableSequence\\',            \\'streamed_output\\': [],            \\'type\\': \\'chain\\'}})----------------------------------------RunLogPatch({\\'op\\': \\'add\\',  \\'path\\': \\'/logs/Docs\\',  \\'value\\': {\\'end_time\\': None,            \\'final_output\\': None,            \\'id\\': \\'9206e94a-57bd-48ee-8c5e-fdd1c52a6da2\\',            \\'metadata\\': {},            \\'name\\': \\'Docs\\',            \\'start_time\\': \\'2024-01-19T22:33:55.902+00:00\\',            \\'streamed_output\\': [],            \\'streamed_output_str\\': [],            \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],            \\'type\\': \\'retriever\\'}})----------------------------------------RunLogPatch({\\'op\\': \\'add\\',  \\'path\\': \\'/logs/Docs/final_output\\',  \\'value\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]}}, {\\'op\\': \\'add\\',  \\'path\\': \\'/logs/Docs/end_time\\',  \\'value\\': \\'2024-01-19T22:33:56.064+00:00\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'H\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'H\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'arrison\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'Harrison\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\' worked\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'Harrison worked\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\' at\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'Harrison worked at\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\' Kens\\'}, {\\'op\\': \\'replace\\', \\'path\\': \\'/final_output\\', \\'value\\': \\'Harrison worked at Kens\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'ho\\'}, {\\'op\\': \\'replace\\',  \\'path\\': \\'/final_output\\',  \\'value\\': \\'Harrison worked at Kensho\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'.\\'}, {\\'op\\': \\'replace\\',  \\'path\\': \\'/final_output\\',  \\'value\\': \\'Harrison worked at Kensho.\\'})----------------------------------------RunLogPatch({\\'op\\': \\'add\\', \\'path\\': \\'/streamed_output/-\\', \\'value\\': \\'\\'})Streaming the incremental RunStateâ€‹You can simply pass diff=False to get incremental values of RunState.\\nYou get more verbose output with more repetitive parts.async for chunk in retrieval_chain.astream_log(    \"where did harrison work?\", include_names=[\"Docs\"], diff=False):    print(\"-\" * 70)    print(chunk)----------------------------------------------------------------------RunLog({\\'final_output\\': None, \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': None, \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': None,                   \\'final_output\\': None,                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': None, \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'H\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked at\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\', \\' at\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked at Kens\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\', \\' at\\', \\' Kens\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked at Kensho\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\', \\' at\\', \\' Kens\\', \\'ho\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked at Kensho.\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\', \\'H\\', \\'arrison\\', \\' worked\\', \\' at\\', \\' Kens\\', \\'ho\\', \\'.\\'], \\'type\\': \\'chain\\'})----------------------------------------------------------------------RunLog({\\'final_output\\': \\'Harrison worked at Kensho.\\', \\'id\\': \\'431d1c55-7c50-48ac-b3a2-2f5ba5f35172\\', \\'logs\\': {\\'Docs\\': {\\'end_time\\': \\'2024-01-19T22:33:57.120+00:00\\',                   \\'final_output\\': {\\'documents\\': [Document(page_content=\\'harrison worked at kensho\\')]},                   \\'id\\': \\'8de10b49-d6af-4cb7-a4e7-fbadf6efa01e\\',                   \\'metadata\\': {},                   \\'name\\': \\'Docs\\',                   \\'start_time\\': \\'2024-01-19T22:33:56.939+00:00\\',                   \\'streamed_output\\': [],                   \\'streamed_output_str\\': [],                   \\'tags\\': [\\'map:key:context\\', \\'FAISS\\', \\'OpenAIEmbeddings\\'],                   \\'type\\': \\'retriever\\'}}, \\'name\\': \\'RunnableSequence\\', \\'streamed_output\\': [\\'\\',                     \\'H\\',                     \\'arrison\\',                     \\' worked\\',                     \\' at\\',                     \\' Kens\\',                     \\'ho\\',                     \\'.\\',                     \\'\\'], \\'type\\': \\'chain\\'})Parallelismâ€‹Let\\'s take a look at how LangChain Expression Language supports parallel requests.\\nFor example, when using a RunnableParallel (often written as a dictionary) it executes each element in parallel.from langchain_core.runnables import RunnableParallelchain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | modelchain2 = (    ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\")    | model)combined = RunnableParallel(joke=chain1, poem=chain2)API Reference:RunnableParallel%%timechain1.invoke({\"topic\": \"bears\"})CPU times: user 18 ms, sys: 1.27 ms, total: 19.3 msWall time: 692 msAIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they already have bear feet!\")%%timechain2.invoke({\"topic\": \"bears\"})CPU times: user 10.5 ms, sys: 166 Âµs, total: 10.7 msWall time: 579 msAIMessage(content=\"In forest\\'s embrace,\\\\nMajestic bears pace.\")%%timecombined.invoke({\"topic\": \"bears\"})CPU times: user 32 ms, sys: 2.59 ms, total: 34.6 msWall time: 816 ms{\\'joke\\': AIMessage(content=\"Sure, here\\'s a bear-related joke for you:\\\\n\\\\nWhy did the bear bring a ladder to the bar?\\\\n\\\\nBecause he heard the drinks were on the house!\"), \\'poem\\': AIMessage(content=\"In wilderness they roam,\\\\nMajestic strength, nature\\'s throne.\")}Parallelism on batchesâ€‹Parallelism can be combined with other runnables.\\nLet\\'s try to use parallelism with batches.%%timechain1.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])CPU times: user 17.3 ms, sys: 4.84 ms, total: 22.2 msWall time: 628 ms[AIMessage(content=\"Why don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"), AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\")]%%timechain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])CPU times: user 15.8 ms, sys: 3.83 ms, total: 19.7 msWall time: 718 ms[AIMessage(content=\\'In the wild, bears roam,\\\\nMajestic guardians of ancient home.\\'), AIMessage(content=\\'Whiskers grace, eyes gleam,\\\\nCats dance through the moonbeam.\\')]%%timecombined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])CPU times: user 44.8 ms, sys: 3.17 ms, total: 48 msWall time: 721 ms[{\\'joke\\': AIMessage(content=\"Sure, here\\'s a bear joke for you:\\\\n\\\\nWhy don\\'t bears wear shoes?\\\\n\\\\nBecause they have bear feet!\"),  \\'poem\\': AIMessage(content=\"Majestic bears roam,\\\\nNature\\'s strength, beauty shown.\")}, {\\'joke\\': AIMessage(content=\"Why don\\'t cats play poker in the wild?\\\\n\\\\nToo many cheetahs!\"),  \\'poem\\': AIMessage(content=\"Whiskers dance, eyes aglow,\\\\nCats embrace the night\\'s gentle flow.\")}]Help us out by providing feedback on this documentation page:PreviousGet startedNextPrimitivesInput SchemaOutput SchemaStreamInvokeBatchAsync StreamAsync InvokeAsync BatchAsync Stream Events (beta)Event ReferenceAsync Stream Intermediate StepsStreaming JSONPatch chunksStreaming the incremental RunStateParallelismParallelism on batchesCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/how_to/decorator/', 'content_type': 'text/html; charset=utf-8', 'title': 'Create a runnable with the @chain decorator | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'You can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionally equivalent to wrapping in a RunnableLambda.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nCreate a runnable with the @chain decorator | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreRoute logic based on inputInspect your runnablesCreate a runnable with the @chain decoratorManaging prompt sizeMultiple chainsEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageMoreCreate a runnable with the @chain decoratorCreate a runnable with the @chain decoratorYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionally equivalent to wrapping in a RunnableLambda.This will have the benefit of improved observability by tracing your chain correctly. Any calls to runnables inside this function will be traced as nested children.It will also allow you to use this as any other runnable, compose it in chain, etc.Let\\'s take a look at this in action!%pip install --upgrade --quiet  langchain langchain-openaifrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import chainfrom langchain_openai import ChatOpenAIAPI Reference:StrOutputParserChatPromptTemplatechainChatOpenAIprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = prompt1.invoke({\"topic\": text})    output1 = ChatOpenAI().invoke(prompt_val1)    parsed_output1 = StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()    return chain2.invoke({\"joke\": parsed_output1})custom_chain is now a runnable, meaning you will need to use invokecustom_chain.invoke(\"bears\")\\'The subject of this joke is bears.\\'If you check out your LangSmith traces, you should see a custom_chain trace in there, with the calls to OpenAI nested underneathHelp us out by providing feedback on this documentation page:PreviousInspect your runnablesNextManaging prompt sizeCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/primitives/functions/', 'content_type': 'text/html; charset=utf-8', 'title': 'Lambda: Run custom functions | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'You can use arbitrary functions in the pipeline.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nLambda: Run custom functions | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguagePrimitivesLambda: Run custom functionsOn this pageRun custom functionsYou can use arbitrary functions in the pipeline.Note that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single input and unpacks it into multiple argument.\\n%pip install --upgrade --quiet langchain langchain-openaifrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdafrom langchain_openai import ChatOpenAIdef length_function(text):    return len(text)def _multiple_length_function(text1, text2):    return len(text1) * len(text2)def multiple_length_function(_dict):    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")model = ChatOpenAI()chain1 = prompt | modelchain = (    {        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | RunnableLambda(multiple_length_function),    }    | prompt    | model)API Reference:ChatPromptTemplateRunnableLambdaChatOpenAIchain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})AIMessage(content=\\'3 + 9 = 12\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 7, \\'prompt_tokens\\': 14, \\'total_tokens\\': 21}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_b28b39ffa8\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-bd204541-81fd-429a-ad92-dd1913af9b1c-0\\')Accepting a Runnable Configâ€‹Runnable lambdas can optionally accept a RunnableConfig, which they can use to pass callbacks, tags, and other configuration information to nested runs.from langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnableConfigAPI Reference:StrOutputParserRunnableConfigimport jsondef parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        ChatPromptTemplate.from_template(            \"Fix the following text:\\\\n\\\\n```text\\\\n{input}\\\\n```\\\\nError: {error}\"            \" Don\\'t narrate, just respond with the fixed data.\"        )        | ChatOpenAI()        | StrOutputParser()    )    for _ in range(3):        try:            return json.loads(text)        except Exception as e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)    return \"Failed to parse\"from langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:get_openai_callback{\\'foo\\': \\'bar\\'}Tokens Used: 62    Prompt Tokens: 56    Completion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05StreamingYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a LCEL pipeline.The signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].These are useful for:implementing a custom output parsermodifying the output of a previous step, while preserving streaming capabilitiesHere\\'s an example of a custom output parser for comma-separated lists:from typing import Iterator, Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\")model = ChatOpenAI(temperature=0.0)str_chain = prompt | model | StrOutputParser()for chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", flush=True)lion, tiger, wolf, gorilla, pandastr_chain.invoke({\"animal\": \"bear\"})\\'lion, tiger, wolf, gorilla, panda\\'# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']list_chain.invoke({\"animal\": \"bear\"})[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'elephant\\']Async versionâ€‹from typing import AsyncIteratorasync def asplit_into_list(    input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    buffer = \"\"    async for (        chunk    ) in input:  # `input` is a `async_generator` object, so use `async for`        buffer += chunk        while \",\" in buffer:            comma_index = buffer.index(\",\")            yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index + 1 :]    yield [buffer.strip()]list_chain = str_chain | asplit_into_listasync for chunk in list_chain.astream({\"animal\": \"bear\"}):    print(chunk, flush=True)[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']await list_chain.ainvoke({\"animal\": \"bear\"})[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'panda\\']Help us out by providing feedback on this documentation page:PreviousBinding: Attach runtime argsNextPassthrough: Pass through inputsAccepting a Runnable ConfigAsync versionCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/primitives/binding/', 'content_type': 'text/html; charset=utf-8', 'title': 'Binding: Attach runtime args | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'Sometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to pass these arguments in.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBinding: Attach runtime args | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguagePrimitivesBinding: Attach runtime argsOn this pageBinding: Attach runtime argsSometimes we want to invoke a Runnable within a Runnable sequence with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use Runnable.bind() to pass these arguments in.Suppose we have a simple prompt + model sequence:%pip install --upgrade --quiet  langchain langchain-openaifrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAIAPI Reference:StrOutputParserChatPromptTemplateRunnablePassthroughChatOpenAIprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Write out the following equation using algebraic symbols then solve it. Use the format\\\\n\\\\nEQUATION:...\\\\nSOLUTION:...\\\\n\\\\n\",        ),        (\"human\", \"{equation_statement}\"),    ])model = ChatOpenAI(temperature=0)runnable = (    {\"equation_statement\": RunnablePassthrough()} | prompt | model | StrOutputParser())print(runnable.invoke(\"x raised to the third plus seven equals 12\"))EQUATION: x^3 + 7 = 12SOLUTION:Subtracting 7 from both sides of the equation, we get:x^3 = 12 - 7x^3 = 5Taking the cube root of both sides, we get:x = âˆ›5Therefore, the solution to the equation x^3 + 7 = 12 is x = âˆ›5.and want to call the model with certain stop words:runnable = (    {\"equation_statement\": RunnablePassthrough()}    | prompt    | model.bind(stop=\"SOLUTION\")    | StrOutputParser())print(runnable.invoke(\"x raised to the third plus seven equals 12\"))EQUATION: x^3 + 7 = 12Attaching OpenAI functionsâ€‹One particularly useful application of binding is to attach OpenAI functions to a compatible OpenAI model:function = {    \"name\": \"solver\",    \"description\": \"Formulates and solves an equation\",    \"parameters\": {        \"type\": \"object\",        \"properties\": {            \"equation\": {                \"type\": \"string\",                \"description\": \"The algebraic expression of the equation\",            },            \"solution\": {                \"type\": \"string\",                \"description\": \"The solution to the equation\",            },        },        \"required\": [\"equation\", \"solution\"],    },}# Need gpt-4 to solve this one correctlyprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Write out the following equation using algebraic symbols then solve it.\",        ),        (\"human\", \"{equation_statement}\"),    ])model = ChatOpenAI(model=\"gpt-4\", temperature=0).bind(    function_call={\"name\": \"solver\"}, functions=[function])runnable = {\"equation_statement\": RunnablePassthrough()} | prompt | modelrunnable.invoke(\"x raised to the third plus seven equals 12\")AIMessage(content=\\'\\', additional_kwargs={\\'function_call\\': {\\'name\\': \\'solver\\', \\'arguments\\': \\'{\\\\n\"equation\": \"x^3 + 7 = 12\",\\\\n\"solution\": \"x = âˆ›5\"\\\\n}\\'}}, example=False)Attaching OpenAI toolsâ€‹tools = [    {        \"type\": \"function\",        \"function\": {            \"name\": \"get_current_weather\",            \"description\": \"Get the current weather in a given location\",            \"parameters\": {                \"type\": \"object\",                \"properties\": {                    \"location\": {                        \"type\": \"string\",                        \"description\": \"The city and state, e.g. San Francisco, CA\",                    },                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},                },                \"required\": [\"location\"],            },        },    }]model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\").bind(tools=tools)model.invoke(\"What\\'s the weather in SF, NYC and LA?\")AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_zHN0ZHwrxM7nZDdqTp6dkPko\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}, {\\'id\\': \\'call_aqdMm9HBSlFW9c9rqxTa7eQv\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"New York, NY\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}, {\\'id\\': \\'call_cx8E567zcLzYV2WSWVgO63f1\\', \\'function\\': {\\'arguments\\': \\'{\"location\": \"Los Angeles, CA\", \"unit\": \"celsius\"}\\', \\'name\\': \\'get_current_weather\\'}, \\'type\\': \\'function\\'}]})Help us out by providing feedback on this documentation page:PreviousParallel: Format dataNextLambda: Run custom functionsAttaching OpenAI functionsAttaching OpenAI toolsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/primitives/passthrough/', 'content_type': 'text/html; charset=utf-8', 'title': 'Passthrough: Pass through inputs | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'RunnablePassthrough on its own allows you to pass inputs unchanged. This typically is used in conjuction with RunnableParallel to pass data through to a new key in the map.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nPassthrough: Pass through inputs | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesSequences: Chaining runnablesParallel: Format dataBinding: Attach runtime argsLambda: Run custom functionsPassthrough: Pass through inputsAssign: Add values to stateConfigure runtime chain internalsPrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguagePrimitivesPassthrough: Pass through inputsOn this pagePassing data throughRunnablePassthrough on its own allows you to pass inputs unchanged. This typically is used in conjuction with RunnableParallel to pass data through to a new key in the map. See the example below:%pip install --upgrade --quiet  langchain langchain-openaifrom langchain_core.runnables import RunnableParallel, RunnablePassthroughrunnable = RunnableParallel(    passed=RunnablePassthrough(),    modified=lambda x: x[\"num\"] + 1,)runnable.invoke({\"num\": 1})API Reference:RunnableParallelRunnablePassthrough{\\'passed\\': {\\'num\\': 1}, \\'extra\\': {\\'num\\': 1, \\'mult\\': 3}, \\'modified\\': 2}As seen above, passed key was called with RunnablePassthrough() and so it simply passed on {\\'num\\': 1}. We also set a second key in the map with modified. This uses a lambda to set a single value adding 1 to the num, which resulted in modified key with the value of 2.Retrieval Exampleâ€‹In the example below, we see a use case where we use RunnablePassthrough along with RunnableParallel. from langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())retrieval_chain.invoke(\"where did harrison work?\")API Reference:FAISSStrOutputParserChatPromptTemplateRunnablePassthroughChatOpenAIOpenAIEmbeddings\\'Harrison worked at Kensho.\\'Here the input to prompt is expected to be a map with keys \"context\" and \"question\". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the \"question\" key. In this case, the RunnablePassthrough allows us to pass on the user\\'s question to the prompt and model.Help us out by providing feedback on this documentation page:PreviousLambda: Run custom functionsNextAssign: Add values to stateRetrieval ExampleCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/expression_language/cookbook/prompt_size/', 'content_type': 'text/html; charset=utf-8', 'title': 'Managing prompt size | ğŸ¦œï¸�ğŸ”— LangChain', 'description': \"Agents dynamically call tools. The results of those tool calls are added back to the prompt, so that the agent can plan the next action. Depending on what tools are being used and how they're being called, the agent prompt can easily grow larger than the model context window.\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nManaging prompt size | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreRoute logic based on inputInspect your runnablesCreate a runnable with the @chain decoratorManaging prompt sizeMultiple chainsEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageMoreManaging prompt sizeManaging prompt sizeAgents dynamically call tools. The results of those tool calls are added back to the prompt, so that the agent can plan the next action. Depending on what tools are being used and how they\\'re being called, the agent prompt can easily grow larger than the model context window.With LCEL, it\\'s easy to add custom functionality for managing the size of prompts within your chain or agent. Let\\'s look at simple agent example that can search Wikipedia for information.%pip install --upgrade --quiet  langchain langchain-openai wikipediafrom operator import itemgetterfrom langchain.agents import AgentExecutor, load_toolsfrom langchain.agents.format_scratchpad import format_to_openai_function_messagesfrom langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParserfrom langchain_community.tools import WikipediaQueryRunfrom langchain_community.utilities import WikipediaAPIWrapperfrom langchain_core.prompt_values import ChatPromptValuefrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_openai import ChatOpenAIAPI Reference:AgentExecutorload_toolsformat_to_openai_function_messagesOpenAIFunctionsAgentOutputParserWikipediaQueryRunWikipediaAPIWrapperChatPromptValueChatPromptTemplateMessagesPlaceholderChatOpenAIwiki = WikipediaQueryRun(    api_wrapper=WikipediaAPIWrapper(top_k_results=5, doc_content_chars_max=10_000))tools = [wiki]prompt = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful assistant\"),        (\"user\", \"{input}\"),        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),    ])llm = ChatOpenAI(model=\"gpt-3.5-turbo\")Let\\'s try a many-step question without any prompt size handling:agent = (    {        \"input\": itemgetter(\"input\"),        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(            x[\"intermediate_steps\"]        ),    }    | prompt    | llm.bind_functions(tools)    | OpenAIFunctionsAgentOutputParser())agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)agent_executor.invoke(    {        \"input\": \"Who is the current US president? What\\'s their home state? What\\'s their home state\\'s bird? What\\'s that bird\\'s scientific name?\"    })\\x1b[1m> Entering new AgentExecutor chain...\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `List of presidents of the United States`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: List of presidents of the United StatesSummary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of individuals who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. president to have served more than two terms. Since the ratification of the Twenty-second Amendment to the United States Constitution in 1951, no person may be elected president more than twice, and no one who has served more than two years of a term to which someone else was elected may be elected more than once.Four presidents died in office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. Harding, and Franklin D. Roosevelt), four were assassinated (Abraham Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one resigned (Richard Nixon, facing impeachment and removal from office). John Tyler was the first vice president to assume the presidency during a presidential term, and set the precedent that a vice president who does so becomes the fully functioning president with his presidency.Throughout most of its history, American politics has been dominated by political parties. The Constitution is silent on the issue of political parties, and at the time it came into force in 1789, no organized parties existed. Soon after the 1st Congress convened, political factions began rallying around dominant Washington administration officials, such as Alexander Hamilton and Thomas Jefferson. Concerned about the capacity of political parties to destroy the fragile unity holding the nation together, Washington remained unaffiliated with any political faction or party throughout his eight-year presidency. He was, and remains, the only U.S. president never affiliated with a political party.Page: List of presidents of the United States by ageSummary: In this list of presidents of the United States by age, the first table charts the age of each president of the United States at the time of presidential inauguration (first inauguration if elected to multiple and consecutive terms), upon leaving office, and at the time of death. Where the president is still living, their lifespan and post-presidency timespan are calculated up to January 25, 2024.Page: List of vice presidents of the United StatesSummary: There have been 49 vice presidents of the United States since the office was created in 1789. Originally, the vice president was the person who received the second-most votes for president in the Electoral College. But after the election of 1800 produced a tie between Thomas Jefferson and Aaron Burr, requiring the House of Representatives to choose between them, lawmakers acted to prevent such a situation from recurring. The Twelfth Amendment was added to the Constitution in 1804, creating the current system where electors cast a separate ballot for the vice presidency.The vice president is the first person in the presidential line of successionâ€”that is, they assume the presidency if the president dies, resigns, or is impeached and removed from office. Nine vice presidents have ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, Harry S. Truman, and Lyndon B. Johnson) through the president\\'s death and one (Gerald Ford) through the president\\'s resignation. The vice president also serves as the president of the Senate and may choose to cast a tie-breaking vote on decisions made by the Senate. Vice presidents have exercised this latter power to varying extents over the years.Before adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the office of the vice president could not be filled until the next post-election inauguration. Several such vacancies occurred: seven vice presidents died, one resigned and eight succeeded to the presidency. This amendment allowed for a vacancy to be filled through appointment by the president and confirmation by both chambers of the Congress. Since its ratification, the vice presidency has been vacant twice (both in the context of scandals surrounding the Nixon administration) and was filled both times through this process, namely in 1973 following Spiro Agnew\\'s resignation, and again in 1974 after Gerald Ford succeeded to the presidency. The amendment also established a procedure whereby a vice president may, if the president is unable to discharge the powers and duties of the office, temporarily assume the powers and duties of the office as acting president. Three vice presidents have briefly acted as president under the 25th Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, and on July 21, 2007; and Kamala Harris on November 19, 2021.The persons who have served as vice president were born in or primarily affiliated with 27 states plus the District of Columbia. New York has produced the most of any state as eight have been born there and three others considered it their home state. Most vice presidents have been in their 50s or 60s and had political experience before assuming the office. Two vice presidentsâ€”George Clinton and John C. Calhounâ€”served under more than one president. Ill with tuberculosis and recovering in Cuba on Inauguration Day in 1853, William R. King, by an Act of Congress, was allowed to take the oath outside the United States. He is the only vice president to take his oath of office in a foreign country.Page: List of presidents of the United States by net worthSummary: The list of presidents of the United States by net worth at peak varies greatly. Debt and depreciation often means that presidents\\' net worth is less than $0 at the time of death. Most presidents before 1845 were extremely wealthy, especially Andrew Jackson and George Washington.    Presidents since 1929, when Herbert Hoover took office, have generally been wealthier than presidents of the late nineteenth and early twentieth centuries; with the exception of Harry S. Truman, all presidents since this time have been millionaires. These presidents have often received income from autobiographies and other writing. Except for Franklin D. Roosevelt and John F. Kennedy (both of whom died while in office), all presidents beginning with Calvin Coolidge have written autobiographies. In addition, many presidentsâ€”including Bill Clintonâ€”have earned considerable income from public speaking after leaving office.The richest president in history may be Donald Trump. However, his net worth is not precisely known because the Trump Organization is privately held.Truman was among the poorest U.S. presidents, with a net worth considerably less than $1 million. His financial situation contributed to the doubling of the presidential salary to $100,000 in 1949. In addition, the presidential pension was created in 1958 when Truman was again experiencing financial difficulties. Harry and Bess Truman received the first Medicare cards in 1966 via the Social Security Act of 1965.Page: List of presidents of the United States by home stateSummary: These lists give the states of primary affiliation and of birth for each president of the United States.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Joe Biden`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Joe BidenSummary: Joseph Robinette Biden Jr. (  BY-dÉ™n; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He graduated from the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and he was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. He became the oldest president in U.S. history, and the first to have a female vice president.As president, Biden signed the American Rescue Plan Act in response to the COVID-19 pandemic and subsequent recession. He signed bipartisan bills on infrastructure and manufacturing. He proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that he signed into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme Court. He worked with congressional Republicans to resolve the 2023 United States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America\\'s membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. During the Israelâ€“Hamas war, Biden announced military support for Israel, and condemned the actions of Hamas and other Palestinian militants as terrorism. In April 2023, Biden announced his candidacy for the Democratic nomination in the 2024 presidential election.Page: Presidency of Joe BidenSummary: Joe Biden\\'s tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president for two terms under president Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Biden won the presidency with a popular vote of over 81 million, the highest number of votes cast for a single United States presidential candidate. Upon his inauguration, he became the oldest president in American history, breaking the record set by his predecessor Trump. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump\\'s energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump\\'s border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan Infrastructure Investment and Jobs Act; a ten-year plan brokered by Biden alongside Democrats and Republicans in Congress, to invest in American roads, bridges, public transit, ports and broadband access. Biden signed the Juneteenth National Independence Day Act, making Juneteenth a federal holiday in the United States. He appointed Ketanji Brown Jackson to the U.S. Supreme Courtâ€”the first Black woman to serve on the court. After The Supreme Court overturned Roe v. Wade, Biden took executive actions, such as the signing of Executive Order 14076, to preserve and protect women\\'s health rights nationwide, against abortion bans in Republican led states. Biden proposed a significant expansion of the U.S. social safety net through the Build Back Better Act, but those efforts, along with voting rights legislation, failed in Congress. However, in August 2022, Biden signed the Inflation Reduction Act of 2022, a domestic appropriations bill that included some of the provisions of the Build Back Better Act after the entire bill failed to pass. It included significant federal investment in climate and domestic clean energy production, tax credits for solar panels, electric cars and other home energy programs as well as a three-year extension of Affordable Care Act subsidies. The administration\\'s economic policies, known as \"Bidenomics\", were inspired and designed by Trickle-up economics. Described as growing the economy from the middle out and bottom up and growing the middle class. Biden signed the CHIPS and Science Act, bolstering the semiconductor and manufacturing industry, the Honoring our PACT Act, expanding health care for US veterans, the Bipartisan Safer Communities Act and the Electoral Count Reform and Presidential Transition Improvement Act. In late 2022, Biden signed the Respect for Marriage Act, which repealed the Defense of Marriage Act and codified same-sex and interracial marriage in the United States. In response to the debt-ceiling crisis of 2023, Biden negotiated and signed the Fiscal Responsibility Act of 2023, which restrains federal spending for fiscal years 2024 and 2025, implements minor changes to SNAP and TANF, includes energy permitting reform, claws back some IRS funding and unspent money for COVID-19, and suspends the debt ceiling to January 1, 2025. Biden established the American Climate Corps and created the first ever White House Office of Gun Violence Prevention. On September 26, 2023, Joe Biden visited a United Auto Workers picket line during the 2023 United Auto Workers strike, making him the first US president to visit one.The foreign policy goal of the Biden administration is to restore the US to a \"position of trusted leadership\" among global democracies in order to address the challenges posed by Russia and China. In foreign policy, Biden completed the withdrawal of U.S. military forces from Afghanistan, declaring an end to nation-building efforts and shifting U.S. foreign policy toward strategic competition with China and, to a lesser extent, Russia. However, during the withdrawal, the Afghan government collapsed and the Taliban seized control, leading to Biden receiving bipartisan criticism. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia as well as providing Ukraine with over $100 billion in combined military, economic, and humanitarian aid. Biden also approved a raid which led to the death of Abu Ibrahim al-Hashimi al-Qurashi, the leader of the Islamic State, and approved a drone strike which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden signed and created AUKUS, an international security alliance, together with Australia and the United Kingdom. Biden called for the expansion of NATO with the addition of Finland and Sweden, and rallied NATO allies in support of Ukraine. During the 2023 Israelâ€“Hamas war, Biden condemned Hamas and other Palestinian militants as terrorism and announced American military support for Israel; Biden also showed his support and sympathy towards Palestinians affected by the war, sent humanitarian aid, and brokered a four-day temporary pause and hostage exchange.Page: Family of Joe BidenSummary: Joe Biden, the 46th and current president of the United States, has family members who are prominent in law, education, activism and politics. Biden\\'s immediate family became the first family of the United States on his inauguration on January 20, 2021. His immediate family circle was also the second family of the United States from 2009 to 2017, when Biden was vice president. Biden\\'s family is mostly descended from the British Isles, with most of their ancestors coming from Ireland and England, and a smaller number descending from the French.Of Joe Biden\\'s sixteen great-great-grandparents, ten were born in Ireland. He is descended from the Blewitts of County Mayo and the Finnegans of County Louth. One of Biden\\'s great-great-great-grandfathers was born in Sussex, England, and emigrated to Maryland in the United States by 1820.Page: Inauguration of Joe BidenSummary: The inauguration of Joe Biden as the 46th president of the United States took place on Wednesday, January 20, 2021, marking the start of the four-year term of Joe Biden as president and Kamala Harris as vice president. The 59th presidential inauguration took place on the West Front of the United States Capitol in Washington, D.C. Biden took the presidential oath of office, before which Harris took the vice presidential oath of office.The inauguration took place amidst extraordinary political, public health, economic, and national security crises, including the ongoing COVID-19 pandemic; outgoing President Donald Trump\\'s attempts to overturn the 2020 United States presidential election, which provoked an attack on the United States Capitol on January 6; Trump\\'\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Delaware`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: DelawareSummary: Delaware (  DEL-É™-wair) is a state in the northeast and Mid-Atlantic regions of the United States. It borders Maryland to its south and west, Pennsylvania to its north, New Jersey to its northeast, and the Atlantic Ocean to its east. The state\\'s name derives from the adjacent Delaware Bay, which in turn was named after Thomas West, 3rd Baron De La Warr, an English nobleman and the Colony of Virginia\\'s first colonial-era governor.Delaware occupies the northeastern portion of the Delmarva Peninsula, and some islands and territory within the Delaware River. It is the 2nd smallest and 6th least populous state, but also the 6th most densely populated. Delaware\\'s most populous city is Wilmington, and the state\\'s capital is Dover, the 2nd most populous city in Delaware. The state is divided into three counties, the fewest number of counties of any of the 50 U.S. states; from north to south, the three counties are: New Castle County, Kent County, and Sussex County.The southern two counties, Kent and Sussex counties, historically have been predominantly agrarian economies. New Castle is more urbanized and is considered part of the Delaware Valley metropolitan statistical area that surrounds and includes Philadelphia, the nation\\'s 6th most populous city. Delaware is considered part of the Southern United States by the U.S. Census Bureau, but the state\\'s geography, culture, and history are a hybrid of the Mid-Atlantic and Northeastern regions of the country.Before Delaware coastline was explored and developed by Europeans in the 16th century, the state was inhabited by several Native Americans tribes, including the Lenape in the north and Nanticoke in the south. The state was first colonized by Dutch traders at Zwaanendael, near present-day Lewes, Delaware, in 1631.Delaware was one of the Thirteen Colonies that participated in the American Revolution and American Revolutionary War, in which the American Continental Army, led by George Washington, defeated the British, ended British colonization and establishing the United States as a sovereign and independent nation.On December 7, 1787, Delaware was the first state to ratify the Constitution of the United States, earning it the nickname \"The First State\".Since the turn of the 20th century, Delaware has become an onshore corporate haven whose corporate laws are deemed appealing to corporations; over half of all New York Stock Exchange-listed corporations and over three-fifths of the Fortune 500 is legally incorporated in the state.Page: Delaware City, DelawareSummary: Delaware City is a city in New Castle County, Delaware, United States. The population was 1,885 as of 2020. It is a small port town on the eastern terminus of the Chesapeake and Delaware Canal and is the location of the Forts Ferry Crossing to Fort Delaware on Pea Patch Island.Page: Delaware RiverSummary: The Delaware River is a major river in the Mid-Atlantic region of the United States and is the longest free-flowing (undammed) river in the Eastern United States. From the meeting of its branches in Hancock, New York, the river flows for 282 miles (454 km) along the borders of New York, Pennsylvania, New Jersey, and Delaware, before emptying into Delaware Bay.The river has been recognized by the National Wildlife Federation as one of the country\\'s Great Waters and has been called the \"Lifeblood of the Northeast\" by American Rivers. Its watershed drains an area of 13,539 square miles (35,070 km2) and provides drinking water for 17 million people, including half of New York City via the Delaware Aqueduct.The Delaware River has two branches that rise in the Catskill Mountains of New York: the West Branch at Mount Jefferson in Jefferson, Schoharie County, and the East Branch at Grand Gorge, Delaware County. The branches merge to form the main Delaware River at Hancock, New York. Flowing south, the river remains relatively undeveloped, with 152 miles (245 km) protected as the Upper, Middle, and Lower Delaware National Scenic Rivers. At Trenton, New Jersey, the Delaware becomes tidal, navigable, and significantly more industrial. This section forms the backbone of the Delaware Valley metropolitan area, serving the port cities of Philadelphia, Camden, New Jersey, and Wilmington, Delaware. The river flows into Delaware Bay at Liston Point, 48 miles (77 km) upstream of the bay\\'s outlet to the Atlantic Ocean between Cape May and Cape Henlopen.Before the arrival of European settlers, the river was the homeland of the Lenape native people. They called the river Lenapewihittuk, or Lenape River, and Kithanne, meaning the largest river in this part of the country.In 1609, the river was visited by a Dutch East India Company expedition led by Henry Hudson. Hudson, an English navigator, was hired to find a western route to Cathay (China), but his encounters set the stage for Dutch colonization of North America in the 17th century. Early Dutch and Swedish settlements were established along the lower section of the river and Delaware Bay. Both colonial powers called the river the South River (Zuidrivier), compared to the Hudson River, which was known as the North River. After the English expelled the Dutch and took control of the New Netherland colony in 1664, the river was renamed Delaware after Sir Thomas West, 3rd Baron De La Warr, an English nobleman and the Virginia colony\\'s first royal governor, who defended the colony during the First Anglo-Powhatan War.Page: University of DelawareSummary: The University of Delaware (colloquially known as UD or Delaware) is a privately governed, state-assisted land-grant research university located in Newark, Delaware. UD is the largest university in Delaware. It offers three associate\\'s programs, 148 bachelor\\'s programs, 121 master\\'s programs (with 13 joint degrees), and 55 doctoral programs across its eight colleges. The main campus is in Newark, with satellite campuses in Dover, Wilmington, Lewes, and Georgetown. It is considered a large institution with approximately 18,200 undergraduate and 4,200 graduate students. It is a privately governed university which receives public funding for being a land-grant, sea-grant, and space-grant state-supported research institution.UD is classified among \"R1: Doctoral Universities â€“ Very high research activity\". According to the National Science Foundation, UD spent $186 million on research and development in 2018, ranking it 119th in the nation.  It is recognized with the Community Engagement Classification by the Carnegie Foundation for the Advancement of Teaching.UD students, alumni, and sports teams are known as the \"Fightin\\' Blue Hens\", more commonly shortened to \"Blue Hens\", and the school colors are Delaware blue and gold. UD sponsors 21 men\\'s and women\\'s NCAA Division-I sports teams and have competed in the Colonial Athletic Association (CAA) since 2001.Page: LenapeSummary: The Lenape (English: , , ; Lenape languages: [lÉ™naË�pe]), also called the Lenni Lenape and Delaware people, are an Indigenous people of the Northeastern Woodlands, who live in the United States and Canada.The Lenape\\'s historical territory includes present-day northeastern Delaware, all of New Jersey, the eastern Pennsylvania regions of the Lehigh Valley and Northeastern Pennsylvania, and New York Bay, western Long Island, and the lower Hudson Valley in New York state. Today they are based in Oklahoma, Wisconsin, and Ontario.During the last decades of the 18th century, European settlers and the effects of the American Revolutionary War displaced most Lenape from their homelands and pushed them north and west. In the 1860s, under the Indian removal policy, the U.S. federal government relocated most Lenape remaining in the Eastern United States to the Indian Territory and surrounding regions. Lenape people currently belong to the Delaware Nation and Delaware Tribe of Indians in Oklahoma, the Stockbridgeâ€“Munsee Community in Wisconsin, and the Munsee-Delaware Nation, Moravian of the Thames First Nation, and Delaware of Six Nations in Ontario.\\x1b[0m---------------------------------------------------------------------------``````outputBadRequestError                           Traceback (most recent call last)``````outputCell In[11], line 14      1 agent = (      2     {      3         \"input\": itemgetter(\"input\"),   (...)     10     | OpenAIFunctionsAgentOutputParser()     11 )     13 agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)---> 14 agent_executor.invoke(     15     {     16         \"input\": \"Who is the current US president? What\\'s their home state? What\\'s their home state\\'s bird? What\\'s that bird\\'s scientific name?\"     17     }     18 )``````outputFile ~/langchain/libs/langchain/langchain/chains/base.py:162, in Chain.invoke(self, input, config, **kwargs)    160 except BaseException as e:    161     run_manager.on_chain_error(e)--> 162     raise e    163 run_manager.on_chain_end(outputs)    164 final_outputs: Dict[str, Any] = self.prep_outputs(    165     inputs, outputs, return_only_outputs    166 )``````outputFile ~/langchain/libs/langchain/langchain/chains/base.py:156, in Chain.invoke(self, input, config, **kwargs)    149 run_manager = callback_manager.on_chain_start(    150     dumpd(self),    151     inputs,    152     name=run_name,    153 )    154 try:    155     outputs = (--> 156         self._call(inputs, run_manager=run_manager)    157         if new_arg_supported    158         else self._call(inputs)    159     )    160 except BaseException as e:    161     run_manager.on_chain_error(e)``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:1391, in AgentExecutor._call(self, inputs, run_manager)   1389 # We now enter the agent loop (until it returns something).   1390 while self._should_continue(iterations, time_elapsed):-> 1391     next_step_output = self._take_next_step(   1392         name_to_tool_map,   1393         color_mapping,   1394         inputs,   1395         intermediate_steps,   1396         run_manager=run_manager,   1397     )   1398     if isinstance(next_step_output, AgentFinish):   1399         return self._return(   1400             next_step_output, intermediate_steps, run_manager=run_manager   1401         )``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:1097, in AgentExecutor._take_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)   1088 def _take_next_step(   1089     self,   1090     name_to_tool_map: Dict[str, BaseTool],   (...)   1094     run_manager: Optional[CallbackManagerForChainRun] = None,   1095 ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:   1096     return self._consume_next_step(-> 1097         [   1098             a   1099             for a in self._iter_next_step(   1100                 name_to_tool_map,   1101                 color_mapping,   1102                 inputs,   1103                 intermediate_steps,   1104                 run_manager,   1105             )   1106         ]   1107     )``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:1097, in <listcomp>(.0)   1088 def _take_next_step(   1089     self,   1090     name_to_tool_map: Dict[str, BaseTool],   (...)   1094     run_manager: Optional[CallbackManagerForChainRun] = None,   1095 ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:   1096     return self._consume_next_step(-> 1097         [   1098             a   1099             for a in self._iter_next_step(   1100                 name_to_tool_map,   1101                 color_mapping,   1102                 inputs,   1103                 intermediate_steps,   1104                 run_manager,   1105             )   1106         ]   1107     )``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:1125, in AgentExecutor._iter_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)   1122     intermediate_steps = self._prepare_intermediate_steps(intermediate_steps)   1124     # Call the LLM to see what to do.-> 1125     output = self.agent.plan(   1126         intermediate_steps,   1127         callbacks=run_manager.get_child() if run_manager else None,   1128         **inputs,   1129     )   1130 except OutputParserException as e:   1131     if isinstance(self.handle_parsing_errors, bool):``````outputFile ~/langchain/libs/langchain/langchain/agents/agent.py:387, in RunnableAgent.plan(self, intermediate_steps, callbacks, **kwargs)    381 # Use streaming to make sure that the underlying LLM is invoked in a streaming    382 # fashion to make it possible to get access to the individual LLM tokens    383 # when using stream_log with the Agent Executor.    384 # Because the response from the plan is not a generator, we need to    385 # accumulate the output into final output and return that.    386 final_output: Any = None--> 387 for chunk in self.runnable.stream(inputs, config={\"callbacks\": callbacks}):    388     if final_output is None:    389         final_output = chunk``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:2424, in RunnableSequence.stream(self, input, config, **kwargs)   2418 def stream(   2419     self,   2420     input: Input,   2421     config: Optional[RunnableConfig] = None,   2422     **kwargs: Optional[Any],   2423 ) -> Iterator[Output]:-> 2424     yield from self.transform(iter([input]), config, **kwargs)``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:2411, in RunnableSequence.transform(self, input, config, **kwargs)   2405 def transform(   2406     self,   2407     input: Iterator[Input],   2408     config: Optional[RunnableConfig] = None,   2409     **kwargs: Optional[Any],   2410 ) -> Iterator[Output]:-> 2411     yield from self._transform_stream_with_config(   2412         input,   2413         self._transform,   2414         patch_config(config, run_name=(config or {}).get(\"run_name\") or self.name),   2415         **kwargs,   2416     )``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:1497, in Runnable._transform_stream_with_config(self, input, transformer, config, run_type, **kwargs)   1495 try:   1496     while True:-> 1497         chunk: Output = context.run(next, iterator)  # type: ignore   1498         yield chunk   1499         if final_output_supported:``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:2375, in RunnableSequence._transform(self, input, run_manager, config)   2366 for step in steps:   2367     final_pipeline = step.transform(   2368         final_pipeline,   2369         patch_config(   (...)   2372         ),   2373     )-> 2375 for output in final_pipeline:   2376     yield output``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:1035, in Runnable.transform(self, input, config, **kwargs)   1032 final: Input   1033 got_first_val = False-> 1035 for chunk in input:   1036     if not got_first_val:   1037         final = chunk``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:3991, in RunnableBindingBase.transform(self, input, config, **kwargs)   3985 def transform(   3986     self,   3987     input: Iterator[Input],   3988     config: Optional[RunnableConfig] = None,   3989     **kwargs: Any,   3990 ) -> Iterator[Output]:-> 3991     yield from self.bound.transform(   3992         input,   3993         self._merge_configs(config),   3994         **{**self.kwargs, **kwargs},   3995     )``````outputFile ~/langchain/libs/core/langchain_core/runnables/base.py:1045, in Runnable.transform(self, input, config, **kwargs)   1042         final = final + chunk  # type: ignore[operator]   1044 if got_first_val:-> 1045     yield from self.stream(final, config, **kwargs)``````outputFile ~/langchain/libs/core/langchain_core/language_models/chat_models.py:249, in BaseChatModel.stream(self, input, config, stop, **kwargs)    242 except BaseException as e:    243     run_manager.on_llm_error(    244         e,    245         response=LLMResult(    246             generations=[[generation]] if generation else []    247         ),    248     )--> 249     raise e    250 else:    251     run_manager.on_llm_end(LLMResult(generations=[[generation]]))``````outputFile ~/langchain/libs/core/langchain_core/language_models/chat_models.py:233, in BaseChatModel.stream(self, input, config, stop, **kwargs)    231 generation: Optional[ChatGenerationChunk] = None    232 try:--> 233     for chunk in self._stream(    234         messages, stop=stop, run_manager=run_manager, **kwargs    235     ):    236         yield chunk.message    237         if generation is None:``````outputFile ~/langchain/libs/partners/openai/langchain_openai/chat_models/base.py:403, in ChatOpenAI._stream(self, messages, stop, run_manager, **kwargs)    400 params = {**params, **kwargs, \"stream\": True}    402 default_chunk_class = AIMessageChunk--> 403 for chunk in self.client.create(messages=message_dicts, **params):    404     if not isinstance(chunk, dict):    405         chunk = chunk.dict()``````outputFile ~/langchain/.venv/lib/python3.9/site-packages/openai/_utils/_utils.py:271, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)    269             msg = f\"Missing required argument: {quote(missing[0])}\"    270     raise TypeError(msg)--> 271 return func(*args, **kwargs)``````outputFile ~/langchain/.venv/lib/python3.9/site-packages/openai/resources/chat/completions.py:648, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)    599 @required_args([\"messages\", \"model\"], [\"messages\", \"model\", \"stream\"])    600 def create(    601     self,   (...)    646     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,    647 ) -> ChatCompletion | Stream[ChatCompletionChunk]:--> 648     return self._post(    649         \"/chat/completions\",    650         body=maybe_transform(    651             {    652                 \"messages\": messages,    653                 \"model\": model,    654                 \"frequency_penalty\": frequency_penalty,    655                 \"function_call\": function_call,    656                 \"functions\": functions,    657                 \"logit_bias\": logit_bias,    658                 \"logprobs\": logprobs,    659                 \"max_tokens\": max_tokens,    660                 \"n\": n,    661                 \"presence_penalty\": presence_penalty,    662                 \"response_format\": response_format,    663                 \"seed\": seed,    664                 \"stop\": stop,    665                 \"stream\": stream,    666                 \"temperature\": temperature,    667                 \"tool_choice\": tool_choice,    668                 \"tools\": tools,    669                 \"top_logprobs\": top_logprobs,    670                 \"top_p\": top_p,    671                 \"user\": user,    672             },    673             completion_create_params.CompletionCreateParams,    674         ),    675         options=make_request_options(    676             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout    677         ),    678         cast_to=ChatCompletion,    679         stream=stream or False,    680         stream_cls=Stream[ChatCompletionChunk],    681     )``````outputFile ~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:1179, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)   1165 def post(   1166     self,   1167     path: str,   (...)   1174     stream_cls: type[_StreamT] | None = None,   1175 ) -> ResponseT | _StreamT:   1176     opts = FinalRequestOptions.construct(   1177         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options   1178     )-> 1179     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))``````outputFile ~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:868, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)    859 def request(    860     self,    861     cast_to: Type[ResponseT],   (...)    866     stream_cls: type[_StreamT] | None = None,    867 ) -> ResponseT | _StreamT:--> 868     return self._request(    869         cast_to=cast_to,    870         options=options,    871         stream=stream,    872         stream_cls=stream_cls,    873         remaining_retries=remaining_retries,    874     )``````outputFile ~/langchain/.venv/lib/python3.9/site-packages/openai/_base_client.py:959, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)    956         err.response.read()    958     log.debug(\"Re-raising status error\")--> 959     raise self._make_status_error_from_response(err.response) from None    961 return self._process_response(    962     cast_to=cast_to,    963     options=options,   (...)    966     stream_cls=stream_cls,    967 )``````outputBadRequestError: Error code: 400 - {\\'error\\': {\\'message\\': \"This model\\'s maximum context length is 4097 tokens. However, your messages resulted in 5487 tokens (5419 in the messages, 68 in the functions). Please reduce the length of the messages or functions.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': \\'messages\\', \\'code\\': \\'context_length_exceeded\\'}}tipLangSmith traceUnfortunately we run out of space in our model\\'s context window before we the agent can get to the final answer. Now let\\'s add some prompt handling logic. To keep things simple, if our messages have too many tokens we\\'ll start dropping the earliest AI, Function message pairs (this is the model tool invocation message and the subsequent tool output message) in the chat history.def condense_prompt(prompt: ChatPromptValue) -> ChatPromptValue:    messages = prompt.to_messages()    num_tokens = llm.get_num_tokens_from_messages(messages)    ai_function_messages = messages[2:]    while num_tokens > 4_000:        ai_function_messages = ai_function_messages[2:]        num_tokens = llm.get_num_tokens_from_messages(            messages[:2] + ai_function_messages        )    messages = messages[:2] + ai_function_messages    return ChatPromptValue(messages=messages)agent = (    {        \"input\": itemgetter(\"input\"),        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(            x[\"intermediate_steps\"]        ),    }    | prompt    | condense_prompt    | llm.bind_functions(tools)    | OpenAIFunctionsAgentOutputParser())agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)agent_executor.invoke(    {        \"input\": \"Who is the current US president? What\\'s their home state? What\\'s their home state\\'s bird? What\\'s that bird\\'s scientific name?\"    })\\x1b[1m> Entering new AgentExecutor chain...\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `List of presidents of the United States`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: List of presidents of the United StatesSummary: The president of the United States is the head of state and head of government of the United States, indirectly elected to a four-year term via the Electoral College. The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College. Grover Cleveland served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of individuals who have served as president. The incumbent president is Joe Biden.The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history. Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. president to have served more than two terms. Since the ratification of the Twenty-second Amendment to the United States Constitution in 1951, no person may be elected president more than twice, and no one who has served more than two years of a term to which someone else was elected may be elected more than once.Four presidents died in office of natural causes (William Henry Harrison, Zachary Taylor, Warren G. Harding, and Franklin D. Roosevelt), four were assassinated (Abraham Lincoln, James A. Garfield, William McKinley, and John F. Kennedy), and one resigned (Richard Nixon, facing impeachment and removal from office). John Tyler was the first vice president to assume the presidency during a presidential term, and set the precedent that a vice president who does so becomes the fully functioning president with his presidency.Throughout most of its history, American politics has been dominated by political parties. The Constitution is silent on the issue of political parties, and at the time it came into force in 1789, no organized parties existed. Soon after the 1st Congress convened, political factions began rallying around dominant Washington administration officials, such as Alexander Hamilton and Thomas Jefferson. Concerned about the capacity of political parties to destroy the fragile unity holding the nation together, Washington remained unaffiliated with any political faction or party throughout his eight-year presidency. He was, and remains, the only U.S. president never affiliated with a political party.Page: List of presidents of the United States by ageSummary: In this list of presidents of the United States by age, the first table charts the age of each president of the United States at the time of presidential inauguration (first inauguration if elected to multiple and consecutive terms), upon leaving office, and at the time of death. Where the president is still living, their lifespan and post-presidency timespan are calculated up to January 25, 2024.Page: List of vice presidents of the United StatesSummary: There have been 49 vice presidents of the United States since the office was created in 1789. Originally, the vice president was the person who received the second-most votes for president in the Electoral College. But after the election of 1800 produced a tie between Thomas Jefferson and Aaron Burr, requiring the House of Representatives to choose between them, lawmakers acted to prevent such a situation from recurring. The Twelfth Amendment was added to the Constitution in 1804, creating the current system where electors cast a separate ballot for the vice presidency.The vice president is the first person in the presidential line of successionâ€”that is, they assume the presidency if the president dies, resigns, or is impeached and removed from office. Nine vice presidents have ascended to the presidency in this way: eight (John Tyler, Millard Fillmore, Andrew Johnson, Chester A. Arthur, Theodore Roosevelt, Calvin Coolidge, Harry S. Truman, and Lyndon B. Johnson) through the president\\'s death and one (Gerald Ford) through the president\\'s resignation. The vice president also serves as the president of the Senate and may choose to cast a tie-breaking vote on decisions made by the Senate. Vice presidents have exercised this latter power to varying extents over the years.Before adoption of the Twenty-fifth Amendment in 1967, an intra-term vacancy in the office of the vice president could not be filled until the next post-election inauguration. Several such vacancies occurred: seven vice presidents died, one resigned and eight succeeded to the presidency. This amendment allowed for a vacancy to be filled through appointment by the president and confirmation by both chambers of the Congress. Since its ratification, the vice presidency has been vacant twice (both in the context of scandals surrounding the Nixon administration) and was filled both times through this process, namely in 1973 following Spiro Agnew\\'s resignation, and again in 1974 after Gerald Ford succeeded to the presidency. The amendment also established a procedure whereby a vice president may, if the president is unable to discharge the powers and duties of the office, temporarily assume the powers and duties of the office as acting president. Three vice presidents have briefly acted as president under the 25th Amendment: George H. W. Bush on July 13, 1985; Dick Cheney on June 29, 2002, and on July 21, 2007; and Kamala Harris on November 19, 2021.The persons who have served as vice president were born in or primarily affiliated with 27 states plus the District of Columbia. New York has produced the most of any state as eight have been born there and three others considered it their home state. Most vice presidents have been in their 50s or 60s and had political experience before assuming the office. Two vice presidentsâ€”George Clinton and John C. Calhounâ€”served under more than one president. Ill with tuberculosis and recovering in Cuba on Inauguration Day in 1853, William R. King, by an Act of Congress, was allowed to take the oath outside the United States. He is the only vice president to take his oath of office in a foreign country.Page: List of presidents of the United States by net worthSummary: The list of presidents of the United States by net worth at peak varies greatly. Debt and depreciation often means that presidents\\' net worth is less than $0 at the time of death. Most presidents before 1845 were extremely wealthy, especially Andrew Jackson and George Washington.    Presidents since 1929, when Herbert Hoover took office, have generally been wealthier than presidents of the late nineteenth and early twentieth centuries; with the exception of Harry S. Truman, all presidents since this time have been millionaires. These presidents have often received income from autobiographies and other writing. Except for Franklin D. Roosevelt and John F. Kennedy (both of whom died while in office), all presidents beginning with Calvin Coolidge have written autobiographies. In addition, many presidentsâ€”including Bill Clintonâ€”have earned considerable income from public speaking after leaving office.The richest president in history may be Donald Trump. However, his net worth is not precisely known because the Trump Organization is privately held.Truman was among the poorest U.S. presidents, with a net worth considerably less than $1 million. His financial situation contributed to the doubling of the presidential salary to $100,000 in 1949. In addition, the presidential pension was created in 1958 when Truman was again experiencing financial difficulties. Harry and Bess Truman received the first Medicare cards in 1966 via the Social Security Act of 1965.Page: List of presidents of the United States by home stateSummary: These lists give the states of primary affiliation and of birth for each president of the United States.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Joe Biden`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Joe BidenSummary: Joseph Robinette Biden Jr. (  BY-dÉ™n; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He graduated from the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and he was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. He became the oldest president in U.S. history, and the first to have a female vice president.As president, Biden signed the American Rescue Plan Act in response to the COVID-19 pandemic and subsequent recession. He signed bipartisan bills on infrastructure and manufacturing. He proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that he signed into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme Court. He worked with congressional Republicans to resolve the 2023 United States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America\\'s membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. During the Israelâ€“Hamas war, Biden announced military support for Israel, and condemned the actions of Hamas and other Palestinian militants as terrorism. In April 2023, Biden announced his candidacy for the Democratic nomination in the 2024 presidential election.Page: Presidency of Joe BidenSummary: Joe Biden\\'s tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president for two terms under president Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Biden won the presidency with a popular vote of over 81 million, the highest number of votes cast for a single United States presidential candidate. Upon his inauguration, he became the oldest president in American history, breaking the record set by his predecessor Trump. Biden entered office amid the COVID-19 pandemic, an economic crisis, and increased political polarization.On the first day of his presidency, Biden made an effort to revert President Trump\\'s energy policy by restoring U.S. participation in the Paris Agreement and revoking the permit for the Keystone XL pipeline. He also halted funding for Trump\\'s border wall, an expansion of the Mexican border wall. On his second day, he issued a series of executive orders to reduce the impact of COVID-19, including invoking the Defense Production Act of 1950, and set an early goal of achieving one hundred million COVID-19 vaccinations in the United States in his first 100 days.Biden signed into law the American Rescue Plan Act of 2021; a $1.9 trillion stimulus bill that temporarily established expanded unemployment insurance and sent $1,400 stimulus checks to most Americans in response to continued economic pressure from COVID-19. He signed the bipartisan Infrastructure Investment and Jobs Act; a ten-year plan brokered by Biden alongside Democrats and Republicans in Congress, to invest in American roads, bridges, public transit, ports and broadband access. Biden signed the Juneteenth National Independence Day Act, making Juneteenth a federal holiday in the United States. He appointed Ketanji Brown Jackson to the U.S. Supreme Courtâ€”the first Black woman to serve on the court. After The Supreme Court overturned Roe v. Wade, Biden took executive actions, such as the signing of Executive Order 14076, to preserve and protect women\\'s health rights nationwide, against abortion bans in Republican led states. Biden proposed a significant expansion of the U.S. social safety net through the Build Back Better Act, but those efforts, along with voting rights legislation, failed in Congress. However, in August 2022, Biden signed the Inflation Reduction Act of 2022, a domestic appropriations bill that included some of the provisions of the Build Back Better Act after the entire bill failed to pass. It included significant federal investment in climate and domestic clean energy production, tax credits for solar panels, electric cars and other home energy programs as well as a three-year extension of Affordable Care Act subsidies. The administration\\'s economic policies, known as \"Bidenomics\", were inspired and designed by Trickle-up economics. Described as growing the economy from the middle out and bottom up and growing the middle class. Biden signed the CHIPS and Science Act, bolstering the semiconductor and manufacturing industry, the Honoring our PACT Act, expanding health care for US veterans, the Bipartisan Safer Communities Act and the Electoral Count Reform and Presidential Transition Improvement Act. In late 2022, Biden signed the Respect for Marriage Act, which repealed the Defense of Marriage Act and codified same-sex and interracial marriage in the United States. In response to the debt-ceiling crisis of 2023, Biden negotiated and signed the Fiscal Responsibility Act of 2023, which restrains federal spending for fiscal years 2024 and 2025, implements minor changes to SNAP and TANF, includes energy permitting reform, claws back some IRS funding and unspent money for COVID-19, and suspends the debt ceiling to January 1, 2025. Biden established the American Climate Corps and created the first ever White House Office of Gun Violence Prevention. On September 26, 2023, Joe Biden visited a United Auto Workers picket line during the 2023 United Auto Workers strike, making him the first US president to visit one.The foreign policy goal of the Biden administration is to restore the US to a \"position of trusted leadership\" among global democracies in order to address the challenges posed by Russia and China. In foreign policy, Biden completed the withdrawal of U.S. military forces from Afghanistan, declaring an end to nation-building efforts and shifting U.S. foreign policy toward strategic competition with China and, to a lesser extent, Russia. However, during the withdrawal, the Afghan government collapsed and the Taliban seized control, leading to Biden receiving bipartisan criticism. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia as well as providing Ukraine with over $100 billion in combined military, economic, and humanitarian aid. Biden also approved a raid which led to the death of Abu Ibrahim al-Hashimi al-Qurashi, the leader of the Islamic State, and approved a drone strike which killed Ayman Al Zawahiri, leader of Al-Qaeda. Biden signed and created AUKUS, an international security alliance, together with Australia and the United Kingdom. Biden called for the expansion of NATO with the addition of Finland and Sweden, and rallied NATO allies in support of Ukraine. During the 2023 Israelâ€“Hamas war, Biden condemned Hamas and other Palestinian militants as terrorism and announced American military support for Israel; Biden also showed his support and sympathy towards Palestinians affected by the war, sent humanitarian aid, and brokered a four-day temporary pause and hostage exchange.Page: Family of Joe BidenSummary: Joe Biden, the 46th and current president of the United States, has family members who are prominent in law, education, activism and politics. Biden\\'s immediate family became the first family of the United States on his inauguration on January 20, 2021. His immediate family circle was also the second family of the United States from 2009 to 2017, when Biden was vice president. Biden\\'s family is mostly descended from the British Isles, with most of their ancestors coming from Ireland and England, and a smaller number descending from the French.Of Joe Biden\\'s sixteen great-great-grandparents, ten were born in Ireland. He is descended from the Blewitts of County Mayo and the Finnegans of County Louth. One of Biden\\'s great-great-great-grandfathers was born in Sussex, England, and emigrated to Maryland in the United States by 1820.Page: Inauguration of Joe BidenSummary: The inauguration of Joe Biden as the 46th president of the United States took place on Wednesday, January 20, 2021, marking the start of the four-year term of Joe Biden as president and Kamala Harris as vice president. The 59th presidential inauguration took place on the West Front of the United States Capitol in Washington, D.C. Biden took the presidential oath of office, before which Harris took the vice presidential oath of office.The inauguration took place amidst extraordinary political, public health, economic, and national security crises, including the ongoing COVID-19 pandemic; outgoing President Donald Trump\\'s attempts to overturn the 2020 United States presidential election, which provoked an attack on the United States Capitol on January 6; Trump\\'\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Delaware`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: DelawareSummary: Delaware (  DEL-É™-wair) is a state in the northeast and Mid-Atlantic regions of the United States. It borders Maryland to its south and west, Pennsylvania to its north, New Jersey to its northeast, and the Atlantic Ocean to its east. The state\\'s name derives from the adjacent Delaware Bay, which in turn was named after Thomas West, 3rd Baron De La Warr, an English nobleman and the Colony of Virginia\\'s first colonial-era governor.Delaware occupies the northeastern portion of the Delmarva Peninsula, and some islands and territory within the Delaware River. It is the 2nd smallest and 6th least populous state, but also the 6th most densely populated. Delaware\\'s most populous city is Wilmington, and the state\\'s capital is Dover, the 2nd most populous city in Delaware. The state is divided into three counties, the fewest number of counties of any of the 50 U.S. states; from north to south, the three counties are: New Castle County, Kent County, and Sussex County.The southern two counties, Kent and Sussex counties, historically have been predominantly agrarian economies. New Castle is more urbanized and is considered part of the Delaware Valley metropolitan statistical area that surrounds and includes Philadelphia, the nation\\'s 6th most populous city. Delaware is considered part of the Southern United States by the U.S. Census Bureau, but the state\\'s geography, culture, and history are a hybrid of the Mid-Atlantic and Northeastern regions of the country.Before Delaware coastline was explored and developed by Europeans in the 16th century, the state was inhabited by several Native Americans tribes, including the Lenape in the north and Nanticoke in the south. The state was first colonized by Dutch traders at Zwaanendael, near present-day Lewes, Delaware, in 1631.Delaware was one of the Thirteen Colonies that participated in the American Revolution and American Revolutionary War, in which the American Continental Army, led by George Washington, defeated the British, ended British colonization and establishing the United States as a sovereign and independent nation.On December 7, 1787, Delaware was the first state to ratify the Constitution of the United States, earning it the nickname \"The First State\".Since the turn of the 20th century, Delaware has become an onshore corporate haven whose corporate laws are deemed appealing to corporations; over half of all New York Stock Exchange-listed corporations and over three-fifths of the Fortune 500 is legally incorporated in the state.Page: Delaware City, DelawareSummary: Delaware City is a city in New Castle County, Delaware, United States. The population was 1,885 as of 2020. It is a small port town on the eastern terminus of the Chesapeake and Delaware Canal and is the location of the Forts Ferry Crossing to Fort Delaware on Pea Patch Island.Page: Delaware RiverSummary: The Delaware River is a major river in the Mid-Atlantic region of the United States and is the longest free-flowing (undammed) river in the Eastern United States. From the meeting of its branches in Hancock, New York, the river flows for 282 miles (454 km) along the borders of New York, Pennsylvania, New Jersey, and Delaware, before emptying into Delaware Bay.The river has been recognized by the National Wildlife Federation as one of the country\\'s Great Waters and has been called the \"Lifeblood of the Northeast\" by American Rivers. Its watershed drains an area of 13,539 square miles (35,070 km2) and provides drinking water for 17 million people, including half of New York City via the Delaware Aqueduct.The Delaware River has two branches that rise in the Catskill Mountains of New York: the West Branch at Mount Jefferson in Jefferson, Schoharie County, and the East Branch at Grand Gorge, Delaware County. The branches merge to form the main Delaware River at Hancock, New York. Flowing south, the river remains relatively undeveloped, with 152 miles (245 km) protected as the Upper, Middle, and Lower Delaware National Scenic Rivers. At Trenton, New Jersey, the Delaware becomes tidal, navigable, and significantly more industrial. This section forms the backbone of the Delaware Valley metropolitan area, serving the port cities of Philadelphia, Camden, New Jersey, and Wilmington, Delaware. The river flows into Delaware Bay at Liston Point, 48 miles (77 km) upstream of the bay\\'s outlet to the Atlantic Ocean between Cape May and Cape Henlopen.Before the arrival of European settlers, the river was the homeland of the Lenape native people. They called the river Lenapewihittuk, or Lenape River, and Kithanne, meaning the largest river in this part of the country.In 1609, the river was visited by a Dutch East India Company expedition led by Henry Hudson. Hudson, an English navigator, was hired to find a western route to Cathay (China), but his encounters set the stage for Dutch colonization of North America in the 17th century. Early Dutch and Swedish settlements were established along the lower section of the river and Delaware Bay. Both colonial powers called the river the South River (Zuidrivier), compared to the Hudson River, which was known as the North River. After the English expelled the Dutch and took control of the New Netherland colony in 1664, the river was renamed Delaware after Sir Thomas West, 3rd Baron De La Warr, an English nobleman and the Virginia colony\\'s first royal governor, who defended the colony during the First Anglo-Powhatan War.Page: University of DelawareSummary: The University of Delaware (colloquially known as UD or Delaware) is a privately governed, state-assisted land-grant research university located in Newark, Delaware. UD is the largest university in Delaware. It offers three associate\\'s programs, 148 bachelor\\'s programs, 121 master\\'s programs (with 13 joint degrees), and 55 doctoral programs across its eight colleges. The main campus is in Newark, with satellite campuses in Dover, Wilmington, Lewes, and Georgetown. It is considered a large institution with approximately 18,200 undergraduate and 4,200 graduate students. It is a privately governed university which receives public funding for being a land-grant, sea-grant, and space-grant state-supported research institution.UD is classified among \"R1: Doctoral Universities â€“ Very high research activity\". According to the National Science Foundation, UD spent $186 million on research and development in 2018, ranking it 119th in the nation.  It is recognized with the Community Engagement Classification by the Carnegie Foundation for the Advancement of Teaching.UD students, alumni, and sports teams are known as the \"Fightin\\' Blue Hens\", more commonly shortened to \"Blue Hens\", and the school colors are Delaware blue and gold. UD sponsors 21 men\\'s and women\\'s NCAA Division-I sports teams and have competed in the Colonial Athletic Association (CAA) since 2001.Page: LenapeSummary: The Lenape (English: , , ; Lenape languages: [lÉ™naË�pe]), also called the Lenni Lenape and Delaware people, are an Indigenous people of the Northeastern Woodlands, who live in the United States and Canada.The Lenape\\'s historical territory includes present-day northeastern Delaware, all of New Jersey, the eastern Pennsylvania regions of the Lehigh Valley and Northeastern Pennsylvania, and New York Bay, western Long Island, and the lower Hudson Valley in New York state. Today they are based in Oklahoma, Wisconsin, and Ontario.During the last decades of the 18th century, European settlers and the effects of the American Revolutionary War displaced most Lenape from their homelands and pushed them north and west. In the 1860s, under the Indian removal policy, the U.S. federal government relocated most Lenape remaining in the Eastern United States to the Indian Territory and surrounding regions. Lenape people currently belong to the Delaware Nation and Delaware Tribe of Indians in Oklahoma, the Stockbridgeâ€“Munsee Community in Wisconsin, and the Munsee-Delaware Nation, Moravian of the Thames First Nation, and Delaware of Six Nations in Ontario.\\x1b[0m\\x1b[32;1m\\x1b[1;3mInvoking: `Wikipedia` with `Blue hen chicken`\\x1b[0m\\x1b[36;1m\\x1b[1;3mPage: Delaware Blue HenSummary: The Delaware Blue Hen or Blue Hen of Delaware is a blue strain of American gamecock. Under the name Blue Hen Chicken it is the official bird of the State of Delaware. It is the emblem or mascot of several institutions in the state, among them the sports teams of the University of Delaware.Page: Delaware Fightin\\' Blue HensSummary: The Delaware Fightin\\' Blue Hens are the athletic teams of the University of Delaware (UD) of Newark, Delaware, in the United States. The Blue Hens compete in the Football Championship Subdivision (FCS) of Division I of the National Collegiate Athletic Association (NCAA) as members of the Coastal Athletic Association and its technically separate football league, CAA Football.On November 28, 2023, UD and Conference USA (CUSA) jointly announced that UD would start a transition to the Division I Football Bowl Subdivision (FBS) in 2024 and join CUSA in 2025. UD will continue to compete in both sides of the CAA in 2024â€“25; it will be ineligible for the FCS playoffs due to NCAA rules for transitioning programs, but will be eligible for all non-football CAA championships. Upon joining CUSA, UD will be eligible for all conference championship events except the football championship game; it will become eligible for that event upon completing the FBS transition in 2026. At the same time, UD also announced it would add one women\\'s sport due to Title IX considerations, and would also be seeking conference homes for the seven sports that UD sponsors but CUSA does not. The new women\\'s sport would later be announced as ice hockey; UD will join College Hockey America for its first season of varsity play in 2025â€“26.Page: Brahma chickenSummary: The Brahma is an American breed of chicken. It was bred in the United States from birds imported from the Chinese port of Shanghai,:â€Š78â€Š and was the principal American meat breed from the 1850s until about 1930.Page: SilkieSummary: The Silkie (also known as the Silky or Chinese silk chicken) is a breed of chicken named for its atypically fluffy plumage, which is said to feel like silk and satin. The breed has several other unusual qualities, such as black skin and bones, blue earlobes, and five toes on each foot, whereas most chickens have only four. They are often exhibited in poultry shows, and also appear in various colors. In addition to their distinctive physical characteristics, Silkies are well known for their calm and friendly temperament. It is among the most docile of poultry. Hens are also exceptionally broody, and care for young well. Although they are fair layers themselves, laying only about three eggs a week, they are commonly used to hatch eggs from other breeds and bird species due to their broody nature. Silkie chickens have been bred to have a wide variety of colors which include but are not limited to: Black, Blue, Buff, Partridge, Splash, White, Lavender, Paint and Porcelain.Page: Silverudd BlueSummary: The Silverudd Blue, Swedish: Silverudds BlÃ¥, is a Swedish breed of chicken. It was developed by Martin Silverudd in SmÃ¥land, in southern Sweden. Hens lay blue/green eggs, weighing 50â€“65 grams. The flock-book for the breed is kept by the Svenska KulturhÃ¶nsfÃ¶reningen â€“ the Swedish Cultural Hen Association. It was initially known by various names including Isbar, Blue Isbar and Svensk GrÃ¶nvÃ¤rpare, or \"Swedish green egg layer\"; in 2016 it was renamed to \\'Silverudd Blue\\' after its creator.\\x1b[0m\\x1b[32;1m\\x1b[1;3mThe current US president is Joe Biden. His home state is Delaware. The home state bird of Delaware is the Delaware Blue Hen. The scientific name of the Delaware Blue Hen is Gallus gallus domesticus.\\x1b[0m\\x1b[1m> Finished chain.\\x1b[0m{\\'input\\': \"Who is the current US president? What\\'s their home state? What\\'s their home state\\'s bird? What\\'s that bird\\'s scientific name?\", \\'output\\': \\'The current US president is Joe Biden. His home state is Delaware. The home state bird of Delaware is the Delaware Blue Hen. The scientific name of the Delaware Blue Hen is Gallus gallus domesticus.\\'}tipLangSmith traceHelp us out by providing feedback on this documentation page:PreviousCreate a runnable with the @chain decoratorNextMultiple chainsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### INDEX\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load docs\n",
    "url = \"https://www.theweathernetwork.com/en/city/ca/ontario/oshawa/current\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# # Split into chunks\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=4500, chunk_overlap=200)\n",
    "# splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# # Embed and store in Chroma\n",
    "# vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# # Index\n",
    "# retriever = vectorstore.as_retriever()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e61ad854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x14a546ed090>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.theweathernetwork.com/en/city/ca/ontario/oshawa/current\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84e9762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oshawa, ON Current Weather - The Weather NetworkThe Weather NetworkWeatherMapsNewsVideoAssistant |CurrentForecastsCurrentHourly7 Days14 DaysHistorical Oshawa, ON Current WeatherOshawa, ONUpdated 10 minutes ago25°CPartly cloudyFeels 33H: 21°   L: 18°Nocturnal storms pose rain, wind threatHourlyFull 72 hours1am23°Feels 3030%2am23°Feels 3030%3am23°Feels 2930%4am22°Feels 2730%5am22°Feels 2630%Radar MapSee all mapsDetailed Observations--Sunrise--Sunset--No Data AvailableWind--Gust: --No Data AvailablePressure--No Data AvailableHumidity--No Data AvailableVisibility--No Data AvailableCeiling--No Data AvailableYesterday--No Data AvailableWatch videosContent continues below7 DaysAll 7 days14 DaysAll 14 daysContent continues belowRound of nocturnal storms could make sleeping difficult in parts of southern Ontario Tuesday nightRead moreNocturnal storms pose rain, wind threatContent continues belowDiscoverContent continues belowNewsRead more newsNighttime storms in southern Ontario could bring gusty winds and heavy rain0:48Volcano nearly engulfs hikers after erupting in IndonesiaThis it it, Toronto, the last 8 p.m. sunset of the year comes Tuesday1:23The downside of selfies: how social media is putting nature at riskDeadly Alaska landslide displaces dozens of families in scenic Ketchikan0:46Tourists panic as rocks plunge from waterfall in ChinaHelium leak delays SpaceX's history-making Polaris Dawn mission0:40Over 300 people seek refuge after flash floods in India30 cm of snow in August? It's possible in Alberta's Rockies1:32A couple recounts evacuation from Iceland as volcano roars back to lifePair of tornadoes confirmed west of Cornwall2:18This abandoned theme park is one of Hurricane Katrina's remaining scars'Nature is punishing us': Drought imperils farmers and bees in Mexico's north1:56Jasper siblings mourn the loss of business parents builtWasps are aggressive right now; here's why and how to stay safe1:29Start planning now to witness the 'eclipse of the century'N.W.T. communities learning to adapt as heat waves become more common2:17Why Canada's oldest operating lighthouse and others are being savedCould climate change cause more lightning — and spark more wildfires?1:58July floods in the GTA cost nearly $1B, August floods may have been worsePowerful 5.4 magnitude earthquake hits Portugal’s coastline0:52Twin waterspouts 'dance' along the Florida coastBangladesh floods leave 23 dead, 5.7 million people affected0:45Update: Waterspout capsizes yacht off Sicily, owner found dead WednesdayAtlantic hurricane activity could pick up in time for Labour Day0:50Watch sewers explode during extreme rainstorm in TokyoExploring our fascination with nature's extreme fury1:00See the moment house collapses amid historic flash floodingUnmasking the controversy: Geoengineering climate change2:05Booming solar industry has a growing appetite for weed-chomping crewsWeather for more locationsVacationSchoolsAirportsParksAttractionsCottageGolfMarineCampingBeachesSkiContent continues belowSupportHelp CenterPrivacy PolicyTerms Of UseAI Code of EthicsAccessibility and AccommodationManage My ConsentCareersAdvertise with UsAbout UsWeather ToolsWeather APIsAndroid AppIOS AppTV Weather AppsMétéoMédiaPelmorex CorpElTiempoOtempoClimaAlert ReadyFollow Us©2024 Pelmorex Corp\n"
     ]
    }
   ],
   "source": [
    "print(loader.load()[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab399c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading ....: 100%|███████████████████████████████████████████████████| 10000000/10000000 [00:02<00:00, 4432645.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(0, 10**7), desc=f\"loading ....\"):\n",
    "    x=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3e3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IoT-Engine",
   "language": "python",
   "name": "iot-engine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
