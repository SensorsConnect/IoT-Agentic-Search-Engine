{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af88477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load a specific environment variable\n",
    "GKGraph_API_KEY = os.environ.get('GKGraph_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27df900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: Tim hortons\n",
      "Name: Tim Hortons\n",
      "Description: Restaurant chain\n",
      "Detailed Description: Tim Hortons Inc., known colloquially as Tim's, Timmies, or Timmy's, is a Canadian multinational coffeehouse and restaurant chain with headquarters in Toronto; it serves coffee, donuts, sandwiches, breakfast egg muffins and other fast-food items. \n"
     ]
    }
   ],
   "source": [
    "# KnowledgeGraphTool\n",
    "import requests\n",
    "\n",
    "class KnowledgeGraphTool:\n",
    "    def __init__(self, api_key):\n",
    "        \"\"\"\n",
    "        Initializes the KnowledgeGraphTool with the provided API key.\n",
    "\n",
    "        Parameters:\n",
    "        api_key (str): The API key for accessing the Google Knowledge Graph API.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://kgsearch.googleapis.com/v1/entities:search\"\n",
    "\n",
    "    def search(self, query):\n",
    "        \"\"\"\n",
    "        Sends a search request to the Google Knowledge Graph API with the given query.\n",
    "\n",
    "        Parameters:\n",
    "        query (str): The user's query to search for in the Knowledge Graph.\n",
    "\n",
    "        Returns:\n",
    "        dict: The JSON response from the API if the request is successful.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'query': query,\n",
    "            'key': self.api_key,\n",
    "            'limit': 1,\n",
    "            'indent': True\n",
    "        }\n",
    "        response = requests.get(self.base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            response.raise_for_status()\n",
    "\n",
    "    def extract_info(self, response):\n",
    "        \"\"\"\n",
    "        Extracts and returns the relevant information from the API response.\n",
    "\n",
    "        Parameters:\n",
    "        response (dict): The JSON response from the Knowledge Graph API.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary containing the name, description, and detailed description \n",
    "              from the response. Returns None if no relevant information is found.\n",
    "        \"\"\"\n",
    "        if 'itemListElement' in response and len(response['itemListElement']) > 0:\n",
    "            element = response['itemListElement'][0]['result']\n",
    "            name = element.get('name', 'N/A')\n",
    "            description = element.get('description', 'N/A')\n",
    "            detailed_description = element.get('detailedDescription', {}).get('articleBody', 'N/A')\n",
    "            return {\n",
    "                'name': name,\n",
    "                'description': description,\n",
    "                'detailed_description': detailed_description\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    The main function that handles user input, calls the search and extraction methods,\n",
    "    and prints the results.\n",
    "    \"\"\"\n",
    "    api_key = GKGraph_API_KEY\n",
    "    tool = KnowledgeGraphTool(api_key)\n",
    "    query = input(\"Enter your query: \")\n",
    "    response = tool.search(query)\n",
    "    info = tool.extract_info(response)\n",
    "    if info:\n",
    "        print(f\"Name: {info['name']}\")\n",
    "        print(f\"Description: {info['description']}\")\n",
    "        print(f\"Detailed Description: {info['detailed_description']}\")\n",
    "    else:\n",
    "        print(\"No information found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09be1c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the location (latitude,longitude): 43.941888,-78.8955136\n",
      "Enter the type of place (e.g., restaurant, cafe): cafe\n",
      "Enter the radius in meters (default is 1000): 5000\n",
      "Name: Tim Hortons\n",
      "Address: 1251 Simcoe Street North, Oshawa\n",
      "Rating: 3.6\n",
      "\n",
      "Name: Tim Hortons\n",
      "Address: 4051 Thickson Road North, Whitby\n",
      "Rating: 3.7\n",
      "\n",
      "Name: Coffee Culture CafÃ© & Eatery\n",
      "Address: 555 Rossland Road East, Oshawa\n",
      "Rating: 4.3\n",
      "\n",
      "Name: Tim Hortons\n",
      "Address: Durham College, Gordon Willey Building, Upper Level, Oshawa\n",
      "Rating: 3.8\n",
      "\n",
      "Name: Country Style\n",
      "Address: Canadian Tire, 1333 Wilson Road North, Oshawa\n",
      "Rating: 3.4\n",
      "\n",
      "Name: Coffee Time\n",
      "Address: 500 Rossland Road West, Oshawa\n",
      "Rating: 4.1\n",
      "\n",
      "Name: Tim Hortons\n",
      "Address: 1361 Harmony Road North, Oshawa\n",
      "Rating: 3.7\n",
      "\n",
      "Name: Tim Hortons\n",
      "Address: 1471 Harmony Road North, Oshawa\n",
      "Rating: 3.6\n",
      "\n",
      "Name: Tim Hortons\n",
      "Address: 1311 Harmony Road North, Oshawa\n",
      "Rating: 3.7\n",
      "\n",
      "Name: Tim Hortons\n",
      "Address: 575 Thornton Road North, Oshawa\n",
      "Rating: 4\n",
      "\n",
      "Name: Tim Hortons\n",
      "Address: 520 Winchester Road East, Whitby\n",
      "Rating: 3.5\n",
      "\n",
      "Name: Tim Hortons\n",
      "Address: 485 Winchester Road East, Whitby\n",
      "Rating: 3.6\n",
      "\n",
      "Name: Tim Hortons\n",
      "Address: 3309 Simcoe Street North, Oshawa\n",
      "Rating: 3.8\n",
      "\n",
      "Name: Starbucks\n",
      "Address: 1365 Wilson Road North, Oshawa\n",
      "Rating: 4.2\n",
      "\n",
      "Name: McDonald's\n",
      "Address: 1349 Simcoe Street North, Oshawa\n",
      "Rating: 3.6\n",
      "\n",
      "Name: Panera Bread\n",
      "Address: 372 Taunton Road East Unit 12, Whitby\n",
      "Rating: 4.1\n",
      "\n",
      "Name: Starbucks\n",
      "Address: Thickson Mills, 660 Taunton Road East, Whitby\n",
      "Rating: 4.2\n",
      "\n",
      "Name: McDonald's\n",
      "Address: 1471 Harmony Road North, Oshawa\n",
      "Rating: 3\n",
      "\n",
      "Name: Tim Hortons\n",
      "Address: 251 Ritson Road North, Oshawa\n",
      "Rating: 3.8\n",
      "\n",
      "Name: Markcol Oshawa\n",
      "Address: 1170 Simcoe Street North, Oshawa\n",
      "Rating: 4.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "class NearbyPlacesTool:\n",
    "    def __init__(self, api_key):\n",
    "        \"\"\"\n",
    "        Initializes the NearbyPlacesTool with the provided API key.\n",
    "\n",
    "        Parameters:\n",
    "        api_key (str): The API key for accessing the Google Places API.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "\n",
    "    def search(self, location, place_type, radius=1000):\n",
    "        \"\"\"\n",
    "        Sends a search request to the Google Places API to find nearby places of a given type.\n",
    "\n",
    "        Parameters:\n",
    "        location (str): The location (latitude,longitude) around which to search for places.\n",
    "        place_type (str): The type of place to search for (e.g., restaurant, cafe).\n",
    "        radius (int): The radius in meters within which to search for places (default is 1000 meters).\n",
    "\n",
    "        Returns:\n",
    "        dict: The JSON response from the API if the request is successful.\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'location': location,\n",
    "            'type': place_type,\n",
    "            'radius': radius,\n",
    "            'key': self.api_key\n",
    "        }\n",
    "        response = requests.get(self.base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            response.raise_for_status()\n",
    "\n",
    "    def extract_info(self, response):\n",
    "        \"\"\"\n",
    "        Extracts and returns the relevant information from the API response.\n",
    "\n",
    "        Parameters:\n",
    "        response (dict): The JSON response from the Google Places API.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of dictionaries containing the name, address, and rating of each place.\n",
    "        \"\"\"\n",
    "        places_info = []\n",
    "        if 'results' in response and len(response['results']) > 0:\n",
    "            for place in response['results']:\n",
    "                name = place.get('name', 'N/A')\n",
    "                address = place.get('vicinity', 'N/A')\n",
    "                rating = place.get('rating', 'N/A')\n",
    "                places_info.append({\n",
    "                    'name': name,\n",
    "                    'address': address,\n",
    "                    'rating': rating\n",
    "                })\n",
    "        return places_info\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    The main function that handles user input, calls the search and extraction methods,\n",
    "    and prints the results.\n",
    "    \"\"\"\n",
    "    api_key = GKGraph_API_KEY\n",
    "    tool = NearbyPlacesTool(api_key)\n",
    "    location = input(\"Enter the location (latitude,longitude): \")\n",
    "    place_type = input(\"Enter the type of place (e.g., restaurant, cafe): \")\n",
    "    radius = input(\"Enter the radius in meters (default is 1000): \")\n",
    "    radius = int(radius) if radius else 1000\n",
    "    response = tool.search(location, place_type, radius)\n",
    "    places_info = tool.extract_info(response)\n",
    "    if places_info:\n",
    "        for place in places_info:\n",
    "            print(f\"Name: {place['name']}\")\n",
    "            print(f\"Address: {place['address']}\")\n",
    "            print(f\"Rating: {place['rating']}\\n\")\n",
    "    else:\n",
    "        print(\"No nearby places found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bed7a9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "import time\n",
    "\n",
    "def scrape_tim_hortons(url):\n",
    "    # Set up the Selenium WebDriver (Chrome in this case)\n",
    "    driver = webdriver.Chrome()  # You can specify the path to your ChromeDriver if it's not in your PATH\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Give time for the page to load\n",
    "    time.sleep(5)  # Adjust this sleep time as needed\n",
    "    \n",
    "    # Find the elements containing the service data\n",
    "    # This part might need adjustments based on the actual HTML structure\n",
    "    services = driver.find_elements(By.CSS_SELECTOR, 'div[role=\"article\"]')\n",
    "    \n",
    "    services_data = []\n",
    "    for service in services:\n",
    "        try:\n",
    "            name = service.find_element(By.CSS_SELECTOR, 'div[jsan=\"7.tAeYtd\"]').text\n",
    "            address = service.find_element(By.CSS_SELECTOR, 'span[jsan=\"7.QsDR1c\"]').text\n",
    "            services_data.append({\n",
    "                'name': name,\n",
    "                'address': address\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "    \n",
    "    return services_data\n",
    "\n",
    "# URL of the Google Maps search results\n",
    "url = 'https://www.google.ca/maps/search/Tim+Hortons/@43.9419461,-78.8955136,14z/data=!3m1!4b1?entry=ttu'\n",
    "\n",
    "# Scrape the services data\n",
    "services_data = scrape_tim_hortons(url)\n",
    "\n",
    "# Convert the data to JSON format\n",
    "services_json = json.dumps(services_data, indent=4)\n",
    "\n",
    "# Print the JSON data\n",
    "print(services_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f50153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import Wikipadia_Scrapper\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def generate_random_email():\n",
    "    \"\"\"\n",
    "    Generates a randomized email address by combining a random username with a predefined domain. \n",
    "    The generated email address serves as a unique identifier for data extraction processes.\n",
    "\n",
    "    Returns:\n",
    "        String : random email address\n",
    "    \"\"\"\n",
    "    # Define the domain name for the email (you can change this to your desired domain)\n",
    "    domain = \"test.com\"\n",
    "\n",
    "    # Generate a random username with a length between 5 and 10 characters\n",
    "    username_length = random.randint(5, 10)\n",
    "    username = ''.join(random.choices(\n",
    "        string.ascii_letters + string.digits, k=username_length))\n",
    "\n",
    "    # Combine the username and domain to form the email address\n",
    "    email = f\"{username}@{domain}\"\n",
    "    return email\n",
    "\n",
    "\n",
    "def extract_place_id(url):\n",
    "    \"\"\"\n",
    "    Extracts and returns the unique place ID from a given URL using a regular expression pattern. \n",
    "    This function is particularly useful for obtaining the place ID associated with a Google Maps link.\n",
    "\n",
    "    Args:\n",
    "        url (String): service url\n",
    "\n",
    "    Returns:\n",
    "        String: place unique id\n",
    "    \"\"\"\n",
    "    pattern = r'placeid=([a-zA-Z0-9\\-]+)'\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_place_unique_ID(service_name, service_address, browser):\n",
    "    \"\"\"\n",
    "    Utilizes a web browser instance to navigate to a review link generator website, performs a series of user interactions \n",
    "    and form submissions to retrieve a unique place ID associated with a specific service name and address combination. \n",
    "    Returns the obtained place ID or an empty string if unsuccessful.\n",
    "\n",
    "    Args:\n",
    "        service_name (String): name of service\n",
    "        service_address (String): address of service \n",
    "        browser (Object): google chrome selenium driver\n",
    "\n",
    "    Returns:\n",
    "        String: place unique id\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://reviewsonmywebsite.com/google-review-link-generator\"\n",
    "        browser.get(url)\n",
    "\n",
    "        search_element = browser.find_element(\n",
    "            By.XPATH, \"/html/body/main/section[1]/div/div/div/div[2]/div[1]/div[1]/div[1]/fieldset/input\")\n",
    "        search_element.send_keys(service_name + \" \" + service_address)\n",
    "        time.sleep(2)\n",
    "        auto_complate_element = browser.find_element(\n",
    "            By.XPATH, \"/html/body/main/section[1]/div/div/div/div[2]/div[1]/div[1]/div[1]/fieldset/ul/li/a\")\n",
    "\n",
    "        auto_complate_element.click()\n",
    "        generate_button_one = browser.find_element(\n",
    "            By.XPATH, \"/html/body/main/section[1]/div/div/div/div[2]/div[1]/div[1]/div[2]/button\")\n",
    "\n",
    "        generate_button_one.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        email_element = browser.find_element(\n",
    "            By.XPATH, \"/html/body/main/section[1]/div/div/div/div[2]/div[2]/div[3]/div/input\")\n",
    "        email_element.send_keys(generate_random_email())\n",
    "\n",
    "        generate_button_two = browser.find_element(\n",
    "            By.XPATH, \"/html/body/main/section[1]/div/div/div/div[2]/div[2]/div[3]/button\")\n",
    "        generate_button_two.click()\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        result_element = browser.find_element(\n",
    "            By.XPATH, \"/html/body/main/section[1]/div/div/div/div[2]/div[2]/div[4]/div/div/input\")\n",
    "        result = result_element.get_attribute(\"value\")\n",
    "        return result\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_service_address(service_URL, browser):\n",
    "    \"\"\"\n",
    "    Uses a web browser instance to visit a service URL and extracts the service address by parsing relevant elements on the page.\n",
    "\n",
    "    Args:\n",
    "        service_URL (String): service url\n",
    "        browser (Object): google chrome selenium driver\n",
    "\n",
    "    Returns:\n",
    "        String: address of service\n",
    "    \"\"\"\n",
    "    try:\n",
    "        browser.get(service_URL)\n",
    "        data_item_id = browser.find_elements(\n",
    "            By.XPATH, \"//button[@data-item-id='address']\")\n",
    "        for div in data_item_id:\n",
    "            if div.get_attribute('aria-label'):\n",
    "                address = div.get_attribute('aria-label')\n",
    "                return address[address.find(\":\") + 2:-1]+', Canada'\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_service_about_type(service_URL, browser):\n",
    "    \"\"\"\n",
    "    Visits a service URL through a web browser instance and extracts descriptive information about the service's background and details.\n",
    "    This extracted information enriches the dataset by providing insights into the service's offerings which will effect the chatbot responses.\n",
    "    Args:\n",
    "        service_URL (String):  service url\n",
    "        browser (Object):  google chrome selenium driver\n",
    "\n",
    "    Returns:\n",
    "        String: description about service\n",
    "    \"\"\"\n",
    "    service_type = None\n",
    "    about_service = None\n",
    "    try:\n",
    "        browser.get(service_URL)\n",
    "        service_type_element = browser.find_element(By.XPATH, \"/html/body/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div/div[1]/div[2]/div/div[2]/span/span/button\")\n",
    "        service_type = service_type_element.text\n",
    "        print(\"Service Type: \" + service_type)\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        browser.get(service_URL)\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        about_element = browser.find_element(\n",
    "            By.XPATH, \"/html/body/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[6]/button/div[2]/div[1]/div[1]\")\n",
    "        about_service =  about_element.text\n",
    "        print(\"About: \" + about_service)\n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(print(e))\n",
    "        about_service = \"\"\n",
    "        service_type = \"\"\n",
    "\n",
    "    return about_service, service_type\n",
    "\n",
    "\n",
    "def get_service_name_URL(Service, coordinates, K=15):\n",
    "    \"\"\"\n",
    "    Constructs a Google Maps search URL based on the provided service name and coordinates, then uses a web browser instance to \n",
    "    retrieve a list of service names and URLs associated with the search. Iteratively collects relevant data, including service names \n",
    "    and corresponding URLs.\n",
    "\n",
    "    Args:\n",
    "        Service (String): type of service\n",
    "        coordinates (String): coordinates of search\n",
    "        K (int, optional): _description_. Defaults to 15.\n",
    "\n",
    "    Returns:\n",
    "        Panda's Dataframe: dataframe contains names, and urls of service from given type around the given coordinates\n",
    "    \"\"\"\n",
    "    url = 'https://www.google.ca/maps/search/'\n",
    "    service = Service.replace(\" \", \"+\")\n",
    "    temp_url = url + service + \"/@\" + coordinates + \",11.54z\"\n",
    "    browser = webdriver.Chrome()\n",
    "\n",
    "    services_list = []\n",
    "    browser.get(temp_url)\n",
    "\n",
    "    for j in range(K):\n",
    "        try:\n",
    "            a_tag_elements = browser.find_elements(By.TAG_NAME, \"a\")\n",
    "            for a_tag_element in a_tag_elements:\n",
    "                if a_tag_element.get_attribute(\"aria-label\") and a_tag_element.get_attribute('href'):\n",
    "                    bar = a_tag_element\n",
    "\n",
    "                    bar.location_once_scrolled_into_view\n",
    "                    service_name = a_tag_element.get_attribute(\"aria-label\")\n",
    "                    service_url = a_tag_element.get_attribute(\"href\")\n",
    "\n",
    "                    data = {}\n",
    "                    data[\"Service Name\"] = service_name\n",
    "                    data[\"Service URL\"] = service_url\n",
    "                    services_list.append(data)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    browser.quit()\n",
    "    df = pd.DataFrame(services_list)\n",
    "    df = df.drop_duplicates(subset=[\"Service URL\"])\n",
    "    df = df[df[\"Service URL\"].str.contains('www.google.ca')]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rate_popular_time(url):\n",
    "    \"\"\"\n",
    "    Uses a web browser instance to access a given URL and extracts information related to service ratings, popular times for each day \n",
    "    of the week, and opening/closing times. \n",
    "\n",
    "    Args:\n",
    "        url (String): google maps url of the service\n",
    "\n",
    "    Returns:\n",
    "        dict : popular time of service for the 7 days\n",
    "        list : opening and closing time for service\n",
    "        float : rate of service\n",
    "    \"\"\"\n",
    "    try:\n",
    "        week_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', ]\n",
    "        data = {}\n",
    "\n",
    "        browser = webdriver.Chrome()\n",
    "        browser.get(url)\n",
    "\n",
    "        time.sleep(1)\n",
    "        data_item_id = browser.find_elements(\n",
    "            By.XPATH, \"//button[@data-item-id='address']\")\n",
    "\n",
    "        try:\n",
    "            rate_element = browser.find_element(\n",
    "                By.XPATH, \"/html/body/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]/div/div[1]/div[2]/div/div[1]/div[2]/span[1]/span[1]\")\n",
    "            rate = rate_element.text\n",
    "        except:\n",
    "            rate = None\n",
    "\n",
    "        popular_time_list = []\n",
    "\n",
    "        for div in data_item_id:\n",
    "            if div.get_attribute('aria-label'):\n",
    "                div_tags = browser.find_elements(By.TAG_NAME, \"div\")\n",
    "\n",
    "                for div in div_tags:\n",
    "                    if (div.get_attribute('aria-label')) and (\"a.m\" in div.get_attribute('aria-label') or \"p.m\" in div.get_attribute('aria-label')):\n",
    "                        popular_time_list.append(div.get_attribute(\n",
    "                            \"aria-label\").replace(\"\\u202f\", \" \").replace(\"..\", \"\"))\n",
    "\n",
    "        if len(popular_time_list) > 0:\n",
    "            if \"Hide open hours for the week\" in popular_time_list[0]:\n",
    "                opening_closing_time = popular_time_list[0].replace(\n",
    "                    \".. Hide open hours for the week\", \"\")\n",
    "                popular_time_list.pop(0)\n",
    "\n",
    "                i = 0\n",
    "                j = len(popular_time_list) // 7\n",
    "                for day in week_days:\n",
    "                    data[day] = popular_time_list[i: j]\n",
    "                    i = j\n",
    "                    j = j+(len(popular_time_list)//7)\n",
    "                return data, opening_closing_time, rate\n",
    "            else:\n",
    "                i = 0\n",
    "                j = len(popular_time_list) // 7\n",
    "                for day in week_days:\n",
    "                    data[day] = popular_time_list[i: j]\n",
    "                    i = j\n",
    "                    j = j+(len(popular_time_list)//7)\n",
    "                return data, None, rate\n",
    "        else:\n",
    "            return {'Monday': [], 'Tuesday': [], 'Wednesday': [], 'Thursday': [], 'Friday': [], 'Saturday': [], 'Sunday': []}, None, rate\n",
    "    except:\n",
    "        return {'Monday': [], 'Tuesday': [], 'Wednesday': [], 'Thursday': [], 'Friday': [], 'Saturday': [], 'Sunday': []}, None, rate\n",
    "\n",
    "\n",
    "def merge_rate_popular_time_with_dataset_function(dataframe):\n",
    "    \"\"\"\n",
    "    Merges the extracted rate and popular time data with an existing dataset, ensuring compatibility and alignment of the information. \n",
    "    This function enhances the dataset by incorporating temporal and rating details for each service.\n",
    "\n",
    "\n",
    "\n",
    "    Args:\n",
    "        dataframe (Panda's Dataframe): dataframe that contains popular service name, address, url, place unique id, about, type\n",
    "\n",
    "    Returns:\n",
    "        Panda's Dataframe: updated dataframe with popular time, opening/closing time, and rates\n",
    "    \"\"\"\n",
    "\n",
    "    days = ['Monday', 'Tuesday',\n",
    "            'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "    columns_order = [\"Service Type\", \"Service Name\", \"Service URL\",\n",
    "                     \"About\", \"Service Address\", \"Service Place Unique ID\"]\n",
    "    df = dataframe.reindex(columns=columns_order)\n",
    "\n",
    "    df[\"Rate\"] = None\n",
    "\n",
    "    for day in days:\n",
    "        df[day] = None\n",
    "\n",
    "    df[\"Opening/Closing Time\"] = None\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        url = df.iloc[i][\"Service URL\"]\n",
    "        popular_time_data, opening_closing_time, rate = get_rate_popular_time(\n",
    "            url)\n",
    "\n",
    "        if opening_closing_time is None:\n",
    "            for day in days:\n",
    "                day_column_index = df.columns.get_loc(day)\n",
    "                df.iloc[i, day_column_index] = json.dumps(\n",
    "                    popular_time_data[day])\n",
    "\n",
    "        else:\n",
    "            opening_closing_time_column_index = df.columns.get_loc(\n",
    "                \"Opening/Closing Time\")\n",
    "            df.iloc[i, opening_closing_time_column_index] = opening_closing_time\n",
    "\n",
    "            for day in days:\n",
    "                day_column_index = df.columns.get_loc(day)\n",
    "                df.iloc[i, day_column_index] = json.dumps(\n",
    "                    popular_time_data[day])\n",
    "\n",
    "        rate_column_index = df.columns.get_loc(\"Rate\")\n",
    "        df.iloc[i, rate_column_index] = rate\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_dataset_helper_function(service, coordinates, K, browser):\n",
    "    \"\"\"\n",
    "    Builds a dataset by utilizing the functions get_service_name_URL, get_service_address, and get_service_about. Collects \n",
    "    service-related information, such as addresses and descriptions, for subsequent integration into a comprehensive dataset.\n",
    "    \"\"\"\n",
    "    df = get_service_name_URL(Service=service, coordinates=coordinates, K=K)\n",
    "    addresses_list = []\n",
    "    about_list = []\n",
    "    service_type_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        service_url = row[\"Service URL\"]\n",
    "        service_address = get_service_address(service_url, browser)\n",
    "        about_service, service_type = get_service_about_type(service_url, browser)\n",
    "\n",
    "        addresses_list.append(service_address)\n",
    "        about_list.append(about_service)\n",
    "        service_type_list.append(service_type)\n",
    "\n",
    "    df[\"Service Type\"] = service_type_list\n",
    "    df[\"About\"] = about_list\n",
    "    df[\"Service Address\"] = addresses_list\n",
    "\n",
    "    df.dropna(subset=[\"Service Address\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_dataset_function(service, coordinates, K=15):\n",
    "    \"\"\"\n",
    "    Coordinates the construction of a dataset by orchestrating functions to retrieve service-related data, including service names, \n",
    "    URLs, addresses, descriptions, and place unique IDs. Integrates temporal and rating information through the \n",
    "    merge_rate_popular_time_with_dataset_function.\n",
    "    \"\"\"\n",
    "    browser = webdriver.Chrome()\n",
    "    df = build_dataset_helper_function(\n",
    "        service=service, coordinates=coordinates, K=K, browser=browser)\n",
    "\n",
    "    places_unique_id_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        service_name = row[\"Service Name\"]\n",
    "        service_address = row[\"Service Address\"]\n",
    "\n",
    "        service_place_unique_id = get_place_unique_ID(\n",
    "            service_name=service_name, service_address=service_address, browser=browser)\n",
    "        \n",
    "        \n",
    "        places_unique_id_list.append(extract_place_id(service_place_unique_id))\n",
    "\n",
    "    df[\"Service Place Unique ID\"] = places_unique_id_list\n",
    "    \n",
    "\n",
    "    browser.quit()\n",
    "    time.sleep(1)\n",
    "    return merge_rate_popular_time_with_dataset_function(df)\n",
    "\n",
    "\n",
    "# Main Loop\n",
    "finished_services_list = []\n",
    "# Path to the output text file\n",
    "subservices_description = \"./subservices_description.txt\"\n",
    "\n",
    "# Read services from a text file and add them to a list\n",
    "services_privot = [\"Starbucks\", \"Tim Horton's\"]\n",
    "with open(\"services.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        service = line.strip()\n",
    "        services_privot.append(service)\n",
    "\n",
    "for service in services_privot:\n",
    "    if service not in finished_services_list and len(finished_services_list) < 1000:\n",
    "        df = build_dataset_function(service, coordinates=\"43.6515,-79.3835\", K=30)\n",
    "        finished_services_list.append(service)\n",
    "        for related_service in df[\"Service Type\"].to_list():\n",
    "            if related_service not in services_privot:\n",
    "                services_privot.append(related_service)\n",
    "        \n",
    "        # Open the file in append mode\n",
    "        with open(subservices_description, \"a\") as file:\n",
    "            file.write(Wikipadia_Scrapper.generate_description(service) + \"\\n\")  # Write the paragraph\n",
    "            file.write(\"---\\n\")           # Write the separator\n",
    "\n",
    "    print(\"Service: \" + service)\n",
    "    print(\"Pivot len: \" + str(len(services_privot)))\n",
    "    print(\"Finished Services List: \" + str(len(finished_services_list)))\n",
    "\n",
    "    # Drop rows with None values in 'id' column\n",
    "    df = df.dropna(subset=[\"Service Place Unique ID\"])\n",
    "\n",
    "    # Drop rows with duplicate values in 'id' column, keeping only the first occurrence\n",
    "    df = df.drop_duplicates(subset=[\"Service Place Unique ID\"], keep='first')\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df.to_csv(\"./Datasets/\"+service+\"_Dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1567d2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raising Cane's Chicken Fingers 1501 Broadway, New York, NY 10036, United States\n",
      "Chick-fil-A 1180 6th Ave, New York, NY 10036, United States\n",
      "Chick-fil-A 675 8th Ave, New York, NY 10036, United States\n",
      "The Smith 1150 Broadway, New York, NY 10001, United States\n",
      "Arby's 611 8th Ave, New York, NY 10018, United States\n",
      "Five Guys 43 W 55th St, New York, NY 10019, United States\n",
      "Applebee's Grill + Bar 234 W 42nd St, New York, NY 10036, United States\n",
      "Yard House 575 7th Ave, New York, NY 10018, United States\n",
      "P.F. Chang's 113 University Pl, New York, NY 10003, United States\n",
      "Chick-fil-A 711 Lexington Ave, New York, NY 10022, United States\n",
      "Shake Shack Astor Place 20 3rd Ave, New York, NY 10003, United States\n",
      "Chain Restaurant Total Rewards Association 330 W 38th St, New York, NY 10018, United States\n",
      "Taco Bell 840 8th Ave, New York, NY 10019, United States\n",
      "Jollibee 609 8th Ave, New York, NY 10018, United States\n",
      "Tick Tock Diner NY 481 8th Ave, New York, NY 10001, United States\n",
      "Junior's Restaurant & Bakery 1515 Broadway @, W 45th St, New York, NY 10036, United States\n",
      "Bubba Gump Shrimp Co. 1501 Broadway, New York, NY 10036, United States\n",
      "McDonald's 966 3rd Ave, New York, NY 10022, United States\n",
      "Olive Garden Italian Restaurant 2 Times Sq, New York, NY 10036, United States\n",
      "NY Express Halal Food 201 E 45th St, New York, NY 10017, United States\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def google_places_text_search(query, api_key):\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'key': api_key\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Replace 'your_api_key' with your actual Google Maps API key and 'your_query' with your search query.\n",
    "api_key = GKGraph_API_KEY\n",
    "# query = 'I want to find middle estern restaurant in NewYork'\n",
    "query=\"ChainRestaurant New York\"\n",
    "result = google_places_text_search(query, api_key)\n",
    "\n",
    "if result:\n",
    "    for place in result['results']:\n",
    "        print(place['name'], place['formatted_address'])\n",
    "else:\n",
    "    print(\"Error fetching data from Google Places API\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd835dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a592e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
